# OpenRouter API Configuration
# Get your API key from https://openrouter.ai/keys
OPENROUTER_API_KEY=your_openrouter_api_key_here

# Speechmatics API Configuration
# Get your API key from https://portal.speechmatics.com/
SPEECHMATICS_API_KEY=your_speechmatics_api_key_here

# Azure Speech to Text (optional replacement for Speechmatics)
# https://learn.microsoft.com/azure/ai-services/speech-service/
AZURE_SPEECH_KEY=your_azure_speech_key_here
AZURE_SPEECH_REGION=eastus
# Optional: custom endpoint, defaults to https://{region}.api.cognitive.microsoft.com
# AZURE_SPEECH_ENDPOINT=https://eastus.api.cognitive.microsoft.com

# Azure Blob Storage (required when Azure STT needs to read local files)
# Provide either AccountKey in the connection string or via AZURE_STORAGE_ACCOUNT_KEY
AZURE_STORAGE_CONNECTION_STRING=DefaultEndpointsProtocol=https;AccountName=example;AccountKey=abc123;EndpointSuffix=core.windows.net
AZURE_STORAGE_CONTAINER=diarization-audio
# AZURE_STORAGE_ACCOUNT_KEY=abc123

# Audio separation (AudioShake Tasks + localtunnel)
# https://developer.audioshake.ai/tasks
AUDIOSHAKE_API_KEY=your_audioshake_api_key_here
# Optional: fixed public URL (fallback if tunnel unavailable)
# PUBLIC_URL=https://your-domain.example.com
# Optional: request a fixed localtunnel subdomain
# LOCALTUNNEL_SUBDOMAIN=your_unique_name

# Application URL (for OpenRouter headers)
APP_URL=http://localhost:3000

# Server Port
PORT=3000

# Optional: override default model IDs
# FAST_MODEL_ID=gpt-oss-120b
# SMART_MODEL_ID=gpt-5.1
# SMART_2_MODEL_ID=google/gemini-3-pro-preview
# TEST_MODEL_ID=google/gemma-3-4b
# TEST2_MODEL_ID=google/gemma-3-1b
# TEST2_MODEL_ID=google/gemma-3-1b

# Local LLM (LM Studio) configuration
# LOCAL_LLM_BASE_URL defaults to http://127.0.0.1:3001 (LM Studio HTTP server)
LOCAL_LLM_BASE_URL=http://127.0.0.1:3001
LOCAL_LLM_MODEL=openai/gpt-oss-20b
# LOCAL_LLM_API_KEY=

# LLM Response Cache Configuration
# LLM_CACHE_ENABLED controls caching of LLM responses (independent from audio cache)
# Set to 'false' to disable LLM response caching (default: true)
# Note: Audio/transcription cache is independent and always enabled
LLM_CACHE_ENABLED=true

# OpenAI (Whisper + LLM classification)
# https://platform.openai.com/
OPENAI_API_KEY=your_openai_api_key_here

# PyAnnote Speech Separation (Mode 2 - Overlap Diarization)
# Get your token from https://huggingface.co/settings/tokens
HUGGINGFACE_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# Demo Page Configuration
# Note: Language is always selected from the interface, not from env variables
# LLM Mode is determined automatically: if DEMO_LLM_MODE=local or DEMO_LOCAL_LLM_MODE=local, uses local config

# Cloud/Remote LLM Configuration (default)
# Use this configuration when DEMO_LLM_MODE is NOT 'local'
DEMO_SPEAKER_COUNT=2
DEMO_PIPELINE_MODE=mode3
DEMO_LLM_MODE=smart
DEMO_TRANSCRIPTION_ENGINE=speechmatics

# Local LLM Configuration
# Use this configuration when DEMO_LOCAL_LLM_MODE=local or DEMO_LLM_MODE=local
# To use local LLM, set DEMO_LLM_MODE=local OR set DEMO_LOCAL_LLM_MODE=local
DEMO_LOCAL_SPEAKER_COUNT=2
DEMO_LOCAL_PIPELINE_MODE=mode3
DEMO_LOCAL_LLM_MODE=local
DEMO_LOCAL_TRANSCRIPTION_ENGINE=speechmatics

# Example: To use Cloud LLM, set:
# DEMO_LLM_MODE=smart
# (or any value except 'local')

# Example: To use Local LLM, set:
# DEMO_LLM_MODE=local
# OR
# DEMO_LOCAL_LLM_MODE=local

