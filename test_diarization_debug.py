#!/usr/bin/env python3
"""
–î—ñ–∞–≥–Ω–æ—Å—Ç–∏—á–Ω–∏–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó –Ω–∞ —Ñ–∞–π–ª—ñ speaker_0.wav
"""
import os
import sys
import json

# –î–æ–¥–∞—î–º–æ –ø–æ—Ç–æ—á–Ω—É –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—é –¥–æ —à–ª—è—Ö—É
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# –Ü–º–ø–æ—Ä—Ç—É—î–º–æ —Ñ—É–Ω–∫—Ü—ñ—ó –∑ app_ios_shortcuts.py
from app_ios_shortcuts import (
    extract_speaker_embeddings,
    diarize_audio,
    transcribe_audio,
    combine_diarization_and_transcription
)

def test_diarization(audio_path):
    """–¢–µ—Å—Ç—É—î –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—é –Ω–∞ –∑–∞–¥–∞–Ω–æ–º—É —Ñ–∞–π–ª—ñ"""
    print(f"üîç Testing diarization on: {audio_path}")
    print("=" * 80)
    
    if not os.path.exists(audio_path):
        print(f"‚ùå File not found: {audio_path}")
        return
    
    # –¢–µ—Å—Ç 1: PyAnnote –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—è
    print("\nüìä TEST 1: PyAnnote Diarization")
    print("-" * 80)
    try:
        import pyannote_patch  # noqa: F401
        from pyannote.audio import Pipeline
        import torch
        import torchaudio
        import soundfile as sf
        
        hf_token = os.getenv('HUGGINGFACE_TOKEN')
        if not hf_token:
            print("‚ö†Ô∏è  HUGGINGFACE_TOKEN not set, skipping PyAnnote test")
        else:
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            print(f"üì¶ Loading PyAnnote pipeline on {device}...")
            
            pipeline = Pipeline.from_pretrained(
                "pyannote/speaker-diarization-3.1",
                use_auth_token=hf_token
            )
            pipeline.to(device)
            
            # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –∞—É–¥—ñ–æ
            try:
                data, sample_rate = sf.read(audio_path, dtype='float32')
                if len(data.shape) == 1:
                    waveform = torch.from_numpy(data).unsqueeze(0).float()
                else:
                    waveform = torch.from_numpy(data).transpose(0, 1).float()
            except Exception as load_error:
                print(f"‚ö†Ô∏è  soundfile failed: {load_error}, trying torchaudio...")
                waveform, sample_rate = torchaudio.load(audio_path)
            
            # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ –≤ mono
            if waveform.shape[0] > 1:
                waveform = torch.mean(waveform, dim=0, keepdim=True)
            
            # Resample –¥–æ 16kHz
            if sample_rate != 16000:
                resampler = torchaudio.transforms.Resample(sample_rate, 16000)
                waveform = resampler(waveform)
                sample_rate = 16000
            
            # –ó–∞–ø—É—Å–∫–∞—î–º–æ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—é
            print("üéØ Running PyAnnote diarization...")
            diarization = pipeline({
                "waveform": waveform,
                "sample_rate": sample_rate
            })
            
            # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            diarization_segments = []
            speaker_map = {}
            next_speaker_id = 0
            
            # –ó–±–∏—Ä–∞—î–º–æ –≤—Å—ñ —É–Ω—ñ–∫–∞–ª—å–Ω—ñ —Å–ø—ñ–∫–µ—Ä—ñ–≤
            for turn, _, speaker in diarization.itertracks(yield_label=True):
                if speaker not in speaker_map:
                    speaker_map[speaker] = next_speaker_id
                    next_speaker_id += 1
            
            # –°—Ç–≤–æ—Ä—é—î–º–æ —Å–µ–≥–º–µ–Ω—Ç–∏
            for turn, _, speaker in diarization.itertracks(yield_label=True):
                speaker_id = speaker_map[speaker]
                diarization_segments.append({
                    'speaker': speaker_id,
                    'start': round(turn.start, 2),
                    'end': round(turn.end, 2)
                })
            
            # –°–æ—Ä—Ç—É—î–º–æ –∑–∞ —á–∞—Å–æ–º
            diarization_segments.sort(key=lambda x: x['start'])
            
            print(f"‚úÖ PyAnnote found {len(diarization_segments)} segments from {len(speaker_map)} speakers")
            print(f"   Speaker mapping: {speaker_map}")
            print(f"\nüìã Diarization segments:")
            for seg in diarization_segments[:20]:  # –ü–µ—Ä—à—ñ 20 —Å–µ–≥–º–µ–Ω—Ç—ñ–≤
                print(f"   [{seg['start']:.2f}s - {seg['end']:.2f}s] Speaker {seg['speaker']}")
            
            # –ü—ñ–¥—Ä–∞—Ö–æ–≤—É—î–º–æ —Ç—Ä–∏–≤–∞–ª—ñ—Å—Ç—å –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
            speaker_durations = {}
            for seg in diarization_segments:
                speaker = seg['speaker']
                duration = seg['end'] - seg['start']
                if speaker not in speaker_durations:
                    speaker_durations[speaker] = 0
                speaker_durations[speaker] += duration
            
            print(f"\nüìä Speaker durations:")
            for speaker, dur in sorted(speaker_durations.items()):
                print(f"   Speaker {speaker}: {dur:.2f}s")
            
    except Exception as e:
        print(f"‚ùå PyAnnote test failed: {e}")
        import traceback
        traceback.print_exc()
    
    # –¢–µ—Å—Ç 2: SpeechBrain –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—è
    print("\nüìä TEST 2: SpeechBrain Diarization")
    print("-" * 80)
    try:
        print("üîÑ Extracting embeddings...")
        embeddings, timestamps = extract_speaker_embeddings(
            audio_path,
            segment_duration=1.5,
            overlap=0.5
        )
        
        if embeddings is None or len(embeddings) == 0:
            print("‚ùå Failed to extract embeddings")
        else:
            print(f"‚úÖ Extracted {len(embeddings)} embeddings")
            
            # –í–∏–∫–æ–Ω—É—î–º–æ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—é
            print("üéØ Running SpeechBrain diarization...")
            diarization_segments_sb = diarize_audio(embeddings, timestamps, num_speakers=2)
            
            if not diarization_segments_sb:
                print("‚ùå SpeechBrain diarization failed")
            else:
                print(f"‚úÖ SpeechBrain found {len(diarization_segments_sb)} segments")
                print(f"\nüìã Diarization segments:")
                for seg in diarization_segments_sb[:20]:  # –ü–µ—Ä—à—ñ 20 —Å–µ–≥–º–µ–Ω—Ç—ñ–≤
                    print(f"   [{seg['start']:.2f}s - {seg['end']:.2f}s] Speaker {seg['speaker']}")
                
                # –ü—ñ–¥—Ä–∞—Ö–æ–≤—É—î–º–æ —Ç—Ä–∏–≤–∞–ª—ñ—Å—Ç—å –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
                speaker_durations_sb = {}
                for seg in diarization_segments_sb:
                    speaker = seg['speaker']
                    duration = seg['end'] - seg['start']
                    if speaker not in speaker_durations_sb:
                        speaker_durations_sb[speaker] = 0
                    speaker_durations_sb[speaker] += duration
                
                print(f"\nüìä Speaker durations:")
                for speaker, dur in sorted(speaker_durations_sb.items()):
                    print(f"   Speaker {speaker}: {dur:.2f}s")
    
    except Exception as e:
        print(f"‚ùå SpeechBrain test failed: {e}")
        import traceback
        traceback.print_exc()
    
    # –¢–µ—Å—Ç 3: –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—è + –æ–±'—î–¥–Ω–∞–Ω–Ω—è
    print("\nüìä TEST 3: Transcription + Combination")
    print("-" * 80)
    try:
        print("üé§ Transcribing audio...")
        transcription_text, transcription_segments, words = transcribe_audio(audio_path, language=None)
        
        if not words:
            print("‚ùå No words transcribed")
        else:
            print(f"‚úÖ Transcribed {len(words)} words")
            print(f"\nüìã First 20 words:")
            for word in words[:20]:
                print(f"   [{word['start']:.2f}s - {word['end']:.2f}s] '{word['word']}'")
            
            # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ PyAnnote –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—é —è–∫—â–æ –¥–æ—Å—Ç—É–ø–Ω–∞
            if 'diarization_segments' in locals() and diarization_segments:
                print(f"\nüîó Combining transcription with PyAnnote diarization...")
                combined = combine_diarization_and_transcription(diarization_segments, words)
                
                print(f"‚úÖ Combined into {len(combined)} segments")
                print(f"\nüìã Combined segments:")
                for seg in combined[:20]:
                    print(f"   [{seg['start']:.2f}s - {seg['end']:.2f}s] Speaker {seg['speaker']}: {seg['text'][:50]}")
                
                # –ü—ñ–¥—Ä–∞—Ö–æ–≤—É—î–º–æ —Å–ª–æ–≤–∞ –ø–æ —Å–ø—ñ–∫–µ—Ä–∞—Ö
                speaker_word_counts = {}
                for seg in combined:
                    speaker = seg['speaker']
                    word_count = len(seg['text'].split())
                    if speaker not in speaker_word_counts:
                        speaker_word_counts[speaker] = 0
                    speaker_word_counts[speaker] += word_count
                
                print(f"\nüìä Word distribution by speaker:")
                for speaker, count in sorted(speaker_word_counts.items()):
                    print(f"   Speaker {speaker}: {count} words")
    
    except Exception as e:
        print(f"‚ùå Transcription test failed: {e}")
        import traceback
        traceback.print_exc()
    
    print("\n" + "=" * 80)
    print("‚úÖ Diagnostics complete")

if __name__ == "__main__":
    test_file = "audio examples/detecting main speakers/speaker_0.wav"
    test_diarization(test_file)

