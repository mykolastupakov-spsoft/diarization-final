#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –æ–±—Ä–æ–±–∫–∏ –∞—É–¥—ñ–æ –ø—Ä–∏–∫–ª–∞–¥—É:
1. –î—ñ–∞—Ä–∏–∑–∞—Ü—ñ—è (SpeechBrain)
2. –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –Ω–∞ –æ–¥–Ω–æ–≥–æ–ª–æ—Å—ñ —Ç—Ä–µ–∫–∏
3. –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—è –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–æ–≥–æ —Ñ–∞–π–ª—É
4. –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—è –∫–æ–∂–Ω–æ–≥–æ —Ç—Ä–µ–∫—É
5. –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
"""

import os
import sys
import json
import numpy as np
import torch
import librosa
import soundfile as sf
from pathlib import Path
import warnings

# –ü–∞—Ç—á –¥–ª—è torchaudio —Å—É–º—ñ—Å–Ω–æ—Å—Ç—ñ
exec(open('patch_torchaudio.py').read())

from speechbrain.pretrained import SpeakerRecognition
from sklearn.cluster import SpectralClustering
from scipy.spatial.distance import pdist, squareform
import whisper
import copy

warnings.filterwarnings("ignore")

# –Ü–º–ø–æ—Ä—Ç—É—î–º–æ —Ñ—É–Ω–∫—Ü—ñ—ó –∑ app_ios_shortcuts.py
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# –ì–ª–æ–±–∞–ª—å–Ω—ñ –∑–º—ñ–Ω–Ω—ñ –¥–ª—è –º–æ–¥–µ–ª–µ–π
speaker_model = None
whisper_model = None

def load_models():
    """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –º–æ–¥–µ–ª—ñ SpeechBrain —Ç–∞ Whisper"""
    global speaker_model, whisper_model
    
    if speaker_model is None:
        print("üîÑ Loading SpeechBrain speaker recognition model...")
        try:
            model_path = "pretrained_models/spkrec-ecapa-voxceleb"
            if os.path.exists(model_path) and os.path.exists(os.path.join(model_path, "hyperparams.yaml")):
                print(f"üìÇ Loading from local directory: {model_path}")
                speaker_model = SpeakerRecognition.from_hparams(
                    source=model_path,
                    savedir=model_path
                )
            else:
                print("üåê Loading from HuggingFace...")
                speaker_model = SpeakerRecognition.from_hparams(
                    source="speechbrain/spkrec-ecapa-voxceleb",
                    savedir="pretrained_models/spkrec-ecapa-voxceleb"
                )
            print("‚úÖ SpeechBrain model loaded")
        except Exception as e:
            print(f"‚ùå Error loading SpeechBrain model: {e}")
            raise
    
    if whisper_model is None:
        print("üîÑ Loading Whisper model (small)...")
        try:
            cache_dir = os.path.expanduser("~/.cache/whisper")
            whisper_model = whisper.load_model("small", download_root=cache_dir)
            print("‚úÖ Whisper model loaded")
        except Exception as e:
            print(f"‚ùå Error loading Whisper model: {e}")
            raise

def extract_speaker_embeddings(audio_path, segment_duration=2.5, overlap=0.4):
    """–í–∏—Ç—è–≥—É—î embeddings –¥–ª—è –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó"""
    print(f"üé§ Extracting embeddings from {audio_path}...")
    try:
        audio, sr = librosa.load(audio_path, sr=16000)
        duration = len(audio) / sr
        print(f"üìä Audio: {duration:.2f}s, {sr}Hz")
        
        segment_samples = int(segment_duration * sr)
        stride_samples = int(segment_samples * (1 - overlap))
        
        embeddings = []
        timestamps = []
        segments_processed = 0
        
        max_start = len(audio) - segment_samples
        if max_start < 0:
            max_start = 0
        
        for start_sample in range(0, max_start + 1, stride_samples):
            end_sample = min(start_sample + segment_samples, len(audio))
            segment = audio[start_sample:end_sample]
            
            if len(segment) == 0:
                continue
            
            try:
                model_device = next(speaker_model.parameters()).device
                segment_tensor = torch.tensor(segment, dtype=torch.float32).unsqueeze(0).to(model_device)
                
                embedding = speaker_model.encode_batch(segment_tensor, normalize=False)
                embedding = embedding.squeeze().cpu().detach().numpy()
                
                if embedding is not None and len(embedding) > 0:
                    embeddings.append(embedding)
                    start_time = start_sample / sr
                    end_time = end_sample / sr
                    timestamps.append((start_time, min(end_time, duration)))
                    segments_processed += 1
            except Exception as e:
                print(f"‚ö†Ô∏è  Error extracting embedding: {e}")
                continue
        
        print(f"‚úÖ Extracted {len(embeddings)} embeddings")
        return np.array(embeddings), timestamps
    
    except Exception as e:
        print(f"‚ùå Error in extract_speaker_embeddings: {e}")
        import traceback
        traceback.print_exc()
        return None, []

def diarize_audio(embeddings, timestamps, num_speakers=2):
    """–í–∏–∫–æ–Ω—É—î –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—é —á–µ—Ä–µ–∑ spectral clustering"""
    print(f"üîç Performing diarization for {num_speakers} speakers...")
    
    if embeddings is None or len(embeddings) < 2:
        print("‚ùå Not enough embeddings for diarization")
        return []
    
    try:
        from sklearn.preprocessing import normalize
        embeddings_normalized = normalize(embeddings, norm='l2')
        
        distances = pdist(embeddings_normalized, metric='cosine')
        distance_matrix = squareform(distances)
        
        mean_dist = np.mean(distances)
        scale = mean_dist if mean_dist > 0.01 else 0.1
        similarity_matrix = np.exp(-distance_matrix / scale)
        
        clustering = SpectralClustering(
            n_clusters=num_speakers,
            affinity='precomputed',
            random_state=42,
            assign_labels='kmeans',
            n_init=10
        )
        labels = clustering.fit_predict(similarity_matrix)
        
        # –ó–ª–∏–≤–∞—î–º–æ —Å—É—Å—ñ–¥–Ω—ñ —Å–µ–≥–º–µ–Ω—Ç–∏ –æ–¥–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
        segments = []
        current_speaker = None
        current_start = None
        
        for i, (label, (start, end)) in enumerate(zip(labels, timestamps)):
            if label != current_speaker:
                if current_speaker is not None:
                    segments.append({
                        'speaker': int(current_speaker),
                        'start': round(current_start, 2),
                        'end': round(timestamps[i-1][1], 2)
                    })
                current_speaker = label
                current_start = start
        
        if current_speaker is not None:
            segments.append({
                'speaker': int(current_speaker),
                'start': round(current_start, 2),
                'end': round(timestamps[-1][1], 2)
            })
        
        print(f"‚úÖ Created {len(segments)} diarization segments")
        return segments
    
    except Exception as e:
        print(f"‚ùå Error in diarize_audio: {e}")
        import traceback
        traceback.print_exc()
        return []

def transcribe_audio(audio_path, language=None):
    """–¢—Ä–∞–Ω—Å–∫—Ä–∏–±—É—î –∞—É–¥—ñ–æ –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é Whisper"""
    print(f"üìù Transcribing {audio_path}...")
    try:
        result = whisper_model.transcribe(
            audio_path,
            language=language,
            task="transcribe",
            beam_size=3,
            fp16=torch.cuda.is_available(),
            verbose=True
        )
        
        transcription = result['text']
        segments = result['segments']
        words = []
        
        for seg in segments:
            for word_info in seg.get('words', []):
                words.append({
                    'word': word_info['word'],
                    'start': word_info['start'],
                    'end': word_info['end']
                })
        
        print(f"‚úÖ Transcription: {len(transcription)} chars, {len(segments)} segments")
        return transcription, segments, words
    
    except Exception as e:
        print(f"‚ùå Error in transcribe_audio: {e}")
        import traceback
        traceback.print_exc()
        return None, [], []

def extract_single_speaker_audio(audio_path, speaker_segments, output_path):
    """–í–∏—Ç—è–≥—É—î —Å–µ–≥–º–µ–Ω—Ç–∏ –æ–¥–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ –∑ –∞—É–¥—ñ–æ —Ñ–∞–π–ª—É"""
    try:
        audio, sr = librosa.load(audio_path, sr=None)
        duration = len(audio) / sr
        
        speaker_audio_segments = []
        for seg in speaker_segments:
            start_time = max(0, seg['start'])
            end_time = min(duration, seg['end'])
            start_sample = int(start_time * sr)
            end_sample = int(end_time * sr)
            
            if start_sample < len(audio) and end_sample <= len(audio) and start_sample < end_sample:
                segment_audio = audio[start_sample:end_sample]
                speaker_audio_segments.append(segment_audio)
        
        if not speaker_audio_segments:
            return None
        
        combined_audio = np.concatenate(speaker_audio_segments)
        sf.write(output_path, combined_audio, sr)
        
        print(f"‚úÖ Extracted speaker audio: {len(combined_audio)/sr:.2f}s ‚Üí {output_path}")
        return output_path
    
    except Exception as e:
        print(f"‚ùå Error in extract_single_speaker_audio: {e}")
        import traceback
        traceback.print_exc()
        return None

def format_dialogue(segments, speaker_label_prefix="Speaker"):
    """–§–æ—Ä–º–∞—Ç—É—î —Å–µ–≥–º–µ–Ω—Ç–∏ —É —á–∏—Ç–∞–±–µ–ª—å–Ω–∏–π –¥—ñ–∞–ª–æ–≥"""
    lines = []
    for seg in segments:
        start_time = seg.get('start', 0)
        minutes = int(start_time // 60)
        seconds = int(start_time % 60)
        time_str = f"{minutes:02d}:{seconds:02d}"
        
        speaker = seg.get('speaker', 0)
        text = seg.get('text', '').strip()
        
        if text:
            lines.append(f"{time_str} {speaker_label_prefix} {speaker}: {text}")
    
    return "\n".join(lines)

def main():
    # –®–ª—è—Ö–∏
    input_file = "audio examples/Screen Recording 2025-12-05 at 07.29.15.m4a"
    output_dir = "Audio Examples/detecting main speakers"
    
    # –°—Ç–≤–æ—Ä—é—î–º–æ –≤–∏—Ö—ñ–¥–Ω—É –ø–∞–ø–∫—É
    os.makedirs(output_dir, exist_ok=True)
    
    print("=" * 60)
    print("üéµ Processing audio example")
    print("=" * 60)
    
    # 1. –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –º–æ–¥–µ–ª—ñ
    load_models()
    
    # 2. –î—ñ–∞—Ä–∏–∑–∞—Ü—ñ—è
    print("\nüìä Step 1: Diarization...")
    embeddings, timestamps = extract_speaker_embeddings(input_file)
    if embeddings is None:
        print("‚ùå Failed to extract embeddings")
        return
    
    diarization_segments = diarize_audio(embeddings, timestamps, num_speakers=2)
    if not diarization_segments:
        print("‚ùå Diarization failed")
        return
    
    # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω—ñ segments
    original_diarization_segments = copy.deepcopy(diarization_segments)
    
    # 3. –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—è –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–æ–≥–æ —Ñ–∞–π–ª—É
    print("\nüìù Step 2: Transcribing original file...")
    original_transcription, original_segments, original_words = transcribe_audio(input_file)
    
    # –û–±'—î–¥–Ω—É—î–º–æ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—é –∑ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—î—é
    print("\nüîó Step 3: Combining diarization with transcription...")
    combined_segments = []
    
    for diar_seg in diarization_segments:
        # –ó–Ω–∞—Ö–æ–¥–∏–º–æ —Å–ª–æ–≤–∞, —è–∫—ñ –ø–æ—Ç—Ä–∞–ø–ª—è—é—Ç—å –≤ —Ü–µ–π —Å–µ–≥–º–µ–Ω—Ç
        segment_words = []
        for word in original_words:
            word_start = word['start']
            word_end = word['end']
            if word_start >= diar_seg['start'] and word_end <= diar_seg['end']:
                segment_words.append(word['word'])
        
        text = ' '.join(segment_words).strip()
        if not text:
            # –Ø–∫—â–æ –Ω–µ–º–∞—î —Å–ª—ñ–≤, —à—É–∫–∞—î–º–æ –Ω–∞–π–±–ª–∏–∂—á–∏–π —Å–µ–≥–º–µ–Ω—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó
            for seg in original_segments:
                seg_start = seg.get('start', 0)
                seg_end = seg.get('end', 0)
                if seg_start >= diar_seg['start'] and seg_end <= diar_seg['end']:
                    text = seg.get('text', '').strip()
                    break
        
        combined_segments.append({
            'speaker': diar_seg['speaker'],
            'start': diar_seg['start'],
            'end': diar_seg['end'],
            'text': text
        })
    
    # 4. –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –Ω–∞ –æ–¥–Ω–æ–≥–æ–ª–æ—Å—ñ —Ñ–∞–π–ª–∏
    print("\nüîÄ Step 4: Splitting into single-speaker files...")
    speakers_segments = {}
    for seg in original_diarization_segments:
        speaker = seg.get('speaker', 0)
        if speaker not in speakers_segments:
            speakers_segments[speaker] = []
        speakers_segments[speaker].append(seg)
    
    speaker_files = {}
    for speaker, segments in speakers_segments.items():
        segments_sorted = sorted(segments, key=lambda x: x['start'])
        output_path = os.path.join(output_dir, f"speaker_{speaker}.wav")
        extract_single_speaker_audio(input_file, segments_sorted, output_path)
        speaker_files[speaker] = {
            'path': output_path,
            'segments': segments_sorted
        }
    
    # 5. –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—è –æ–¥–Ω–æ–≥–æ–ª–æ—Å–∏—Ö —Ñ–∞–π–ª—ñ–≤
    print("\nüìù Step 5: Transcribing single-speaker files...")
    speaker_transcriptions = {}
    for speaker, file_info in speaker_files.items():
        print(f"\nüé§ Transcribing speaker {speaker}...")
        transcription, segments, words = transcribe_audio(file_info['path'])
        
        if transcription:
            # –û–±'—î–¥–Ω—É—î–º–æ –∑ —Ç–∞–π–º—Å—Ç–µ–º–ø–∞–º–∏ –∑ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó
            speaker_combined = []
            diar_segments = file_info['segments']
            
            # –í–∏–∑–Ω–∞—á–∞—î–º–æ offset –¥–ª—è —Ç–∞–π–º—Å—Ç–µ–º–ø—ñ–≤ (–ø–µ—Ä—à–∏–π —Å–µ–≥–º–µ–Ω—Ç –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó)
            first_diar_start = diar_segments[0]['start'] if diar_segments else 0
            
            # –ú–∞–ø—ñ–º–æ —Å–ª–æ–≤–∞ –∑ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó –Ω–∞ —Å–µ–≥–º–µ–Ω—Ç–∏ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó
            for diar_seg in diar_segments:
                # –ó–Ω–∞—Ö–æ–¥–∏–º–æ —Å–ª–æ–≤–∞, —è–∫—ñ –ø–æ—Ç—Ä–∞–ø–ª—è—é—Ç—å –≤ —Ü–µ–π —Å–µ–≥–º–µ–Ω—Ç
                segment_words = []
                for word in words:
                    # –¢–∞–π–º—Å—Ç–µ–º–ø–∏ –≤ –æ–¥–Ω–æ–≥–æ–ª–æ—Å–æ–º—É —Ñ–∞–π–ª—ñ –≤—ñ–¥–Ω–æ—Å–Ω—ñ (–ø–æ—á–∏–Ω–∞—é—Ç—å—Å—è –∑ 0)
                    # –ü–æ—Ç—Ä—ñ–±–Ω–æ –ø–µ—Ä–µ—Ç–≤–æ—Ä–∏—Ç–∏ —ó—Ö –Ω–∞ –∞–±—Å–æ–ª—é—Ç–Ω—ñ —Ç–∞–π–º—Å—Ç–µ–º–ø–∏
                    word_start_absolute = first_diar_start + word['start']
                    word_end_absolute = first_diar_start + word['end']
                    
                    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —Å–ª–æ–≤–æ –ø–æ—Ç—Ä–∞–ø–ª—è—î –≤ —Å–µ–≥–º–µ–Ω—Ç –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó
                    if word_start_absolute >= diar_seg['start'] and word_end_absolute <= diar_seg['end']:
                        segment_words.append(word['word'])
                
                text = ' '.join(segment_words).strip()
                
                # –Ø–∫—â–æ –Ω–µ–º–∞—î —Å–ª—ñ–≤, —à—É–∫–∞—î–º–æ –Ω–∞–π–±–ª–∏–∂—á–∏–π —Å–µ–≥–º–µ–Ω—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó
                if not text and segments:
                    for seg in segments:
                        seg_start_absolute = first_diar_start + seg.get('start', 0)
                        seg_end_absolute = first_diar_start + seg.get('end', 0)
                        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –ø–µ—Ä–µ–∫—Ä–∏—Ç—Ç—è
                        overlap_start = max(seg_start_absolute, diar_seg['start'])
                        overlap_end = min(seg_end_absolute, diar_seg['end'])
                        if overlap_end > overlap_start:
                            text = seg.get('text', '').strip()
                            break
                
                if text:  # –î–æ–¥–∞—î–º–æ —Ç—ñ–ª—å–∫–∏ —è–∫—â–æ —î —Ç–µ–∫—Å—Ç
                    speaker_combined.append({
                        'speaker': speaker,
                        'start': diar_seg['start'],
                        'end': diar_seg['end'],
                        'text': text
                    })
            
            speaker_transcriptions[speaker] = {
                'transcription': transcription,
                'segments': speaker_combined
            }
    
    # 6. –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
    print("\nüíæ Step 6: Saving results...")
    
    # –û—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç
    original_dialogue = format_dialogue(combined_segments)
    original_path = os.path.join(output_dir, "original_dialogue.txt")
    with open(original_path, 'w', encoding='utf-8') as f:
        f.write(original_dialogue)
    print(f"‚úÖ Saved: {original_path}")
    
    # –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∏ –æ–¥–Ω–æ–≥–æ–ª–æ—Å–∏—Ö —Ñ–∞–π–ª—ñ–≤
    for speaker, info in speaker_transcriptions.items():
        dialogue = format_dialogue(info['segments'], speaker_label_prefix="Speaker")
        transcript_path = os.path.join(output_dir, f"speaker_{speaker}_transcript.txt")
        with open(transcript_path, 'w', encoding='utf-8') as f:
            f.write(dialogue)
        print(f"‚úÖ Saved: {transcript_path}")
    
    # JSON –∑ –º–µ—Ç–∞–¥–∞–Ω–∏–º–∏
    metadata = {
        'original_file': input_file,
        'num_speakers': len(speaker_files),
        'original_diarization_segments': original_diarization_segments,
        'combined_segments': combined_segments,
        'speaker_files': {
            speaker: {
                'path': info['path'],
                'num_segments': len(info['segments']),
                'total_duration': sum(seg['end'] - seg['start'] for seg in info['segments'])
            }
            for speaker, info in speaker_files.items()
        }
    }
    
    metadata_path = os.path.join(output_dir, "metadata.json")
    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    print(f"‚úÖ Saved: {metadata_path}")
    
    print("\n" + "=" * 60)
    print("‚úÖ Processing completed!")
    print("=" * 60)
    print(f"\nüìÅ Results saved in: {output_dir}")
    print("\nFiles created:")
    print(f"  - speaker_0.wav")
    print(f"  - speaker_1.wav")
    print(f"  - original_dialogue.txt")
    print(f"  - speaker_0_transcript.txt")
    print(f"  - speaker_1_transcript.txt")
    print(f"  - metadata.json")

if __name__ == "__main__":
    main()

