#!/usr/bin/env python3
"""
Flask —Å–µ—Ä–≤–µ—Ä –¥–ª—è iOS Shortcuts: SpeechBrain –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—è + Whisper —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—è
–û–∫—Ä–µ–º–∏–π —Ñ–∞–π–ª –¥–ª—è —É–Ω–∏–∫–Ω–µ–Ω–Ω—è –∫–æ–Ω—Ñ–ª—ñ–∫—Ç—ñ–≤ –∑ —ñ–Ω—à–∏–º–∏ –ø—Ä–æ—Ü–µ—Å–∞–º–∏
"""

import os
import sys
import json
import base64
import numpy as np
import torch
import librosa
import soundfile as sf
from flask import Flask, request, jsonify, send_file, send_from_directory
import time
from werkzeug.utils import secure_filename
import threading
import uuid
from datetime import datetime, timedelta

# –ü–∞—Ç—á –¥–ª—è torchaudio —Å—É–º—ñ—Å–Ω–æ—Å—Ç—ñ –∑ speechbrain (–∑–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –î–û —ñ–º–ø–æ—Ä—Ç—É speechbrain)
exec(open('patch_torchaudio.py').read())

from speechbrain.pretrained import SpeakerRecognition
from sklearn.cluster import SpectralClustering
from scipy.spatial.distance import pdist, squareform
import whisper
import warnings
from pathlib import Path
import requests

warnings.filterwarnings("ignore")

app = Flask(__name__)

# Middleware –¥–ª—è –≥–∞—Ä–∞–Ω—Ç—ñ—ó –ø—Ä–∞–≤–∏–ª—å–Ω–∏—Ö CORS –∑–∞–≥–æ–ª–æ–≤–∫—ñ–≤ –¥–ª—è –≤—Å—ñ—Ö –∑–∞–ø–∏—Ç—ñ–≤ –∑ –±—Ä–∞—É–∑–µ—Ä–∞
@app.after_request
def after_request(response):
    """–î–æ–¥–∞—î CORS –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–æ –≤—Å—ñ—Ö –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π"""
    # –í—Å—Ç–∞–Ω–æ–≤–ª—é—î–º–æ –∑–∞–≥–æ–ª–æ–≤–∫–∏ —Ç—ñ–ª—å–∫–∏ —è–∫—â–æ —ó—Ö —â–µ –Ω–µ–º–∞—î (—â–æ–± —É–Ω–∏–∫–Ω—É—Ç–∏ –¥—É–±–ª—é–≤–∞–Ω–Ω—è)
    if 'Access-Control-Allow-Origin' not in response.headers:
        response.headers['Access-Control-Allow-Origin'] = '*'
    if 'Access-Control-Allow-Methods' not in response.headers:
        response.headers['Access-Control-Allow-Methods'] = 'POST'
    if 'Access-Control-Allow-Headers' not in response.headers:
        response.headers['Access-Control-Allow-Headers'] = 'Content-Type'
    return response

# –û–±—Ä–æ–±–∫–∞ OPTIONS –∑–∞–ø–∏—Ç—ñ–≤ (preflight)
@app.before_request
def handle_preflight():
    """–û–±—Ä–æ–±–∫–∞ preflight OPTIONS –∑–∞–ø–∏—Ç—ñ–≤"""
    if request.method == "OPTIONS":
        response = jsonify({})
        response.headers['Access-Control-Allow-Origin'] = '*'
        # –î–ª—è preflight –ø–æ—Ç—Ä—ñ–±–Ω–æ –ø–æ–≤–µ—Ä—Ç–∞—Ç–∏ OPTIONS, –∞–ª–µ —Ç–∞–∫–æ–∂ POST –¥–ª—è –¥–æ–∑–≤–æ–ª—É
        response.headers['Access-Control-Allow-Methods'] = 'POST, OPTIONS'
        response.headers['Access-Control-Allow-Headers'] = 'Content-Type'
        return response

app.config['MAX_CONTENT_LENGTH'] = 100 * 1024 * 1024  # 100 MB max file size

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç–∏ –¥–ª—è iOS Shortcuts API
MAX_FILE_SIZE = 100 * 1024 * 1024  # 100 MB
ALLOWED_EXTENSIONS = {'wav', 'mp3', 'm4a', 'flac', 'ogg', 'aac'}
PROCESSING_TIMEOUT = 300  # 5 —Ö–≤–∏–ª–∏–Ω

# –î–æ–∑–≤–æ–ª–∏ –¥–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω—å
UPLOAD_FOLDER = 'temp_uploads'
os.makedirs(UPLOAD_FOLDER, exist_ok=True)

# –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞ –æ–±—Ä–æ–±–∫–∞: —Å–ª–æ–≤–Ω–∏–∫ –¥–ª—è –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è —Å—Ç–∞—Ç—É—Å—ñ–≤ –∑–∞–≤–¥–∞–Ω—å
jobs = {}  # {job_id: {'status': 'pending'|'processing'|'completed'|'failed', 'result': {...}, 'error': '...', 'created_at': datetime}}
jobs_lock = threading.Lock()

# –û—á–∏—â–µ–Ω–Ω—è —Å—Ç–∞—Ä–∏—Ö –∑–∞–≤–¥–∞–Ω—å (—Å—Ç–∞—Ä—ñ—à–µ 1 –≥–æ–¥–∏–Ω–∏)
def cleanup_old_jobs():
    """–§–æ–Ω–æ–≤–∏–π –ø–æ—Ç—ñ–∫ –¥–ª—è –æ—á–∏—â–µ–Ω–Ω—è —Å—Ç–∞—Ä–∏—Ö –∑–∞–≤–¥–∞–Ω—å"""
    while True:
        time.sleep(3600)  # –ö–æ–∂–Ω—É –≥–æ–¥–∏–Ω—É
        with jobs_lock:
            now = datetime.now()
            expired = [job_id for job_id, job in jobs.items() 
                      if now - job['created_at'] > timedelta(hours=1)]
            for job_id in expired:
                del jobs[job_id]
                print(f"üßπ Cleaned up expired job: {job_id}")

# –ó–∞–ø—É—Å–∫–∞—î–º–æ –æ—á–∏—â–µ–Ω–Ω—è –≤ —Ñ–æ–Ω—ñ
cleanup_thread = threading.Thread(target=cleanup_old_jobs, daemon=True)
cleanup_thread.start()

# –ì–ª–æ–±–∞–ª—å–Ω—ñ –∑–º—ñ–Ω–Ω—ñ –¥–ª—è –º–æ–¥–µ–ª–µ–π (–∑–∞–≤–∞–Ω—Ç–∞–∂—É—é—Ç—å—Å—è –æ–¥–∏–Ω —Ä–∞–∑)
speaker_model = None
whisper_model = None

def load_models():
    """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –º–æ–¥–µ–ª—ñ SpeechBrain —Ç–∞ Whisper –æ–¥–∏–Ω —Ä–∞–∑ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç—ñ"""
    global speaker_model, whisper_model
    
    if speaker_model is None:
        print("üîÑ Loading SpeechBrain speaker recognition model...")
        try:
            # –°–ø—Ä–æ–±—É—î–º–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –∑ –ª–æ–∫–∞–ª—å–Ω–æ—ó –ø–∞–ø–∫–∏
            model_path = "pretrained_models/spkrec-ecapa-voxceleb"
            if os.path.exists(model_path) and os.path.exists(os.path.join(model_path, "hyperparams.yaml")):
                print(f"üìÇ Loading from local directory: {model_path}")
                speaker_model = SpeakerRecognition.from_hparams(
                    source=model_path,
                    savedir=model_path
                )
            else:
                # –Ø–∫—â–æ –ª–æ–∫–∞–ª—å–Ω–æ—ó –º–æ–¥–µ–ª—ñ –Ω–µ–º–∞—î, –∑–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –∑ HuggingFace
                print("üåê Loading from HuggingFace...")
                speaker_model = SpeakerRecognition.from_hparams(
                    source="speechbrain/spkrec-ecapa-voxceleb",
                    savedir="pretrained_models/spkrec-ecapa-voxceleb"
                )
            print("‚úÖ SpeechBrain model loaded successfully!")
        except Exception as e:
            print(f"‚ùå Error loading SpeechBrain model: {e}")
            import traceback
            traceback.print_exc()
            raise
    
    if whisper_model is None:
        print("üîÑ Loading Whisper model...")
        try:
            # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ large-v3 - –Ω–∞–π–ø–æ—Ç—É–∂–Ω—ñ—à—É –º–æ–¥–µ–ª—å Whisper –¥–ª—è –∫—Ä–∞—â–æ—ó —è–∫–æ—Å—Ç—ñ
            # –ü—ñ–¥—Ç—Ä–∏–º—É—î—Ç—å—Å—è –Ω–∞ —Å–∏—Å—Ç–µ–º–∞—Ö –∑ 48 –ì–ë+ –û–ó–£
            # –ú–æ–¥–µ–ª—å –∑–∞–π–º–∞—î ~3-4 –ì–ë –≤ –ø–∞–º'—è—Ç—ñ
            model_size = os.environ.get('WHISPER_MODEL_SIZE', 'small')
            
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ –º–æ–¥–µ–ª—å –≤–∂–µ –≤ –∫–µ—à—ñ
            cache_dir = os.path.expanduser("~/.cache/whisper")
            model_path = os.path.join(cache_dir, f"{model_size}.pt")
            
            if os.path.exists(model_path):
                file_size_mb = os.path.getsize(model_path) / (1024 * 1024)
                print(f"üì¶ Loading Whisper {model_size} model from cache ({file_size_mb:.1f} MB)...")
                print(f"   üìÇ Cache location: {model_path}")
            else:
                print(f"üì¶ Loading Whisper {model_size} model (downloading to cache first time)...")
            
            # Whisper –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –∫–µ—à –∑ ~/.cache/whisper/
            whisper_model = whisper.load_model(model_size, download_root=cache_dir)
            print(f"‚úÖ Whisper model ({model_size}) loaded successfully!")
            print(f"   üíæ Model size: ~3-4 GB in memory")
        except Exception as e:
            print(f"‚ùå Error loading Whisper model: {e}")
            print(f"   üí° If you have less RAM, try: WHISPER_MODEL_SIZE=medium")
            raise

# –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –º–æ–¥–µ–ª—ñ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç—ñ –≤ –æ–∫—Ä–µ–º–æ–º—É –ø–æ—Ç–æ—Ü—ñ (—â–æ–± –Ω–µ –±–ª–æ–∫—É–≤–∞—Ç–∏ –∑–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞)
def load_models_background():
    """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –º–æ–¥–µ–ª—ñ –≤ —Ñ–æ–Ω—ñ"""
    import sys
    try:
        print("üîÑ Starting background model loading...", flush=True)
        sys.stdout.flush()
        load_models()
        print("‚úÖ All models loaded successfully in background!", flush=True)
        sys.stdout.flush()
    except Exception as e:
        print(f"‚ö†Ô∏è  Warning: Could not load models at startup: {e}", flush=True)
        print("   Models will be loaded on first request", flush=True)
        import traceback
        traceback.print_exc()
        sys.stdout.flush()

# –ó–∞–ø—É—Å–∫–∞—î–º–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª–µ–π –≤ —Ñ–æ–Ω—ñ
print("üöÄ Starting model loading thread...", flush=True)
model_loading_thread = threading.Thread(target=load_models_background, daemon=True)
model_loading_thread.start()


def extract_speaker_embeddings(audio_path, segment_duration=1.5, overlap=0.5):
    """
    –í–∏—Ç—è–≥—É—î –µ–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–ø—ñ–∫–µ—Ä–∞ –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ –∞—É–¥—ñ–æ.
    
    Args:
        audio_path: —à–ª—è—Ö –¥–æ –∞—É–¥—ñ–æ—Ñ–∞–π–ª—É
        segment_duration: –¥–æ–≤–∂–∏–Ω–∞ —Å–µ–≥–º–µ–Ω—Ç–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        overlap: –ø–µ—Ä–µ–∫—Ä–∏—Ç—Ç—è –º—ñ–∂ —Å–µ–≥–º–µ–Ω—Ç–∞–º–∏ (0-1)
    
    Returns:
        embeddings: –º–∞—Ç—Ä–∏—Ü—è –µ–º–±–µ–¥–∏–Ω–≥—ñ–≤ (N, 192)
        timestamps: —Å–ø–∏—Å–æ–∫ (start, end) –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞
    """
    global speaker_model
    
    if speaker_model is None:
        load_models()
    
    try:
        # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –∞—É–¥—ñ–æ
        print(f"üìÇ Loading audio from: {audio_path}")
        import sys
        sys.stdout.flush()
        
        audio, sr = librosa.load(audio_path, sr=16000, mono=True)
        duration = librosa.get_duration(y=audio, sr=sr)
        print(f"‚è±Ô∏è  Audio duration: {duration:.2f} seconds, sample rate: {sr} Hz, samples: {len(audio)}")
        sys.stdout.flush()
        
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –º—ñ–Ω—ñ–º–∞–ª—å–Ω–æ—ó –¥–æ–≤–∂–∏–Ω–∏
        min_duration = 0.5  # –ú—ñ–Ω—ñ–º—É–º 0.5 —Å–µ–∫—É–Ω–¥–∏
        if duration < min_duration:
            print(f"‚ö†Ô∏è  Audio too short ({duration:.2f}s < {min_duration}s), using entire audio as single segment")
            # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –≤—Å–µ –∞—É–¥—ñ–æ —è–∫ –æ–¥–∏–Ω —Å–µ–≥–º–µ–Ω—Ç
            embedding = None
            try:
                # –°–ø—Ä–æ–±—É—î–º–æ —á–µ—Ä–µ–∑ –ø—Ä—è–º–∏–π –¥–æ—Å—Ç—É–ø –¥–æ embedding_model
                segment_tensor = torch.tensor(audio, dtype=torch.float32).unsqueeze(0)  # [1, samples]
                wav_lens = torch.tensor([duration], dtype=torch.float32)
                
                with torch.no_grad():
                    if hasattr(speaker_model, 'mods') and hasattr(speaker_model.mods, 'encoder'):
                        features = speaker_model.mods.encoder(segment_tensor, wav_lens=wav_lens)
                        if hasattr(speaker_model.mods, 'embedding_model'):
                            embedding = speaker_model.mods.embedding_model(features, wav_lens=wav_lens)
                        else:
                            embedding = features
                        embedding = embedding.squeeze().cpu().detach().numpy()
            except Exception as e1:
                try:
                    # Fallback –¥–æ encode_batch
                    segment_tensor = torch.tensor(audio, dtype=torch.float32).unsqueeze(0).unsqueeze(0)
                    embedding = speaker_model.encode_batch(segment_tensor).squeeze().cpu().detach().numpy()
                except Exception as e2:
                    print(f"‚ùå Error processing short audio: {e1}, {e2}")
                    return None, []
            
            if embedding is not None and len(embedding) > 0:
                return np.array([embedding]), [(0.0, duration)]
            else:
                return None, []
        
        embeddings = []
        timestamps = []
        
        # –ö–æ–≤–∑–Ω—ñ –≤—ñ–∫–Ω–∞
        segment_samples = int(segment_duration * sr)
        stride_samples = int(segment_duration * (1 - overlap) * sr)
        
        print(f"üîç Processing with segment_duration={segment_duration}s, overlap={overlap}, segment_samples={segment_samples}, stride_samples={stride_samples}")
        
        # –Ø–∫—â–æ –∞—É–¥—ñ–æ –∫–æ—Ä–æ—Ç—à–µ –∑–∞ –æ–¥–∏–Ω —Å–µ–≥–º–µ–Ω—Ç, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –≤—Å–µ –∞—É–¥—ñ–æ
        if len(audio) < segment_samples:
            print(f"‚ö†Ô∏è  Audio shorter than segment ({len(audio)} < {segment_samples}), using entire audio")
            embedding = None
            try:
                # –°–ø—Ä–æ–±—É—î–º–æ —á–µ—Ä–µ–∑ –ø—Ä—è–º–∏–π –¥–æ—Å—Ç—É–ø –¥–æ embedding_model
                segment_tensor = torch.tensor(audio, dtype=torch.float32).unsqueeze(0)  # [1, samples]
                wav_lens = torch.tensor([duration], dtype=torch.float32)
                
                with torch.no_grad():
                    if hasattr(speaker_model, 'mods') and hasattr(speaker_model.mods, 'encoder'):
                        features = speaker_model.mods.encoder(segment_tensor, wav_lens=wav_lens)
                        if hasattr(speaker_model.mods, 'embedding_model'):
                            embedding = speaker_model.mods.embedding_model(features, wav_lens=wav_lens)
                        else:
                            embedding = features
                        embedding = embedding.squeeze().cpu().detach().numpy()
            except Exception as e1:
                try:
                    # Fallback –¥–æ encode_batch
                    segment_tensor = torch.tensor(audio, dtype=torch.float32).unsqueeze(0).unsqueeze(0)
                    embedding = speaker_model.encode_batch(segment_tensor).squeeze().cpu().detach().numpy()
                except Exception as e2:
                    print(f"‚ùå Error processing short audio segment: {e1}, {e2}")
                    return None, []
            
            if embedding is not None and len(embedding) > 0:
                return np.array([embedding]), [(0.0, duration)]
            else:
                return None, []
        
        # –û–±—Ä–æ–±–∫–∞ —Å–µ–≥–º–µ–Ω—Ç—ñ–≤
        max_start = len(audio) - segment_samples
        if max_start < 0:
            max_start = 0
        
        segments_processed = 0
        for start_sample in range(0, max_start + 1, stride_samples):
            end_sample = min(start_sample + segment_samples, len(audio))
            segment = audio[start_sample:end_sample]
            
            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞, —â–æ —Å–µ–≥–º–µ–Ω—Ç –Ω–µ –ø–æ—Ä–æ–∂–Ω—ñ–π
            if len(segment) == 0:
                continue
            
            # –í–∏—Ç—è–≥—É—î–º–æ –µ–º–±–µ–¥–¥–∏–Ω–≥ —á–µ—Ä–µ–∑ SpeechBrain
            # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ classify_file –∑ —Ç–∏–º—á–∞—Å–æ–≤–∏–º —Ñ–∞–π–ª–æ–º (–Ω–∞–π–Ω–∞–¥—ñ–π–Ω—ñ—à–∏–π –º–µ—Ç–æ–¥)
            embedding = None
            tmp_path = None
            
            try:
                # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ encode_batch –Ω–∞–ø—Ä—è–º—É –∑ —Ç–µ–Ω–∑–æ—Ä–æ–º (–æ–±—Ö–æ–¥–∏–º–æ torchaudio/torchcodec)
                # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ —Å–µ–≥–º–µ–Ω—Ç —É —Ç–µ–Ω–∑–æ—Ä —É –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—ñ –¥–ª—è SpeechBrain
                segment_tensor = torch.tensor(segment, dtype=torch.float32).unsqueeze(0)  # [1, samples]
                
                # SpeechBrain –æ—á—ñ–∫—É—î —Ñ–æ—Ä–º–∞—Ç [batch, channels, samples] –∞–±–æ [batch, samples]
                # –°–ø—Ä–æ–±—É—î–º–æ –æ–±–∏–¥–≤–∞ –≤–∞—Ä—ñ–∞–Ω—Ç–∏
                try:
                    # –í–∞—Ä—ñ–∞–Ω—Ç 1: [1, 1, samples] - –∑ –∫–∞–Ω–∞–ª–æ–º
                    segment_tensor_with_channel = segment_tensor.unsqueeze(0)  # [1, 1, samples]
                    embedding_tensor = speaker_model.encode_batch(segment_tensor_with_channel)
                    embedding = embedding_tensor.squeeze().cpu().detach().numpy()
                except Exception as e1:
                    try:
                        # –í–∞—Ä—ñ–∞–Ω—Ç 2: [1, samples] - –±–µ–∑ –∫–∞–Ω–∞–ª—É
                        embedding_tensor = speaker_model.encode_batch(segment_tensor)
                        embedding = embedding_tensor.squeeze().cpu().detach().numpy()
                    except Exception as e2:
                        # –í–∞—Ä—ñ–∞–Ω—Ç 3: —á–µ—Ä–µ–∑ –ø—Ä—è–º–∏–π –¥–æ—Å—Ç—É–ø –¥–æ encoder
                        try:
                            wav_lens = torch.tensor([len(segment) / sr], dtype=torch.float32)
                            with torch.no_grad():
                                if hasattr(speaker_model, 'mods') and hasattr(speaker_model.mods, 'encoder'):
                                    features = speaker_model.mods.encoder(segment_tensor, wav_lens=wav_lens)
                                    if hasattr(speaker_model.mods, 'embedding_model'):
                                        embedding_tensor = speaker_model.mods.embedding_model(features, wav_lens=wav_lens)
                                    else:
                                        embedding_tensor = features
                                    embedding = embedding_tensor.squeeze().cpu().detach().numpy()
                                else:
                                    raise Exception("No encoder found in model")
                        except Exception as e3:
                            print(f"‚ùå All embedding extraction methods failed: {e1}, {e2}, {e3}")
                            embedding = None
                
                if embedding is not None and len(embedding) > 0:
                    if segments_processed < 3:  # –õ–æ–≥—É—î–º–æ –ø–µ—Ä—à—ñ 3 —É—Å–ø—ñ—à–Ω—ñ
                        print(f"‚úÖ Extracted embedding for segment at {start_sample}, shape: {embedding.shape}")
            except Exception as e:
                if start_sample == 0:
                    print(f"‚ùå Embedding extraction failed for first segment: {e}")
                    import traceback
                    traceback.print_exc()
                embedding = None
            
            if embedding is not None and len(embedding) > 0:
                embeddings.append(embedding)
                
                start_time = start_sample / sr
                end_time = end_sample / sr
                timestamps.append((start_time, min(end_time, duration)))
                segments_processed += 1
                if segments_processed <= 3:  # –õ–æ–≥—É—î–º–æ –ø–µ—Ä—à—ñ 3 —É—Å–ø—ñ—à–Ω—ñ
                    print(f"‚úÖ Extracted embedding for segment at {start_sample}, shape: {embedding.shape}")
            else:
                if start_sample == 0:
                    print(f"‚ö†Ô∏è  No embedding extracted for first segment at {start_sample}")
                continue
        
        print(f"‚úÖ Processed {segments_processed} segments, extracted {len(embeddings)} embeddings")
        sys.stdout.flush()
        
        if len(embeddings) == 0:
            print("‚ùå No embeddings extracted!")
            print(f"   Audio info: duration={duration:.2f}s, samples={len(audio)}, sr={sr}Hz")
            print(f"   Segment params: segment_duration={segment_duration}s, overlap={overlap}")
            print(f"   Segment samples: {segment_samples}, stride_samples={stride_samples}")
            sys.stdout.flush()
            return None, []
        
        print(f"‚úÖ Returning {len(embeddings)} embeddings with shapes: {[e.shape for e in embeddings[:3]]}")
        sys.stdout.flush()
        return np.array(embeddings), timestamps
    
    except Exception as e:
        print(f"‚ùå Error in extract_speaker_embeddings: {e}")
        import traceback
        traceback.print_exc()
        sys.stdout.flush()
        return None, []


def diarize_audio(embeddings, timestamps, num_speakers=None):
    """
    –í–∏–∫–æ–Ω—É—î –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—é —á–µ—Ä–µ–∑ spectral clustering –Ω–∞ –µ–º–±–µ–¥–∏–Ω–≥–∞—Ö.
    
    Args:
        embeddings: –º–∞—Ç—Ä–∏—Ü—è –µ–º–±–µ–¥–∏–Ω–≥—ñ–≤ (N, 192)
        timestamps: —Å–ø–∏—Å–æ–∫ (start, end) –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞
        num_speakers: –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Å–ø—ñ–∫–µ—Ä—ñ–≤ (—è–∫—â–æ None, –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≤–∏–∑–Ω–∞—á–∞—î—Ç—å—Å—è)
    
    Returns:
        segments: —Å–ø–∏—Å–æ–∫ {'speaker': int, 'start': float, 'end': float}
    """
    if embeddings is None:
        print("‚ùå No embeddings provided for diarization")
        return []
    
    if len(embeddings) < 2:
        print(f"‚ö†Ô∏è  Only {len(embeddings)} embedding(s) available, need at least 2 for clustering")
        # –Ø–∫—â–æ —Ç—ñ–ª—å–∫–∏ –æ–¥–∏–Ω —Å–µ–≥–º–µ–Ω—Ç, –ø–æ–≤–µ—Ä—Ç–∞—î–º–æ –π–æ–≥–æ —è–∫ –æ–¥–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
        if len(embeddings) == 1 and timestamps:
            return [{
                'speaker': 0,
                'start': round(timestamps[0][0], 2),
                'end': round(timestamps[0][1], 2)
            }]
        return []
    
    try:
        # –ù–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ –µ–º–±–µ–¥–¥–∏–Ω–≥–∏ (L2 –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è)
        from sklearn.preprocessing import normalize
        embeddings_normalized = normalize(embeddings, norm='l2')
        
        # –û–±—á–∏—Å–ª—é—î–º–æ –∫–æ—Å–∏–Ω—É—Å–Ω—É –≤—ñ–¥—Å—Ç–∞–Ω—å –º—ñ–∂ –µ–º–±–µ–¥–∏–Ω–≥–∞–º–∏
        distances = pdist(embeddings_normalized, metric='cosine')
        distance_matrix = squareform(distances)
        
        # –î—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞: –ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ä–æ–∑–ø–æ–¥—ñ–ª –≤—ñ–¥—Å—Ç–∞–Ω–µ–π
        mean_dist = np.mean(distances)
        std_dist = np.std(distances)
        print(f"üìä Distance stats: mean={mean_dist:.4f}, std={std_dist:.4f}, min={np.min(distances):.4f}, max={np.max(distances):.4f}")
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ similarity matrix –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—ó
        if std_dist < 1e-6:
            print(f"‚ö†Ô∏è  All distances are nearly identical, using uniform similarity")
            similarity_matrix = np.ones_like(distance_matrix) * 0.5
        else:
            # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –∞–¥–∞–ø—Ç–∏–≤–Ω–µ –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è
            scale = mean_dist if mean_dist > 0.01 else 0.1
            similarity_matrix = np.exp(-distance_matrix / scale)
        
        # –í–∏–∑–Ω–∞—á–∞—î–º–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Å–ø—ñ–∫–µ—Ä—ñ–≤ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ, —è–∫—â–æ –Ω–µ –∑–∞–¥–∞–Ω–æ
        if num_speakers is None:
            # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –µ–ª—å–±–æ–≤–∏–π –º–µ—Ç–æ–¥ –¥–ª—è –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ—ó –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤
            from sklearn.metrics import silhouette_score
            
            best_k = 2
            best_score = -1
            
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ k –≤—ñ–¥ 2 –¥–æ min(5, –∫—ñ–ª—å–∫—ñ—Å—Ç—å_—Å–µ–≥–º–µ–Ω—Ç—ñ–≤/3)
            max_k = min(5, max(2, len(embeddings) // 3))
            
            for k in range(2, max_k + 1):
                try:
                    test_clustering = SpectralClustering(
                        n_clusters=k,
                        affinity='precomputed',
                        random_state=42,
                        assign_labels='kmeans',
                        n_init=5
                    )
                    test_labels = test_clustering.fit_predict(similarity_matrix)
                    
                    # –û–±—á–∏—Å–ª—é—î–º–æ silhouette score (–ø–æ—Ç—Ä–µ–±—É—î –ø—Ä–∏–Ω–∞–π–º–Ω—ñ 2 –∫–ª–∞—Å—Ç–µ—Ä–∏)
                    if len(np.unique(test_labels)) > 1:
                        score = silhouette_score(embeddings_normalized, test_labels, metric='cosine')
                        print(f"   k={k}: silhouette_score={score:.4f}")
                        if score > best_score:
                            best_score = score
                            best_k = k
                except Exception as e:
                    print(f"   k={k}: error - {e}")
                    continue
            
            num_speakers = best_k
            print(f"üîç Auto-detected {num_speakers} speakers (best silhouette_score={best_score:.4f})")
            
            # –ö–†–ò–¢–ò–ß–ù–û: –ù–µ –ø—Ä–∏–º—É—Å–æ–≤–æ –≤—Å—Ç–∞–Ω–æ–≤–ª—é—î–º–æ 2 —Å–ø—ñ–∫–µ—Ä—ñ–≤, —è–∫—â–æ —Ñ–∞–π–ª –æ–¥–Ω–æ–≥–æ–ª–æ—Å–∏–π!
            # –Ø–∫—â–æ –≤—Å—ñ —Å–µ–≥–º–µ–Ω—Ç–∏ –¥—É–∂–µ —Å—Ö–æ–∂—ñ (mean_dist < 0.05), —Ü–µ –æ–∑–Ω–∞—á–∞—î –æ–¥–∏–Ω —Å–ø—ñ–∫–µ—Ä
            if mean_dist < 0.05:
                if num_speakers > 1:
                    print(f"‚ö†Ô∏è  Very low distance ({mean_dist:.4f}), but detected {num_speakers} speakers - likely single speaker, forcing 1")
                    num_speakers = 1
                else:
                    print(f"‚úÖ Very low distance ({mean_dist:.4f}) confirms single speaker")
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ –¥–æ—Å—Ç–∞—Ç–Ω—å–æ —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—ó
        if len(embeddings) < num_speakers:
            print(f"‚ö†Ô∏è  Not enough segments ({len(embeddings)}) for {num_speakers} speakers, using {len(embeddings)}")
            num_speakers = len(embeddings)
        
        # –°–ø—Ä–æ–±—É—î–º–æ —Ä—ñ–∑–Ω—ñ –∞–ª–≥–æ—Ä–∏—Ç–º–∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—ó
        # –°–ø–æ—á–∞—Ç–∫—É Spectral clustering
        labels = None
        try:
            clustering = SpectralClustering(
                n_clusters=num_speakers,
                affinity='precomputed',
                random_state=42,
                assign_labels='kmeans',
                n_init=10  # –ë—ñ–ª—å—à–µ —Å–ø—Ä–æ–± –¥–ª—è –∫—Ä–∞—â–æ—ó —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ
            )
            labels = clustering.fit_predict(similarity_matrix)
            print(f"‚úÖ Used SpectralClustering")
        except Exception as e:
            print(f"‚ö†Ô∏è  Spectral clustering failed: {e}, trying AgglomerativeClustering")
            # Fallback –¥–æ AgglomerativeClustering
            from sklearn.cluster import AgglomerativeClustering
            clustering = AgglomerativeClustering(
                n_clusters=num_speakers,
                linkage='average',
                affinity='precomputed'
            )
            labels = clustering.fit_predict(similarity_matrix)
            print(f"‚úÖ Used AgglomerativeClustering")
        
        if labels is None:
            print("‚ùå Clustering failed completely")
            return []
        
        # –î—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞: –ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ä–æ–∑–ø–æ–¥—ñ–ª –ª–µ–π–±–ª—ñ–≤
        unique_labels, counts = np.unique(labels, return_counts=True)
        print(f"üìä Clustering result: {len(unique_labels)} unique speakers found")
        for label, count in zip(unique_labels, counts):
            print(f"   Speaker {label}: {count} segments ({count/len(labels)*100:.1f}%)")
        
        # –ö–†–ò–¢–ò–ß–ù–û: –Ø–∫—â–æ –æ–¥–∏–Ω —Å–ø—ñ–∫–µ—Ä –∑–∞–π–º–∞—î >90% —Å–µ–≥–º–µ–Ω—Ç—ñ–≤, —Ü–µ –æ–¥–Ω–æ–≥–æ–ª–æ—Å–∏–π —Ñ–∞–π–ª
        # –ê–ª–µ —Ç—ñ–ª—å–∫–∏ —è–∫—â–æ —Å–µ—Ä–µ–¥–Ω—è –≤—ñ–¥—Å—Ç–∞–Ω—å –Ω–µ –¥—É–∂–µ –≤–∏—Å–æ–∫–∞ (—è–∫—â–æ –≤–∏—Å–æ–∫–∞, –º–æ–∂—É—Ç—å –±—É—Ç–∏ —Ä—ñ–∑–Ω—ñ —Å–ø—ñ–∫–µ—Ä–∏)
        if len(unique_labels) > 1:
            max_count = max(counts)
            max_ratio = max_count / len(labels)
            
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ç—Ä–∏–≤–∞–ª—ñ—Å—Ç—å —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ —Å–ø—ñ–∫–µ—Ä–∞ 1
            speaker1_count = counts[1] if len(counts) > 1 else 0
            speaker1_ratio = speaker1_count / len(labels) if len(labels) > 0 else 0
            
            # –Ø–∫—â–æ —Å–ø—ñ–∫–µ—Ä 1 –∑–∞–π–º–∞—î <15% —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ –Ü —Å–µ—Ä–µ–¥–Ω—è –≤—ñ–¥—Å—Ç–∞–Ω—å –≤–∏—Å–æ–∫–∞ (>0.5), —Ü–µ –æ–¥–Ω–æ–≥–æ–ª–æ—Å–∏–π —Ñ–∞–π–ª
            # (–≤–∏—Å–æ–∫–∞ –≤—ñ–¥—Å—Ç–∞–Ω—å –æ–∑–Ω–∞—á–∞—î, —â–æ —Å–µ–≥–º–µ–Ω—Ç–∏ —Ä—ñ–∑–Ω—ñ, –∞–ª–µ —Ü–µ –æ–¥–∏–Ω —Å–ø—ñ–∫–µ—Ä –∑ —Ä—ñ–∑–Ω–∏–º–∏ —ñ–Ω—Ç–æ–Ω–∞—Ü—ñ—è–º–∏)
            if max_ratio > 0.90 and speaker1_ratio < 0.15 and mean_dist > 0.5:
                print(f"‚ö†Ô∏è  One speaker has {max_ratio*100:.1f}% of segments, other has only {speaker1_ratio*100:.1f}% (mean_dist={mean_dist:.4f}) - likely single speaker file, forcing all to speaker 0")
                labels = np.zeros_like(labels)
                unique_labels = [0]
                counts = [len(labels)]
        
        # –Ø–∫—â–æ –≤—Å—ñ —Å–µ–≥–º–µ–Ω—Ç–∏ –æ–¥–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞, —Å–ø—Ä–æ–±—É—î–º–æ —ñ–Ω—à–∏–π –ø—ñ–¥—Ö—ñ–¥ (—Ç—ñ–ª—å–∫–∏ —è–∫—â–æ num_speakers > 1)
        if len(unique_labels) == 1 and num_speakers > 1:
            print(f"‚ö†Ô∏è  All segments assigned to one speaker, trying alternative clustering...")
            # –°–ø—Ä–æ–±—É—î–º–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ –≤—ñ–¥—Å—Ç–∞–Ω—ñ –±–µ–∑–ø–æ—Å–µ—Ä–µ–¥–Ω—å–æ
            from sklearn.cluster import AgglomerativeClustering
            clustering_alt = AgglomerativeClustering(
                n_clusters=num_speakers,
                linkage='ward',
                metric='euclidean'
            )
            labels_alt = clustering_alt.fit_predict(embeddings_normalized)
            unique_alt, counts_alt = np.unique(labels_alt, return_counts=True)
            if len(unique_alt) > 1:
                print(f"‚úÖ Alternative clustering found {len(unique_alt)} speakers")
                labels = labels_alt
                unique_labels, counts = unique_alt, counts_alt
        
        # –ö–†–ò–¢–ò–ß–ù–û: –ó–ª–∏–≤–∞—î–º–æ —Å—É—Å—ñ–¥–Ω—ñ —Å–µ–≥–º–µ–Ω—Ç–∏ –æ–¥–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞, –∞–ª–µ –ù–ï —á–µ—Ä–µ–∑ —ñ–Ω—à—ñ —Å–ø—ñ–∫–µ—Ä–∏
        # –°—Ç–≤–æ—Ä—é—î–º–æ —Å–µ–≥–º–µ–Ω—Ç–∏ –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º —Ç–æ–≥–æ, —â–æ –º—ñ–∂ —Å–µ–≥–º–µ–Ω—Ç–∞–º–∏ –æ–¥–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ –º–æ–∂—É—Ç—å –±—É—Ç–∏ —ñ–Ω—à—ñ —Å–ø—ñ–∫–µ—Ä–∏
        segments = []
        current_speaker = None
        current_start = None
        
        for i, (label, (start, end)) in enumerate(zip(labels, timestamps)):
            if label != current_speaker:
                # –Ø–∫—â–æ –∑–º—ñ–Ω–∏–≤—Å—è —Å–ø—ñ–∫–µ—Ä, –∑–±–µ—Ä—ñ–≥–∞—î–º–æ –ø–æ—Ç–æ—á–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç
                if current_speaker is not None:
                    segments.append({
                        'speaker': int(current_speaker),
                        'start': round(current_start, 2),
                        'end': round(timestamps[i-1][1], 2)
                    })
                # –ü–æ—á–∏–Ω–∞—î–º–æ –Ω–æ–≤–∏–π —Å–µ–≥–º–µ–Ω—Ç
                current_speaker = label
                current_start = start
            # –Ø–∫—â–æ —Å–ø—ñ–∫–µ—Ä —Ç–æ–π —Å–∞–º–∏–π, –ø—Ä–æ–¥–æ–≤–∂—É—î–º–æ —Å–µ–≥–º–µ–Ω—Ç (–Ω–µ –∑–±–µ—Ä—ñ–≥–∞—î–º–æ, –ø–æ–∫–∏ –Ω–µ –∑–º—ñ–Ω–∏—Ç—å—Å—è)
        
        # –î–æ–¥–∞—î–º–æ –æ—Å—Ç–∞–Ω–Ω—ñ–π —Å–µ–≥–º–µ–Ω—Ç
        if current_speaker is not None:
            segments.append({
                'speaker': int(current_speaker),
                'start': round(current_start, 2),
                'end': round(timestamps[-1][1], 2)
            })
        
        # –ö–†–ò–¢–ò–ß–ù–û: –í–∏–ø—Ä–∞–≤–ª—è—î–º–æ –ø–µ—Ä–µ–∫—Ä–∏—Ç—Ç—è —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ —Ä—ñ–∑–Ω–∏—Ö —Å–ø—ñ–∫–µ—Ä—ñ–≤
        # –ü—Ä–æ—Å—Ç–∏–π –ø—ñ–¥—Ö—ñ–¥: —è–∫—â–æ —Å–µ–≥–º–µ–Ω—Ç–∏ –ø–µ—Ä–µ–∫—Ä–∏–≤–∞—é—Ç—å—Å—è, —Ä–æ–∑–±–∏–≤–∞—î–º–æ —ó—Ö –Ω–∞ –º–µ–∂—ñ –ø–µ—Ä–µ–∫—Ä–∏—Ç—Ç—è
        # –ê–ª–µ –ù–ï –¥–æ–¥–∞—î–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω—ñ —Å–µ–≥–º–µ–Ω—Ç–∏ –∑–Ω–æ–≤—É - —Ü–µ —Å—Ç–≤–æ—Ä—é—î –¥—É–±–ª—ñ–∫–∞—Ç–∏!
        
        # –°–ø–æ—á–∞—Ç–∫—É –∑–±–∏—Ä–∞—î–º–æ –≤—Å—ñ —Ç–æ—á–∫–∏ –ø–µ—Ä–µ—Ç–∏–Ω—É
        split_points = set()
        for i, seg1 in enumerate(segments):
            for j, seg2 in enumerate(segments):
                if i != j and seg1['speaker'] != seg2['speaker']:
                    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –ø–µ—Ä–µ–∫—Ä–∏—Ç—Ç—è
                    if seg1['start'] < seg2['end'] and seg1['end'] > seg2['start']:
                        # –î–æ–¥–∞—î–º–æ —Ç–æ—á–∫–∏ –ø–æ—á–∞—Ç–∫—É —Ç–∞ –∫—ñ–Ω—Ü—è –ø–µ—Ä–µ–∫—Ä–∏—Ç—Ç—è
                        overlap_start = max(seg1['start'], seg2['start'])
                        overlap_end = min(seg1['end'], seg2['end'])
                        split_points.add(round(overlap_start, 2))
                        split_points.add(round(overlap_end, 2))
        
        # –Ø–∫—â–æ —î —Ç–æ—á–∫–∏ —Ä–æ–∑–±–∏—Ç—Ç—è, —Ä–æ–∑–±–∏–≤–∞—î–º–æ —Å–µ–≥–º–µ–Ω—Ç–∏
        if split_points:
            split_points = sorted(split_points)
            fixed_segments = []
            
            for seg in segments:
                seg_start = seg['start']
                seg_end = seg['end']
                seg_speaker = seg['speaker']
                
                # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –≤—Å—ñ —Ç–æ—á–∫–∏ —Ä–æ–∑–±–∏—Ç—Ç—è –≤ –º–µ–∂–∞—Ö —Ü—å–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞
                points_in_segment = [p for p in split_points if seg_start < p < seg_end]
                
                if points_in_segment:
                    # –†–æ–∑–±–∏–≤–∞—î–º–æ —Å–µ–≥–º–µ–Ω—Ç –Ω–∞ —á–∞—Å—Ç–∏–Ω–∏
                    all_points = [seg_start] + points_in_segment + [seg_end]
                    for k in range(len(all_points) - 1):
                        part_start = all_points[k]
                        part_end = all_points[k + 1]
                        
                        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —Ü—è —á–∞—Å—Ç–∏–Ω–∞ –Ω–µ –ø–µ—Ä–µ–∫—Ä–∏–≤–∞—î—Ç—å—Å—è –∑ —ñ–Ω—à–∏–º —Å–ø—ñ–∫–µ—Ä–æ–º
                        # –Ø–∫—â–æ –ø–µ—Ä–µ–∫—Ä–∏–≤–∞—î—Ç—å—Å—è –∑–Ω–∞—á–Ω–æ (>50%), –ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ —Ü—é —á–∞—Å—Ç–∏–Ω—É
                        overlaps_with_other = False
                        for other_seg in segments:
                            if other_seg['speaker'] != seg_speaker:
                                if part_start < other_seg['end'] and part_end > other_seg['start']:
                                    overlap_size = min(part_end, other_seg['end']) - max(part_start, other_seg['start'])
                                    part_size = part_end - part_start
                                    if overlap_size > part_size * 0.5:  # –ë—ñ–ª—å—à–µ 50% –ø–µ—Ä–µ–∫—Ä–∏—Ç—Ç—è
                                        overlaps_with_other = True
                                        break
                        
                        if not overlaps_with_other and part_end > part_start:
                            fixed_segments.append({
                                'speaker': seg_speaker,
                                'start': round(part_start, 2),
                                'end': round(part_end, 2)
                            })
                else:
                    # –ù–µ–º–∞—î —Ç–æ—á–æ–∫ —Ä–æ–∑–±–∏—Ç—Ç—è, –¥–æ–¥–∞—î–º–æ —è–∫ —î
                    fixed_segments.append(seg)
            
            # –í–ê–ñ–õ–ò–í–û: –ù–ï –¥–æ–¥–∞—î–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω—ñ —Å–µ–≥–º–µ–Ω—Ç–∏ –∑–Ω–æ–≤—É - —Ü–µ —Å—Ç–≤–æ—Ä—é—î –¥—É–±–ª—ñ–∫–∞—Ç–∏!
            # –í—Å—ñ —Å–µ–≥–º–µ–Ω—Ç–∏ –≤–∂–µ –æ–±—Ä–æ–±–ª–µ–Ω—ñ –≤–∏—â–µ
            segments = fixed_segments
        
        # –°–æ—Ä—Ç—É—î–º–æ –∑–∞ —á–∞—Å–æ–º
        segments = sorted(segments, key=lambda x: x['start'])
        
        print(f"‚úÖ Created {len(segments)} diarization segments")
        return segments
    
    except Exception as e:
        print(f"‚ùå Error in diarize_audio: {e}")
        import traceback
        traceback.print_exc()
        return []


def transcribe_with_speechmatics(audio_path, language='en'):
    """
    –¢—Ä–∞–Ω—Å–∫—Ä–∏–±—É—î –∞—É–¥—ñ–æ –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é Speechmatics API –∑ word timestamps.
    
    Args:
        audio_path: —à–ª—è—Ö –¥–æ –∞—É–¥—ñ–æ—Ñ–∞–π–ª—É
        language: –∫–æ–¥ –º–æ–≤–∏ (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, 'uk', 'en', 'ar')
    
    Returns:
        transcription: —Ç–µ–∫—Å—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó
        segments: —Å–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ –∑ —Ç–µ–∫—Å—Ç–æ–º —Ç–∞ —á–∞—Å–æ–≤–∏–º–∏ –º—ñ—Ç–∫–∞–º–∏
        words: —Å–ø–∏—Å–æ–∫ —Å–ª—ñ–≤ –∑ timestamps –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –º–∞—Ç—á–∏–Ω–≥—É
    """
    import sys
    from transcribe_with_speechmatics import upload_to_speechmatics, poll_speechmatics_job
    
    api_key = os.getenv('SPEECHMATICS_API_KEY')
    if not api_key:
        raise ValueError("SPEECHMATICS_API_KEY environment variable is not set")
    
    print(f"üé§ Transcribing with Speechmatics: {audio_path}")
    print(f"   Language: {language}")
    sys.stdout.flush()
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ —Ñ–∞–π–ª
    job_id = upload_to_speechmatics(api_key, audio_path, language, is_separated_track=True)
    
    # –û—á—ñ–∫—É—î–º–æ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è
    transcript_data = poll_speechmatics_job(api_key, job_id)
    
    # –ü–∞—Ä—Å–∏–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    words = []
    if transcript_data.get('results') and isinstance(transcript_data['results'], list):
        for result in transcript_data['results']:
            if result.get('type') == 'punctuation':
                continue
            
            if result.get('type') == 'word' and result.get('alternatives'):
                alt = result['alternatives'][0]
                speaker_label = alt.get('speaker', 'S1')
                
                # Convert "S1" -> 0, "S2" -> 1, etc.
                speaker_num = 0
                if speaker_label.startswith('S'):
                    num_str = speaker_label[1:]
                    speaker_num = int(num_str) - 1 if num_str.isdigit() else 0
                else:
                    speaker_num = int(speaker_label) if str(speaker_label).isdigit() else 0
                
                words.append({
                    'word': alt.get('content', ''),
                    'start': result.get('start_time', 0),
                    'end': result.get('end_time', result.get('start_time', 0)),
                    'speaker': speaker_num
                })
    
    # –§–æ—Ä–º—É—î–º–æ —Å–µ–≥–º–µ–Ω—Ç–∏ —Ç–∞ —Ç–µ–∫—Å—Ç
    segments = []
    transcription_parts = []
    
    if words:
        current_speaker = None
        current_start = None
        current_words = []
        
        for word_info in words:
            speaker = word_info.get('speaker', 0)
            word_start = word_info.get('start', 0)
            word_text = word_info.get('word', '').strip()
            
            if not word_text:
                continue
            
            if speaker != current_speaker:
                if current_speaker is not None and current_words:
                    text = ' '.join(current_words).strip()
                    segments.append({
                        'speaker': current_speaker,
                        'start': round(current_start, 2),
                        'end': round(word_start, 2),
                        'text': text
                    })
                    transcription_parts.append(text)
                current_speaker = speaker
                current_start = word_start
                current_words = [word_text]
            else:
                current_words.append(word_text)
        
        if current_words:
            text = ' '.join(current_words).strip()
            segments.append({
                'speaker': current_speaker if current_speaker is not None else 0,
                'start': round(current_start, 2) if current_start is not None else 0,
                'end': round(words[-1].get('end', 0), 2) if words else 0,
                'text': text
            })
            transcription_parts.append(text)
    
    transcription = ' '.join(transcription_parts)
    
    print(f"‚úÖ Speechmatics transcription completed: {len(segments)} segments, {len(words)} words")
    sys.stdout.flush()
    
    return transcription, segments, words


def transcribe_with_azure(audio_path, language='en-US'):
    """
    –¢—Ä–∞–Ω—Å–∫—Ä–∏–±—É—î –∞—É–¥—ñ–æ –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é Azure Speech Services –∑ word timestamps.
    
    Args:
        audio_path: —à–ª—è—Ö –¥–æ –∞—É–¥—ñ–æ—Ñ–∞–π–ª—É
        language: –∫–æ–¥ –º–æ–≤–∏ (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, 'uk-UA', 'en-US', 'ar-SA')
    
    Returns:
        transcription: —Ç–µ–∫—Å—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó
        segments: —Å–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ –∑ —Ç–µ–∫—Å—Ç–æ–º —Ç–∞ —á–∞—Å–æ–≤–∏–º–∏ –º—ñ—Ç–∫–∞–º–∏
        words: —Å–ø–∏—Å–æ–∫ —Å–ª—ñ–≤ –∑ timestamps –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –º–∞—Ç—á–∏–Ω–≥—É
    """
    import sys
    from azure_stt import AzureSpeechClient
    
    subscription_key = os.getenv('AZURE_SPEECH_KEY')
    region = os.getenv('AZURE_SPEECH_REGION')
    
    if not subscription_key or not region:
        raise ValueError("AZURE_SPEECH_KEY and AZURE_SPEECH_REGION environment variables are required")
    
    print(f"üé§ Transcribing with Azure: {audio_path}")
    print(f"   Language: {language}")
    sys.stdout.flush()
    
    # Azure –ø–æ—Ç—Ä–µ–±—É—î —Ñ–∞–π–ª —É —Ö–º–∞—Ä—ñ (Blob Storage) –∞–±–æ –º–æ–∂–Ω–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ –ª–æ–∫–∞–ª—å–Ω–∏–π —Ñ–∞–π–ª —á–µ—Ä–µ–∑ SAS URL
    # –î–ª—è —Å–ø—Ä–æ—â–µ–Ω–Ω—è, –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—î–º–æ –ª–æ–∫–∞–ª—å–Ω–∏–π —Ñ–∞–π–ª (Azure –ø—ñ–¥—Ç—Ä–∏–º—É—î —Ü–µ —á–µ—Ä–µ–∑ file://)
    # –ê–ª–µ –∫—Ä–∞—â–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ Azure Blob Storage –∞–±–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Ñ–∞–π–ª
    
    # –¢–∏–º—á–∞—Å–æ–≤–æ: –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ Azure Speech SDK –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–∏—Ö —Ñ–∞–π–ª—ñ–≤
    try:
        import azure.cognitiveservices.speech as speechsdk
    except ImportError:
        raise ImportError("azure-cognitiveservices-speech is required. Install: pip install azure-cognitiveservices-speech")
    
    speech_config = speechsdk.SpeechConfig(subscription=subscription_key, region=region)
    speech_config.speech_recognition_language = language
    speech_config.request_word_level_timestamps()
    
    audio_config = speechsdk.audio.AudioConfig(filename=audio_path)
    
    # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ ConversationTranscriber –¥–ª—è –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó
    transcriber = speechsdk.transcription.ConversationTranscriber(speech_config=speech_config, audio_config=audio_config)
    
    segments = []
    words = []
    transcription_parts = []
    
    done = False
    error_holder = {}
    
    def recognized_cb(evt):
        if evt.result.reason == speechsdk.ResultReason.RecognizedSpeech:
            text = evt.result.text
            start_time = evt.result.offset / 10000000.0  # Convert from 100-nanosecond units
            duration = evt.result.duration / 10000000.0
            end_time = start_time + duration
            
            # Azure –ø–æ–≤–µ—Ä—Ç–∞—î speaker_id –≤ evt.result.speaker_id (—è–∫—â–æ –¥–æ—Å—Ç—É–ø–Ω–æ)
            speaker = getattr(evt.result, 'speaker_id', 0)
            if isinstance(speaker, str) and speaker.startswith('Guest'):
                speaker = int(speaker.replace('Guest', '')) if speaker.replace('Guest', '').isdigit() else 0
            
            segments.append({
                'speaker': speaker,
                'start': round(start_time, 2),
                'end': round(end_time, 2),
                'text': text.strip()
            })
            transcription_parts.append(text.strip())
            
            # –†–æ–∑–±–∏–≤–∞—î–º–æ –Ω–∞ —Å–ª–æ–≤–∞ (Azure –Ω–µ –∑–∞–≤–∂–¥–∏ –ø–æ–≤–µ—Ä—Ç–∞—î word-level timestamps –≤ —Ü—å–æ–º—É API)
            # –î–ª—è –ø—Ä–æ—Å—Ç–æ—Ç–∏, —Ä–æ–∑–±–∏–≤–∞—î–º–æ —Ç–µ–∫—Å—Ç –Ω–∞ —Å–ª–æ–≤–∞ –∑ –ø—Ä–∏–±–ª–∏–∑–Ω–∏–º–∏ timestamps
            word_list = text.strip().split()
            if word_list:
                word_duration = duration / len(word_list)
                for i, word in enumerate(word_list):
                    word_start = start_time + (i * word_duration)
                    word_end = word_start + word_duration
                    words.append({
                        'word': word,
                        'start': round(word_start, 2),
                        'end': round(word_end, 2),
                        'speaker': speaker
                    })
    
    def canceled_cb(evt):
        error_holder['error'] = evt.error_details
        done = True
    
    def stop_cb(evt):
        nonlocal done
        done = True
    
    transcriber.transcribed.connect(recognized_cb)
    transcriber.session_stopped.connect(stop_cb)
    transcriber.canceled.connect(canceled_cb)
    
    transcriber.start_transcribing_async().wait()
    
    # –û—á—ñ–∫—É—î–º–æ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è
    import time
    timeout = 300  # 5 —Ö–≤–∏–ª–∏–Ω
    start_time = time.time()
    while not done and (time.time() - start_time) < timeout:
        time.sleep(0.1)
    
    transcriber.stop_transcribing_async().wait()
    
    if 'error' in error_holder:
        raise RuntimeError(f"Azure transcription error: {error_holder['error']}")
    
    transcription = ' '.join(transcription_parts)
    
    print(f"‚úÖ Azure transcription completed: {len(segments)} segments, {len(words)} words")
    sys.stdout.flush()
    
    return transcription, segments, words


def transcribe_audio(audio_path, language=None, transcription_provider='whisper'):
    """
    –¢—Ä–∞–Ω—Å–∫—Ä–∏–±—É—î –∞—É–¥—ñ–æ –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é –≤–∏–±—Ä–∞–Ω–æ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ –∑ word timestamps.
    
    Args:
        audio_path: —à–ª—è—Ö –¥–æ –∞—É–¥—ñ–æ—Ñ–∞–π–ª—É
        language: –∫–æ–¥ –º–æ–≤–∏ (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, 'uk', 'en', 'ar') –∞–±–æ None –¥–ª—è –∞–≤—Ç–æ-–≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è
        transcription_provider: –ø—Ä–æ–≤–∞–π–¥–µ—Ä —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó ('whisper', 'azure', 'speechmatics')
    
    Returns:
        transcription: —Ç–µ–∫—Å—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó
        segments: —Å–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ –∑ —Ç–µ–∫—Å—Ç–æ–º —Ç–∞ —á–∞—Å–æ–≤–∏–º–∏ –º—ñ—Ç–∫–∞–º–∏
        words: —Å–ø–∏—Å–æ–∫ —Å–ª—ñ–≤ –∑ timestamps –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –º–∞—Ç—á–∏–Ω–≥—É
    """
    if transcription_provider == 'speechmatics':
        # Speechmatics –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î —Ñ–æ—Ä–º–∞—Ç 'en', 'uk', 'ar'
        lang_map = {'en': 'en', 'uk': 'uk', 'ar': 'ar', 'en-US': 'en', 'uk-UA': 'uk', 'ar-SA': 'ar'}
        speechmatics_lang = lang_map.get(language, language or 'en')
        return transcribe_with_speechmatics(audio_path, speechmatics_lang)
    elif transcription_provider == 'azure':
        # Azure –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î —Ñ–æ—Ä–º–∞—Ç 'en-US', 'uk-UA', 'ar-SA'
        lang_map = {'en': 'en-US', 'uk': 'uk-UA', 'ar': 'ar-SA'}
        azure_lang = lang_map.get(language, language or 'en-US')
        return transcribe_with_azure(audio_path, azure_lang)
    else:
        # Whisper (–∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º)
        return transcribe_audio_whisper(audio_path, language)


def transcribe_audio_whisper(audio_path, language=None):
    """
    –¢—Ä–∞–Ω—Å–∫—Ä–∏–±—É—î –∞—É–¥—ñ–æ –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é Whisper –∑ word timestamps.
    
    Args:
        audio_path: —à–ª—è—Ö –¥–æ –∞—É–¥—ñ–æ—Ñ–∞–π–ª—É
        language: –∫–æ–¥ –º–æ–≤–∏ (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, 'uk', 'en', 'ar') –∞–±–æ None –¥–ª—è –∞–≤—Ç–æ-–≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è
    
    Returns:
        transcription: —Ç–µ–∫—Å—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó
        segments: —Å–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ –∑ —Ç–µ–∫—Å—Ç–æ–º —Ç–∞ —á–∞—Å–æ–≤–∏–º–∏ –º—ñ—Ç–∫–∞–º–∏
        words: —Å–ø–∏—Å–æ–∫ —Å–ª—ñ–≤ –∑ timestamps –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –º–∞—Ç—á–∏–Ω–≥—É
    """
    import sys
    global whisper_model
    
    if whisper_model is None:
        load_models()
    
    try:
        # –û—Ç—Ä–∏–º—É—î–º–æ —Ç—Ä–∏–≤–∞–ª—ñ—Å—Ç—å –∞—É–¥—ñ–æ –¥–ª—è –æ—Ü—ñ–Ω–∫–∏ —á–∞—Å—É –æ–±—Ä–æ–±–∫–∏
        try:
            import librosa
            audio_duration = librosa.get_duration(path=audio_path)
        except:
            audio_duration = 0
        
        print(f"üé§ Transcribing audio: {audio_path}")
        if audio_duration > 0:
            print(f"   Audio duration: {audio_duration:.2f} seconds ({audio_duration/60:.1f} minutes)")
        
        # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó (–æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–æ –¥–ª—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ)
        import torch
        device = next(whisper_model.parameters()).device
        use_fp16 = device.type == 'cuda'  # fp16 —Ç—ñ–ª—å–∫–∏ –Ω–∞ GPU, –Ω–∞ CPU –º–æ–∂–µ –±—É—Ç–∏ –ø–æ–≤—ñ–ª—å–Ω—ñ—à–µ
        
        # –î–ª—è large –º–æ–¥–µ–ª—ñ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –±—ñ–ª—å—à –∞–≥—Ä–µ—Å–∏–≤–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –¥–ª—è –∫—Ä–∞—â–æ–≥–æ —Ä–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è
        model_size = os.environ.get('WHISPER_MODEL_SIZE', 'small')
        is_large_model = model_size in ['large', 'large-v2', 'large-v3']
        
        transcribe_options = {
            'word_timestamps': True,
            'verbose': True,  # –£–≤—ñ–º–∫–Ω—É—Ç–æ –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—É
            'task': 'transcribe',  # –ó–∞–≤–∂–¥–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±—É—î–º–æ, –Ω–µ –ø–µ—Ä–µ–∫–ª–∞–¥–∞—î–º–æ
            'fp16': use_fp16,  # fp16 –Ω–∞ GPU –¥–ª—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ, fp32 –Ω–∞ CPU
            'temperature': 0.0,  # –ú–µ–Ω—à–µ –≤–∏–ø–∞–¥–∫–æ–≤–æ—Å—Ç—ñ = –±—ñ–ª—å—à —Å—Ç–∞–±—ñ–ª—å–Ω–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            'best_of': 2 if is_large_model else 1,  # –î–ª—è large –º–æ–¥–µ–ª—ñ - –±—ñ–ª—å—à–µ –≤–∞—Ä—ñ–∞–Ω—Ç—ñ–≤
            'beam_size': 5 if is_large_model else 3,  # –î–ª—è large –º–æ–¥–µ–ª—ñ - –±—ñ–ª—å—à–∏–π beam –¥–ª—è –∫—Ä–∞—â–æ—ó —è–∫–æ—Å—Ç—ñ
            'compression_ratio_threshold': 2.4,  # –§—ñ–ª—å—Ç—Ä –ø–æ–≤—Ç–æ—Ä–µ–Ω—å
            'logprob_threshold': -1.0,  # –§—ñ–ª—å—Ç—Ä –Ω–∏–∑—å–∫–æ—ó –≤–ø–µ–≤–Ω–µ–Ω–æ—Å—Ç—ñ
            'no_speech_threshold': 0.5 if is_large_model else 0.6  # –î–ª—è large –º–æ–¥–µ–ª—ñ - –º–µ–Ω—à–∏–π –ø–æ—Ä—ñ–≥ —Ç–∏—à—ñ (–±—ñ–ª—å—à–µ —Ä–æ–∑–ø—ñ–∑–Ω–∞—î)
        }
        
        print(f"‚öôÔ∏è  Whisper settings: fp16={use_fp16}, beam_size={transcribe_options['beam_size']}, device={device.type}")
        
        if language:
            transcribe_options['language'] = language
            print(f"üåê Using specified language: {language}")
        else:
            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –º–æ–≤–∏ - Whisper –∑—Ä–æ–±–∏—Ç—å —Ü–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ
            print(f"üåê Auto-detecting language (Whisper will detect automatically)")
            print(f"   üí° Tip: Specify 'language=uk' for Ukrainian to improve accuracy")
        
        # –¢—Ä–∞–Ω—Å–∫—Ä–∏–±—É—î–º–æ –∑ –¥–µ—Ç–∞–ª—å–Ω–∏–º–∏ —Å–µ–≥–º–µ–Ω—Ç–∞–º–∏ —Ç–∞ word timestamps
        import time
        start_time = time.time()
        print(f"‚è±Ô∏è  Starting Whisper transcription (this may take a while for long audio)...")
        print(f"   Estimated time: ~{audio_duration * 0.5:.1f} seconds (rough estimate)")
        sys.stdout.flush()
        
        result = whisper_model.transcribe(
            audio_path,
            **transcribe_options
        )
        
        elapsed_time = time.time() - start_time
        print(f"‚úÖ Whisper transcription completed in {elapsed_time:.1f} seconds ({elapsed_time/60:.1f} minutes)")
        print(f"   Processing speed: {audio_duration/elapsed_time:.2f}x real-time")
        sys.stdout.flush()
        
        detected_lang = result.get('language', 'unknown')
        print(f"üåê Detected language: {detected_lang}")
        
        transcription = result.get("text", "")
        print(f"üìù Transcription text length: {len(transcription) if transcription else 0} characters")
        print(f"üìù Transcription preview: {transcription[:200] if transcription else 'EMPTY'}")
        
        segments = []
        words = []
        
        # –§–æ—Ä–º—É—î–º–æ —Å–µ–≥–º–µ–Ω—Ç–∏ –∑ —Ç–µ–∫—Å—Ç–æ–º —Ç–∞ –∑–±–∏—Ä–∞—î–º–æ –≤—Å—ñ —Å–ª–æ–≤–∞
        for seg in result["segments"]:
            segments.append({
                'start': round(seg['start'], 2),
                'end': round(seg['end'], 2),
                'text': seg['text'].strip()
            })
            
            # –ó–±–∏—Ä–∞—î–º–æ —Å–ª–æ–≤–∞ –∑ timestamps
            if 'words' in seg:
                for word_info in seg['words']:
                    words.append({
                        'word': word_info.get('word', '').strip(),
                        'start': round(word_info.get('start', 0), 2),
                        'end': round(word_info.get('end', 0), 2)
                    })
        
        print(f"‚úÖ Transcribed {len(segments)} segments, language: {detected_lang}")
        return transcription, segments, words
    
    except Exception as e:
        print(f"‚ùå Error in transcribe_audio_whisper: {e}")
        import traceback
        traceback.print_exc()
        return "", [], []


def transcribe_with_speechmatics(audio_path, language='en'):
    """
    –¢—Ä–∞–Ω—Å–∫—Ä–∏–±—É—î –∞—É–¥—ñ–æ –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é Speechmatics API –∑ word timestamps.
    
    Args:
        audio_path: —à–ª—è—Ö –¥–æ –∞—É–¥—ñ–æ—Ñ–∞–π–ª—É
        language: –∫–æ–¥ –º–æ–≤–∏ (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, 'uk', 'en', 'ar')
    
    Returns:
        transcription: —Ç–µ–∫—Å—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó
        segments: —Å–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ –∑ —Ç–µ–∫—Å—Ç–æ–º —Ç–∞ —á–∞—Å–æ–≤–∏–º–∏ –º—ñ—Ç–∫–∞–º–∏
        words: —Å–ø–∏—Å–æ–∫ —Å–ª—ñ–≤ –∑ timestamps –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –º–∞—Ç—á–∏–Ω–≥—É
    """
    import sys
    from transcribe_with_speechmatics import upload_to_speechmatics, poll_speechmatics_job
    
    api_key = os.getenv('SPEECHMATICS_API_KEY')
    if not api_key:
        raise ValueError("SPEECHMATICS_API_KEY environment variable is not set")
    
    print(f"üé§ Transcribing with Speechmatics: {audio_path}")
    print(f"   Language: {language}")
    sys.stdout.flush()
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ —Ñ–∞–π–ª
    job_id = upload_to_speechmatics(api_key, audio_path, language, is_separated_track=True)
    
    # –û—á—ñ–∫—É—î–º–æ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è
    transcript_data = poll_speechmatics_job(api_key, job_id)
    
    # –ü–∞—Ä—Å–∏–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    words = []
    if transcript_data.get('results') and isinstance(transcript_data['results'], list):
        for result in transcript_data['results']:
            if result.get('type') == 'punctuation':
                continue
            
            if result.get('type') == 'word' and result.get('alternatives'):
                alt = result['alternatives'][0]
                speaker_label = alt.get('speaker', 'S1')
                
                # Convert "S1" -> 0, "S2" -> 1, etc.
                speaker_num = 0
                if speaker_label.startswith('S'):
                    num_str = speaker_label[1:]
                    speaker_num = int(num_str) - 1 if num_str.isdigit() else 0
                else:
                    speaker_num = int(speaker_label) if str(speaker_label).isdigit() else 0
                
                words.append({
                    'word': alt.get('content', ''),
                    'start': result.get('start_time', 0),
                    'end': result.get('end_time', result.get('start_time', 0)),
                    'speaker': speaker_num
                })
    
    # –§–æ—Ä–º—É—î–º–æ —Å–µ–≥–º–µ–Ω—Ç–∏ —Ç–∞ —Ç–µ–∫—Å—Ç
    segments = []
    transcription_parts = []
    
    if words:
        current_speaker = None
        current_start = None
        current_words = []
        
        for word_info in words:
            speaker = word_info.get('speaker', 0)
            word_start = word_info.get('start', 0)
            word_text = word_info.get('word', '').strip()
            
            if not word_text:
                continue
            
            if speaker != current_speaker:
                if current_speaker is not None and current_words:
                    text = ' '.join(current_words).strip()
                    segments.append({
                        'speaker': current_speaker,
                        'start': round(current_start, 2),
                        'end': round(word_start, 2),
                        'text': text
                    })
                    transcription_parts.append(text)
                current_speaker = speaker
                current_start = word_start
                current_words = [word_text]
            else:
                current_words.append(word_text)
        
        if current_words:
            text = ' '.join(current_words).strip()
            segments.append({
                'speaker': current_speaker if current_speaker is not None else 0,
                'start': round(current_start, 2) if current_start is not None else 0,
                'end': round(words[-1].get('end', 0), 2) if words else 0,
                'text': text
            })
            transcription_parts.append(text)
    
    transcription = ' '.join(transcription_parts)
    
    print(f"‚úÖ Speechmatics transcription completed: {len(segments)} segments, {len(words)} words")
    sys.stdout.flush()
    
    return transcription, segments, words


def transcribe_with_azure(audio_path, language='en-US'):
    """
    –¢—Ä–∞–Ω—Å–∫—Ä–∏–±—É—î –∞—É–¥—ñ–æ –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é Azure Speech Services –∑ word timestamps.
    
    Args:
        audio_path: —à–ª—è—Ö –¥–æ –∞—É–¥—ñ–æ—Ñ–∞–π–ª—É
        language: –∫–æ–¥ –º–æ–≤–∏ (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, 'uk-UA', 'en-US', 'ar-SA')
    
    Returns:
        transcription: —Ç–µ–∫—Å—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó
        segments: —Å–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ –∑ —Ç–µ–∫—Å—Ç–æ–º —Ç–∞ —á–∞—Å–æ–≤–∏–º–∏ –º—ñ—Ç–∫–∞–º–∏
        words: —Å–ø–∏—Å–æ–∫ —Å–ª—ñ–≤ –∑ timestamps –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –º–∞—Ç—á–∏–Ω–≥—É
    """
    import sys
    try:
        import azure.cognitiveservices.speech as speechsdk
    except ImportError:
        raise ImportError("azure-cognitiveservices-speech is required. Install: pip install azure-cognitiveservices-speech")
    
    subscription_key = os.getenv('AZURE_SPEECH_KEY')
    region = os.getenv('AZURE_SPEECH_REGION')
    
    if not subscription_key or not region:
        raise ValueError("AZURE_SPEECH_KEY and AZURE_SPEECH_REGION environment variables are required")
    
    print(f"üé§ Transcribing with Azure: {audio_path}")
    print(f"   Language: {language}")
    sys.stdout.flush()
    
    speech_config = speechsdk.SpeechConfig(subscription=subscription_key, region=region)
    speech_config.speech_recognition_language = language
    speech_config.request_word_level_timestamps()
    
    audio_config = speechsdk.audio.AudioConfig(filename=audio_path)
    
    # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ ConversationTranscriber –¥–ª—è –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó
    transcriber = speechsdk.transcription.ConversationTranscriber(speech_config=speech_config, audio_config=audio_config)
    
    segments = []
    words = []
    transcription_parts = []
    
    done = False
    error_holder = {}
    
    def recognized_cb(evt):
        if evt.result.reason == speechsdk.ResultReason.RecognizedSpeech:
            text = evt.result.text
            start_time = evt.result.offset / 10000000.0  # Convert from 100-nanosecond units
            duration = evt.result.duration / 10000000.0
            end_time = start_time + duration
            
            # Azure –ø–æ–≤–µ—Ä—Ç–∞—î speaker_id –≤ evt.result.speaker_id (—è–∫—â–æ –¥–æ—Å—Ç—É–ø–Ω–æ)
            speaker = getattr(evt.result, 'speaker_id', 0)
            if isinstance(speaker, str):
                if speaker.startswith('Guest'):
                    speaker = int(speaker.replace('Guest', '')) if speaker.replace('Guest', '').isdigit() else 0
                elif speaker.startswith('SPEAKER_'):
                    speaker = int(speaker.replace('SPEAKER_', '')) if speaker.replace('SPEAKER_', '').isdigit() else 0
            
            segments.append({
                'speaker': speaker,
                'start': round(start_time, 2),
                'end': round(end_time, 2),
                'text': text.strip()
            })
            transcription_parts.append(text.strip())
            
            # –†–æ–∑–±–∏–≤–∞—î–º–æ –Ω–∞ —Å–ª–æ–≤–∞ –∑ –ø—Ä–∏–±–ª–∏–∑–Ω–∏–º–∏ timestamps
            word_list = text.strip().split()
            if word_list:
                word_duration = duration / len(word_list)
                for i, word in enumerate(word_list):
                    word_start = start_time + (i * word_duration)
                    word_end = word_start + word_duration
                    words.append({
                        'word': word,
                        'start': round(word_start, 2),
                        'end': round(word_end, 2),
                        'speaker': speaker
                    })
    
    def canceled_cb(evt):
        error_holder['error'] = evt.error_details
        nonlocal done
        done = True
    
    def stop_cb(evt):
        nonlocal done
        done = True
    
    transcriber.transcribed.connect(recognized_cb)
    transcriber.session_stopped.connect(stop_cb)
    transcriber.canceled.connect(canceled_cb)
    
    transcriber.start_transcribing_async().wait()
    
    # –û—á—ñ–∫—É—î–º–æ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è
    import time
    timeout = 300  # 5 —Ö–≤–∏–ª–∏–Ω
    start_time = time.time()
    while not done and (time.time() - start_time) < timeout:
        time.sleep(0.1)
    
    transcriber.stop_transcribing_async().wait()
    
    if 'error' in error_holder:
        raise RuntimeError(f"Azure transcription error: {error_holder['error']}")
    
    transcription = ' '.join(transcription_parts)
    
    print(f"‚úÖ Azure transcription completed: {len(segments)} segments, {len(words)} words")
    sys.stdout.flush()
    
    return transcription, segments, words


def clean_punctuation(text):
    """–û—á–∏—â–∞—î –ø—É–Ω–∫—Ç—É–∞—Ü—ñ—é –∑ –ø–æ—á–∞—Ç–∫—É —Ç–∞ –∫—ñ–Ω—Ü—è —Ç–µ–∫—Å—Ç—É"""
    import string
    if not text:
        return text
    # –í–∏–¥–∞–ª—è—î–º–æ –ø—É–Ω–∫—Ç—É–∞—Ü—ñ—é –∑ –ø–æ—á–∞—Ç–∫—É —Ç–∞ –∫—ñ–Ω—Ü—è
    text = text.strip()
    while text and text[0] in string.punctuation:
        text = text[1:].strip()
    while text and text[-1] in string.punctuation:
        text = text[:-1].strip()
    return text


def get_model_id(mode='smart'):
    """
    –û—Ç—Ä–∏–º—É—î ID –º–æ–¥–µ–ª—ñ –¥–ª—è –∑–∞–¥–∞–Ω–æ–≥–æ —Ä–µ–∂–∏–º—É (–∞–Ω–∞–ª–æ–≥ getModelId –∑ server.js)
    
    Args:
        mode: –†–µ–∂–∏–º LLM ('local', 'fast', 'smart', 'smart-2', 'test', 'test2')
    
    Returns:
        str: ID –º–æ–¥–µ–ª—ñ
    """
    if mode == 'local':
        return os.getenv('LOCAL_LLM_MODEL') or 'openai/gpt-oss-20b'
    elif mode == 'test':
        return os.getenv('TEST_MODEL_ID') or os.getenv('OPENROUTER_TEST_MODEL_ID') or 'google/gemma-3-4b'
    elif mode == 'test2':
        return os.getenv('TEST2_MODEL_ID') or os.getenv('OPENROUTER_TEST2_MODEL_ID') or 'llama-3.2-1b-instruct'
    elif mode == 'fast':
        return os.getenv('FAST_MODEL_ID') or os.getenv('OPENROUTER_FAST_MODEL_ID') or 'gpt-oss-120b'
    elif mode == 'smart-2' or mode == 'smart2':
        return os.getenv('SMART_2_MODEL_ID') or os.getenv('OPENROUTER_SMART_2_MODEL_ID') or 'google/gemini-3-pro-preview'
    else:
        # Default to 'smart'
        return os.getenv('SMART_MODEL_ID') or os.getenv('OPENROUTER_SMART_MODEL_ID') or 'google/gemini-3.0-pro'


def call_llm_for_segment_splitting(segment, all_segments_context=None, mode='local'):
    """
    –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î LLM –¥–ª—è –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è, —á–∏ –ø–æ—Ç—Ä—ñ–±–Ω–æ —Ä–æ–∑–¥—ñ–ª–∏—Ç–∏ —Å–µ–≥–º–µ–Ω—Ç –Ω–∞ —á–∞—Å—Ç–∏–Ω–∏,
    —è–∫—â–æ –≤—ñ–Ω –º—ñ—Å—Ç–∏—Ç—å –ø–∏—Ç–∞–Ω–Ω—è + –≤—ñ–¥–ø–æ–≤—ñ–¥—å –≤—ñ–¥ —Ä—ñ–∑–Ω–∏—Ö —Å–ø—ñ–∫–µ—Ä—ñ–≤.
    
    Args:
        segment: –°–µ–≥–º–µ–Ω—Ç –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É {'speaker': int, 'start': float, 'end': float, 'text': str}
        all_segments_context: –°–ø–∏—Å–æ–∫ –≤—Å—ñ—Ö —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –¥—ñ–∞–ª–æ–≥—É
        mode: –†–µ–∂–∏–º LLM ('local', 'fast', 'smart', 'smart-2')
    
    Returns:
        dict or None: {
            'should_split': bool,
            'parts': [
                {'text': str, 'speaker': int, 'start': float, 'end': float},
                ...
            ]
        } –∞–±–æ None —è–∫—â–æ LLM –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π
    """
    import sys
    
    # –í–∏–∑–Ω–∞—á–∞—î–º–æ, —á–∏ —Ü–µ –ª–æ–∫–∞–ª—å–Ω–∏–π LLM
    use_local_llm = mode == 'local' or mode == 'test' or mode == 'test2'
    
    # –û—Ç—Ä–∏–º—É—î–º–æ –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤—ñ —Ä–µ–∂–∏–º—É
    llm_model = get_model_id(mode)
    
    # –í–∏–∑–Ω–∞—á–∞—î–º–æ API URL —Ç–∞ –∫–ª—é—á
    if use_local_llm:
        llm_api_url = os.getenv('LOCAL_LLM_BASE_URL') or 'http://127.0.0.1:3001'
        llm_api_key = os.getenv('LOCAL_LLM_API_KEY') or ''
    else:
        llm_api_url = 'https://openrouter.ai/api/v1/chat/completions'
        llm_api_key = os.getenv('OPENROUTER_API_KEY') or ''
    
    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ –Ω–∞–ª–∞—à—Ç–æ–≤–∞–Ω–∏–π LLM
    if use_local_llm:
        if not llm_api_url:
            return None
    else:
        if not llm_api_key:
            return None
    
    # –§–æ—Ä–º—É—î–º–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥—ñ–∞–ª–æ–≥—É
    context_segments = ""
    if all_segments_context:
        for i, seg in enumerate(all_segments_context):
            context_segments += f"\n{i+1}. [{seg['start']:.2f}s-{seg['end']:.2f}s] –°–ø—ñ–∫–µ—Ä {seg['speaker']}: \"{seg.get('text', '')}\""
    
    # –í–∏–∑–Ω–∞—á–∞—î–º–æ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
    speaker_word_counts = {}
    if all_segments_context:
        for seg in all_segments_context:
            speaker = seg['speaker']
            word_count = len(seg.get('text', '').split())
            speaker_word_counts[speaker] = speaker_word_counts.get(speaker, 0) + word_count
    main_speaker = max(speaker_word_counts.items(), key=lambda x: x[1])[0] if speaker_word_counts else 0
    
    system_prompt = """–¢–∏ –µ–∫—Å–ø–µ—Ä—Ç –∑ –∞–Ω–∞–ª—ñ–∑—É –¥—ñ–∞–ª–æ–≥—ñ–≤. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –≤–∏–∑–Ω–∞—á–∏—Ç–∏, —á–∏ –æ–¥–∏–Ω —Å–µ–≥–º–µ–Ω—Ç –º—ñ—Å—Ç–∏—Ç—å –ø–∏—Ç–∞–Ω–Ω—è + –≤—ñ–¥–ø–æ–≤—ñ–¥—å –≤—ñ–¥ –†–Ü–ó–ù–ò–• —Å–ø—ñ–∫–µ—Ä—ñ–≤, —ñ —è–∫—â–æ —Ç–∞–∫, —Ä–æ–∑–¥—ñ–ª–∏—Ç–∏ –π–æ–≥–æ –Ω–∞ —á–∞—Å—Ç–∏–Ω–∏.

–ö–†–ò–¢–ò–ß–ù–û –í–ê–ñ–õ–ò–í–û:
- –Ø–∫—â–æ –ø—ñ—Å–ª—è –ø–∏—Ç–∞–Ω–Ω—è –π–¥–µ –≤—ñ–¥–ø–æ–≤—ñ–¥—å, —è–∫–∞ –ø–æ—á–∏–Ω–∞—î—Ç—å—Å—è –∑ "Uh", "Um", "Well", "Yes", "No", –∞–±–æ –º—ñ—Å—Ç–∏—Ç—å –∫–æ—Ä–æ—Ç–∫—ñ —Ñ—Ä–∞–∑–∏ —Ç–∏–ø—É "per second", "per minute" - —Ü–µ –ó–ê–í–ñ–î–ò –≤—ñ–¥–ø–æ–≤—ñ–¥—å –≤—ñ–¥ –Ü–ù–®–û–ì–û —Å–ø—ñ–∫–µ—Ä–∞
- –ü–∏—Ç–∞–Ω–Ω—è –∑–∞–∑–≤–∏—á–∞–π –∑–∞–¥–∞—î –æ—Å–Ω–æ–≤–Ω–∏–π —Å–ø—ñ–∫–µ—Ä (—Ç–æ–π, —Ö—Ç–æ –≤–µ–¥–µ –¥—ñ–∞–ª–æ–≥, –º–∞—î –±—ñ–ª—å—à–µ —Å–ª—ñ–≤)
- –í—ñ–¥–ø–æ–≤—ñ–¥—ñ –Ω–∞ –ø–∏—Ç–∞–Ω–Ω—è –∑–∞–∑–≤–∏—á–∞–π –Ω–∞–ª–µ–∂–∞—Ç—å —ñ–Ω—à–æ–º—É —Å–ø—ñ–∫–µ—Ä—É (–Ω–µ –æ—Å–Ω–æ–≤–Ω–æ–º—É)

–ü–†–ò–ö–õ–ê–î–ò –†–û–ó–î–Ü–õ–ï–ù–ù–Ø:

1. "What speed does it show? Uh, per second."
   ‚Üí –†–û–ó–î–Ü–õ–ò–¢–ò –Ω–∞:
   * "What speed does it show?" (–æ—Å–Ω–æ–≤–Ω–∏–π —Å–ø—ñ–∫–µ—Ä, —Ö—Ç–æ –∑–∞–¥–∞—î –ø–∏—Ç–∞–Ω–Ω—è)
   * "Uh, per second." (—ñ–Ω—à–∏–π —Å–ø—ñ–∫–µ—Ä, —Ö—Ç–æ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î)

2. "Can you try to reset? Yes, I did."
   ‚Üí –†–û–ó–î–Ü–õ–ò–¢–ò –Ω–∞:
   * "Can you try to reset?" (–æ—Å–Ω–æ–≤–Ω–∏–π —Å–ø—ñ–∫–µ—Ä)
   * "Yes, I did." (—ñ–Ω—à–∏–π —Å–ø—ñ–∫–µ—Ä)

3. "Did you check the settings? Well, I think so."
   ‚Üí –†–û–ó–î–Ü–õ–ò–¢–ò –Ω–∞:
   * "Did you check the settings?" (–æ—Å–Ω–æ–≤–Ω–∏–π —Å–ø—ñ–∫–µ—Ä)
   * "Well, I think so." (—ñ–Ω—à–∏–π —Å–ø—ñ–∫–µ—Ä)

–ü–†–ê–í–ò–õ–ê –í–ò–Ø–í–õ–ï–ù–ù–Ø:

1. –ü–ò–¢–ê–ù–ù–Ø:
   - –ú—ñ—Å—Ç–∏—Ç—å "?"
   - –ê–±–æ –ø–æ—á–∏–Ω–∞—î—Ç—å—Å—è –∑ "What", "How", "Why", "When", "Where", "Did you", "Can you", "Do you", "Try to"
   - –ó–∞–∑–≤–∏—á–∞–π –∑–∞–¥–∞—î –æ—Å–Ω–æ–≤–Ω–∏–π —Å–ø—ñ–∫–µ—Ä

2. –í–Ü–î–ü–û–í–Ü–î–Ü:
   - –ü–æ—á–∏–Ω–∞—é—Ç—å—Å—è –∑ "Uh", "Um", "Well", "Yes", "No", "Yeah", "Sure", "Okay"
   - –ê–±–æ –º—ñ—Å—Ç—è—Ç—å –∫–æ—Ä–æ—Ç–∫—ñ —Ñ—Ä–∞–∑–∏ —Ç–∏–ø—É "per second", "per minute", "I did", "I do"
   - –ó–∞–∑–≤–∏—á–∞–π –Ω–∞–ª–µ–∂–∞—Ç—å —ñ–Ω—à–æ–º—É —Å–ø—ñ–∫–µ—Ä—É (–Ω–µ –æ—Å–Ω–æ–≤–Ω–æ–º—É)

3. –ö–û–ù–¢–ï–ö–°–¢:
   - –û—Å–Ω–æ–≤–Ω–∏–π —Å–ø—ñ–∫–µ—Ä - —Ç–æ–π, —Ö—Ç–æ –º–∞—î –±—ñ–ª—å—à–µ —Å–ª—ñ–≤ —É –¥—ñ–∞–ª–æ–∑—ñ
   - –û—Å–Ω–æ–≤–Ω–∏–π —Å–ø—ñ–∫–µ—Ä –∑–∞–∑–≤–∏—á–∞–π –∑–∞–¥–∞—î –ø–∏—Ç–∞–Ω–Ω—è —Ç–∞ –¥–∞—î —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—ó
   - –Ü–Ω—à–∏–π —Å–ø—ñ–∫–µ—Ä –∑–∞–∑–≤–∏—á–∞–π –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î –Ω–∞ –ø–∏—Ç–∞–Ω–Ω—è

4. –ö–û–õ–ò –ù–ï –†–û–ó–î–Ü–õ–Ø–¢–ò:
   - –Ø–∫—â–æ —Ü–µ —Ä–∏—Ç–æ—Ä–∏—á–Ω–µ –ø–∏—Ç–∞–Ω–Ω—è –∑ –≤—ñ–¥–ø–æ–≤—ñ–¥–¥—é –≤—ñ–¥ —Ç–æ–≥–æ –∂ —Å–ø—ñ–∫–µ—Ä–∞
   - –Ø–∫—â–æ —Ü–µ —Å–∞–º–æ–ø–µ—Ä–µ–ø–∏—Ç—É–≤–∞–Ω–Ω—è (—Å–ø—ñ–∫–µ—Ä —Å–∞–º —Å–æ–±—ñ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î)

–ü–æ–≤–µ—Ä–Ω–∏ JSON —É —Ñ–æ—Ä–º–∞—Ç—ñ:
{
  "should_split": true/false,
  "parts": [
    {"text": "–ø–µ—Ä—à–∞ —á–∞—Å—Ç–∏–Ω–∞", "speaker": 0 –∞–±–æ 1},
    {"text": "–¥—Ä—É–≥–∞ —á–∞—Å—Ç–∏–Ω–∞", "speaker": 0 –∞–±–æ 1}
  ]
}

–í–ê–ñ–õ–ò–í–û: –Ø–∫—â–æ –±–∞—á–∏—à –ø–∏—Ç–∞–Ω–Ω—è + –≤—ñ–¥–ø–æ–≤—ñ–¥—å –∑ "Uh", "Um", "per second" —Ç–æ—â–æ - –ó–ê–í–ñ–î–ò —Ä–æ–∑–¥—ñ–ª—è–π!"""

    user_prompt = f"""–ê–Ω–∞–ª—ñ–∑—É–π —Ü–µ–π —Å–µ–≥–º–µ–Ω—Ç —ñ –≤–∏–∑–Ω–∞—á, —á–∏ –ø–æ—Ç—Ä—ñ–±–Ω–æ –π–æ–≥–æ —Ä–æ–∑–¥—ñ–ª–∏—Ç–∏:

–°–µ–≥–º–µ–Ω—Ç –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É:
- –°–ø—ñ–∫–µ—Ä (–ø–æ—Ç–æ—á–Ω–∏–π): {segment['speaker']}
- –¢–µ–∫—Å—Ç: "{segment.get('text', '')}"
- –ß–∞—Å: {segment['start']:.2f}s - {segment['end']:.2f}s

–ö–æ–Ω—Ç–µ–∫—Å—Ç –¥—ñ–∞–ª–æ–≥—É (–≤—Å—ñ —Å–µ–≥–º–µ–Ω—Ç–∏):{context_segments}

–û—Å–Ω–æ–≤–Ω–∏–π —Å–ø—ñ–∫–µ—Ä (–±—ñ–ª—å—à–µ —Å–ª—ñ–≤): {main_speaker}
–†–æ–∑–ø–æ–¥—ñ–ª —Å–ª—ñ–≤ –ø–æ —Å–ø—ñ–∫–µ—Ä–∞—Ö: {speaker_word_counts}

–í–∏–∑–Ω–∞—á, —á–∏ —Ü–µ–π —Å–µ–≥–º–µ–Ω—Ç –º—ñ—Å—Ç–∏—Ç—å –ø–∏—Ç–∞–Ω–Ω—è + –≤—ñ–¥–ø–æ–≤—ñ–¥—å –≤—ñ–¥ —Ä—ñ–∑–Ω–∏—Ö —Å–ø—ñ–∫–µ—Ä—ñ–≤. –Ø–∫—â–æ —Ç–∞–∫, —Ä–æ–∑–¥—ñ–ª–∏ –π–æ–≥–æ –Ω–∞ —á–∞—Å—Ç–∏–Ω–∏ —Ç–∞ –≤–∏–∑–Ω–∞—á –ø—Ä–∞–≤–∏–ª—å–Ω–∏—Ö —Å–ø—ñ–∫–µ—Ä—ñ–≤ –¥–ª—è –∫–æ–∂–Ω–æ—ó —á–∞—Å—Ç–∏–Ω–∏.

–ü–æ–≤–µ—Ä–Ω–∏ –¢–Ü–õ–¨–ö–ò JSON –±–µ–∑ –¥–æ–¥–∞—Ç–∫–æ–≤–∏—Ö –ø–æ—è—Å–Ω–µ–Ω—å."""

    try:
        # –§–æ—Ä–º—É—î–º–æ URL –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ LLM
        if use_local_llm:
            if not llm_api_url.startswith('http'):
                llm_api_url = f"http://{llm_api_url}"
            llm_api_url = llm_api_url.rstrip('/')
            if not llm_api_url.endswith('/v1/chat/completions'):
                llm_api_url = f"{llm_api_url}/v1/chat/completions"
        
        # –§–æ—Ä–º—É—î–º–æ –∑–∞–≥–æ–ª–æ–≤–∫–∏
        headers = {"Content-Type": "application/json"}
        if llm_api_key:
            headers["Authorization"] = f"Bearer {llm_api_key}"
        if not use_local_llm:
            headers["HTTP-Referer"] = os.getenv('APP_URL', 'http://localhost:5005')
            headers["X-Title"] = "Segment Splitting"
        
        # –§–æ—Ä–º—É—î–º–æ payload
        payload = {
            "model": llm_model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            "temperature": 0,
            "max_tokens": 500
        }
        
        # –í–∏–∫–ª–∏–∫–∞—î–º–æ LLM
        timeout = 30 if use_local_llm else 10
        print(f"üì§ [LLM Split] –í—ñ–¥–ø—Ä–∞–≤–ª—è—î–º–æ –∑–∞–ø–∏—Ç –¥–ª—è —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è —Å–µ–≥–º–µ–Ω—Ç–∞: '{segment.get('text', '')[:50]}...'")
        sys.stdout.flush()
        
        response = requests.post(llm_api_url, json=payload, headers=headers, timeout=timeout)
        
        if response.status_code == 200:
            response_data = response.json()
            content = response_data.get("choices", [{}])[0].get("message", {}).get("content", "").strip()
            
            print(f"üìù [LLM Split] –í—ñ–¥–ø–æ–≤—ñ–¥—å LLM: {content[:200]}...")
            sys.stdout.flush()
            
            # –ü–∞—Ä—Å–∏–º–æ JSON –≤—ñ–¥–ø–æ–≤—ñ–¥—å
            try:
                import json
                # –í–∏–¥–∞–ª—è—î–º–æ markdown code blocks —è–∫—â–æ —î
                if content.startswith('```'):
                    content = content.split('```')[1]
                    if content.startswith('json'):
                        content = content[4:]
                content = content.strip()
                
                result = json.loads(content)
                
                if result.get('should_split') and result.get('parts'):
                    # –î–æ–¥–∞—î–º–æ timestamps –¥–æ —á–∞—Å—Ç–∏–Ω (–ø—Ä–∏–±–ª–∏–∑–Ω–æ, –Ω–∞ –æ—Å–Ω–æ–≤—ñ –¥–æ–≤–∂–∏–Ω–∏ —Ç–µ–∫—Å—Ç—É)
                    total_duration = segment['end'] - segment['start']
                    total_text_length = len(segment.get('text', ''))
                    
                    parts_with_timestamps = []
                    current_time = segment['start']
                    
                    for i, part in enumerate(result['parts']):
                        part_text = part.get('text', '').strip()
                        if not part_text:
                            continue
                        
                        # –ü—Ä–∏–±–ª–∏–∑–Ω–∞ —Ç—Ä–∏–≤–∞–ª—ñ—Å—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤—ñ –¥–æ–≤–∂–∏–Ω–∏ —Ç–µ–∫—Å—Ç—É
                        if total_text_length > 0:
                            part_ratio = len(part_text) / total_text_length
                        else:
                            part_ratio = 1.0 / len(result['parts'])
                        
                        part_duration = total_duration * part_ratio
                        part_start = current_time
                        part_end = current_time + part_duration
                        
                        parts_with_timestamps.append({
                            'text': part_text,
                            'speaker': int(part.get('speaker', segment['speaker'])),
                            'start': round(part_start, 2),
                            'end': round(part_end, 2)
                        })
                        
                        current_time = part_end
                    
                    print(f"‚úÖ [LLM Split] –†–æ–∑–¥—ñ–ª–µ–Ω–æ –Ω–∞ {len(parts_with_timestamps)} —á–∞—Å—Ç–∏–Ω")
                    sys.stdout.flush()
                    
                    return {
                        'should_split': True,
                        'parts': parts_with_timestamps
                    }
                else:
                    print(f"‚ÑπÔ∏è [LLM Split] LLM –≤–∏–∑–Ω–∞—á–∏–≤, —â–æ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –Ω–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ")
                    sys.stdout.flush()
                    return {
                        'should_split': False,
                        'parts': []
                    }
            except (json.JSONDecodeError, KeyError, ValueError) as e:
                print(f"‚ö†Ô∏è [LLM Split] –ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É JSON: {e}, –≤—ñ–¥–ø–æ–≤—ñ–¥—å: {content[:200]}")
                sys.stdout.flush()
                return None
        else:
            print(f"‚ö†Ô∏è [LLM Split] LLM API –ø–æ–≤–µ—Ä–Ω—É–≤ —Å—Ç–∞—Ç—É—Å {response.status_code}: {response.text[:200]}")
            sys.stdout.flush()
            return None
            
    except requests.exceptions.Timeout:
        print(f"‚ö†Ô∏è [LLM Split] LLM API timeout")
        sys.stdout.flush()
        return None
    except Exception as e:
        print(f"‚ö†Ô∏è [LLM Split] –ü–æ–º–∏–ª–∫–∞ –∑–∞–ø–∏—Ç—É: {e}")
        import traceback
        traceback.print_exc()
        sys.stdout.flush()
        return None


def call_llm_for_speaker_correction(prev_seg, current_seg, gap_to_prev, all_segments_context=None, mode='local'):
    """
    –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î LLM —è–∫ –ø–æ—Å–µ—Ä–µ–¥–Ω–∏–∫–∞ –¥–ª—è –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ –¥–ª—è –ø–æ—Ç–æ—á–Ω–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞.
    
    Args:
        prev_seg: –ü–æ–ø–µ—Ä–µ–¥–Ω—ñ–π —Å–µ–≥–º–µ–Ω—Ç {'speaker': int, 'start': float, 'end': float, 'text': str}
        current_seg: –ü–æ—Ç–æ—á–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç {'speaker': int, 'start': float, 'end': float, 'text': str}
        gap_to_prev: Gap –º—ñ–∂ —Å–µ–≥–º–µ–Ω—Ç–∞–º–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        all_segments_context: –û–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ, —Å–ø–∏—Å–æ–∫ –≤—Å—ñ—Ö —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç—É
        mode: –†–µ–∂–∏–º LLM ('local', 'fast', 'smart', 'smart-2', 'test', 'test2')
    
    Returns:
        int or None: –ü—Ä–∞–≤–∏–ª—å–Ω–∏–π —Å–ø—ñ–∫–µ—Ä –¥–ª—è –ø–æ—Ç–æ—á–Ω–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞, –∞–±–æ None —è–∫—â–æ LLM –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π
    """
    # –í–∏–∑–Ω–∞—á–∞—î–º–æ, —á–∏ —Ü–µ –ª–æ–∫–∞–ª—å–Ω–∏–π LLM
    use_local_llm = mode == 'local' or mode == 'test' or mode == 'test2'
    
    # –û—Ç—Ä–∏–º—É—î–º–æ –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤—ñ —Ä–µ–∂–∏–º—É
    llm_model = get_model_id(mode)
    
    # –í–∏–∑–Ω–∞—á–∞—î–º–æ API URL —Ç–∞ –∫–ª—é—á
    if use_local_llm:
        llm_api_url = os.getenv('LOCAL_LLM_BASE_URL') or 'http://127.0.0.1:3001'
        llm_api_key = os.getenv('LOCAL_LLM_API_KEY') or ''
    else:
        llm_api_url = 'https://openrouter.ai/api/v1/chat/completions'
        llm_api_key = os.getenv('OPENROUTER_API_KEY') or ''
    
    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ –Ω–∞–ª–∞—à—Ç–æ–≤–∞–Ω–∏–π LLM
    if use_local_llm:
        if not llm_api_url:
            print(f"‚ö†Ô∏è LLM –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π: LOCAL_LLM_BASE_URL –Ω–µ –Ω–∞–ª–∞—à—Ç–æ–≤–∞–Ω–∏–π")
            sys.stdout.flush()
            return None
        print(f"üîç [LLM] –í–∏–∫–ª–∏–∫–∞—î–º–æ –ª–æ–∫–∞–ª—å–Ω–∏–π LLM: {llm_api_url}, –º–æ–¥–µ–ª—å: {llm_model}")
    else:
        if not llm_api_key:
            print(f"‚ö†Ô∏è LLM –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π: OPENROUTER_API_KEY –Ω–µ –Ω–∞–ª–∞—à—Ç–æ–≤–∞–Ω–∏–π")
            sys.stdout.flush()
            return None
        print(f"üîç [LLM] –í–∏–∫–ª–∏–∫–∞—î–º–æ OpenRouter LLM: {llm_api_url}, –º–æ–¥–µ–ª—å: {llm_model}")
    
    sys.stdout.flush()
    
    # –§–æ—Ä–º—É—î–º–æ –ø—Ä–æ–º–ø—Ç –¥–ª—è LLM
    prev_duration = prev_seg['end'] - prev_seg['start']
    current_duration = current_seg['end'] - current_seg['start']
    
    context_info = ""
    if all_segments_context:
        # –î–æ–¥–∞—î–º–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–æ —Ä–æ–∑–ø–æ–¥—ñ–ª —Å–ø—ñ–∫–µ—Ä—ñ–≤
        speaker_word_counts = {}
        for seg in all_segments_context:
            speaker = seg['speaker']
            word_count = len(seg.get('text', '').split())
            speaker_word_counts[speaker] = speaker_word_counts.get(speaker, 0) + word_count
        
        main_speaker = max(speaker_word_counts.items(), key=lambda x: x[1])[0] if speaker_word_counts else 0
        context_info = f"\n\n–ö–æ–Ω—Ç–µ–∫—Å—Ç –¥—ñ–∞–ª–æ–≥—É:\n- –û—Å–Ω–æ–≤–Ω–∏–π —Å–ø—ñ–∫–µ—Ä (–±—ñ–ª—å—à–µ —Å–ª—ñ–≤): {main_speaker}\n- –†–æ–∑–ø–æ–¥—ñ–ª —Å–ª—ñ–≤ –ø–æ —Å–ø—ñ–∫–µ—Ä–∞—Ö: {speaker_word_counts}"
    
    system_prompt = """–¢–∏ –µ–∫—Å–ø–µ—Ä—Ç –∑ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó —Å–ø—ñ–∫–µ—Ä—ñ–≤. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –≤–∏–∑–Ω–∞—á–∏—Ç–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–∏–π —Å–ø—ñ–∫–µ—Ä –¥–ª—è –ø–æ—Ç–æ—á–Ω–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –¥—ñ–∞–ª–æ–≥—É —Ç–∞ –º–æ–≤–Ω–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω—ñ–≤.

–í–ê–ñ–õ–ò–í–Ü –ü–†–ê–í–ò–õ–ê –î–õ–Ø –ê–ù–ê–õ–Ü–ó–£:

1. –ö–û–†–û–¢–ö–Ü –†–ï–ü–õ–Ü–ö–ò –¢–ê –ü–ò–¢–ê–ù–ù–Ø:
   - –Ø–∫—â–æ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π —Å–µ–≥–º–µ–Ω—Ç –∫–æ—Ä–æ—Ç–∫–∏–π (<1 —Å–µ–∫—É–Ω–¥–∞) —ñ –Ω–∞–ª–µ–∂–∏—Ç—å –æ–¥–Ω–æ–º—É —Å–ø—ñ–∫–µ—Ä—É,
   - –ê –ø–æ—Ç–æ—á–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç —î –ø–∏—Ç–∞–Ω–Ω—è–º –∞–±–æ —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—î—é (–ø–æ—á–∏–Ω–∞—î—Ç—å—Å—è –∑ "Hey", "Did you", "Can you", "Try to", "What", "How", "Why", "When", "Where", "You should", "You can", "You need", –º—ñ—Å—Ç–∏—Ç—å "?"),
   - –Ü gap –º—ñ–∂ —Å–µ–≥–º–µ–Ω—Ç–∞–º–∏ <3 —Å–µ–∫—É–Ω–¥–∏,
   - –¢–æ –ø–æ—Ç–æ—á–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç –º–∞—î –Ω–∞–ª–µ–∂–∞—Ç–∏ –û–°–ù–û–í–ù–û–ú–£ —Å–ø—ñ–∫–µ—Ä—É (—Ç–æ–º—É, —Ö—Ç–æ –≤–µ–¥–µ –¥—ñ–∞–ª–æ–≥, –∑–∞–¥–∞—î –ø–∏—Ç–∞–Ω–Ω—è, –¥–∞—î —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—ó).

2. –°–õ–û–í–ê –ù–ê –ü–û–ß–ê–¢–ö–£ –§–ê–ô–õ–£:
   - –Ø–∫—â–æ —Å–ª–æ–≤–æ –∑–Ω–∞—Ö–æ–¥–∏—Ç—å—Å—è –Ω–∞ –ø–æ—á–∞—Ç–∫—É —Ñ–∞–π–ª—É (<3 —Å–µ–∫—É–Ω–¥–∏ –≤—ñ–¥ –ø–æ—á–∞—Ç–∫—É),
   - –Ü —î –ø–∏—Ç–∞–Ω–Ω—è–º/—ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—î—é (–ø–æ—á–∏–Ω–∞—î—Ç—å—Å—è –∑ "Hey", "Did", "Can", "Try", "What", "How", "Why", "When", "Where", "You"),
   - –¢–æ –≤–æ–Ω–æ –º–∞—î –Ω–∞–ª–µ–∂–∞—Ç–∏ –æ—Å–Ω–æ–≤–Ω–æ–º—É —Å–ø—ñ–∫–µ—Ä—É.

3. –ö–û–ù–¢–ï–ö–°–¢–ù–Ü –ó–í'–Ø–ó–ö–ò:
   - –Ø–∫—â–æ –ø–æ–ø–µ—Ä–µ–¥–Ω—î —ñ –Ω–∞—Å—Ç—É–ø–Ω–µ —Å–ª–æ–≤–∞ –Ω–∞–ª–µ–∂–∞—Ç—å –æ–¥–Ω–æ–º—É —Å–ø—ñ–∫–µ—Ä—É, —ñ gap <2 —Å–µ–∫—É–Ω–¥–∏,
   - –ü–æ—Ç–æ—á–Ω–µ —Å–ª–æ–≤–æ —Ç–µ–∂ –º–∞—î –Ω–∞–ª–µ–∂–∞—Ç–∏ —Ç–æ–º—É —Å–ø—ñ–∫–µ—Ä—É (—Ü–µ –≤–∏—Ä—ñ—à—É—î –ø—Ä–æ–±–ª–µ–º—É –∑ —Ñ—Ä–∞–∑–∞–º–∏ –º—ñ–∂ —Ä–µ–ø–ª—ñ–∫–∞–º–∏ –æ–¥–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞).

4. –ë–Ü–î–ò–†–ï–ö–¶–Ü–ô–ù–ò–ô –ö–û–ù–¢–ï–ö–°–¢:
   - –Ø–∫—â–æ —Å–ª–æ–≤–æ –æ—Ç–æ—á–µ–Ω–µ —Å–ª–æ–≤–∞–º–∏ –æ–¥–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ –∑ –æ–±–æ—Ö –±–æ–∫—ñ–≤ (gap <2 —Å–µ–∫—É–Ω–¥–∏),
   - –í–æ–Ω–æ –º–∞—î –Ω–∞–ª–µ–∂–∞—Ç–∏ —Ç–æ–º—É –∂ —Å–ø—ñ–∫–µ—Ä—É.

5. –í–ò–ó–ù–ê–ß–ï–ù–ù–Ø –û–°–ù–û–í–ù–û–ì–û –°–ü–Ü–ö–ï–†–ê:
   - –û—Å–Ω–æ–≤–Ω–∏–π —Å–ø—ñ–∫–µ—Ä - —Ü–µ —Ç–æ–π, —Ö—Ç–æ –≤–µ–¥–µ –¥—ñ–∞–ª–æ–≥, –∑–∞–¥–∞—î –ø–∏—Ç–∞–Ω–Ω—è, –¥–∞—î —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—ó, –º–∞—î –±—ñ–ª—å—à–µ —Å–ª—ñ–≤ —É –¥—ñ–∞–ª–æ–∑—ñ.
   - –í—ñ–Ω –∑–∞–∑–≤–∏—á–∞–π –ø—Ä–æ–¥–æ–≤–∂—É—î —Ä–æ–∑–º–æ–≤—É –ø—ñ—Å–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ä–µ–ø–ª—ñ–∫ —ñ–Ω—à–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞.

6. –ó–ê–ü–ï–†–ï–ß–ï–ù–ù–Ø –¢–ê –í–ò–ü–†–ê–í–õ–ï–ù–ù–Ø:
   - –Ø–∫—â–æ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π —Å–µ–≥–º–µ–Ω—Ç –º—ñ—Å—Ç–∏—Ç—å –ø–∏—Ç–∞–Ω–Ω—è (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, "What speed does it show?") –∞–±–æ —Ä–µ–ø–ª—ñ–∫—É –∑ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—î—é (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, "Uh, per second."),
   - –ê –ø–æ—Ç–æ—á–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç –ø–æ—á–∏–Ω–∞—î—Ç—å—Å—è –∑ "No" –∞–±–æ –ø–æ–¥—ñ–±–Ω–∏—Ö –∑–∞–ø–µ—Ä–µ—á–µ–Ω—å (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, "No, it should be 200."),
   - –¢–æ —Ü–µ –ó–ê–ó–í–ò–ß–ê–ô –≤—ñ–¥–ø–æ–≤—ñ–¥—å –≤—ñ–¥ –Ü–ù–®–û–ì–û —Å–ø—ñ–∫–µ—Ä–∞, —è–∫–∏–π –≤–∏–ø—Ä–∞–≤–ª—è—î –∞–±–æ —É—Ç–æ—á–Ω—é—î —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é.
   - –ü–†–ò–ö–õ–ê–î: "What speed does it show?" (—Å–ø—ñ–∫–µ—Ä 0) ‚Üí "Uh, per second." (—Å–ø—ñ–∫–µ—Ä 1) ‚Üí "No, it should be 200." (—Å–ø—ñ–∫–µ—Ä 0 –∞–±–æ 1?)
   - –í —Ç–∞–∫–æ–º—É –≤–∏–ø–∞–¥–∫—É "No, it should be 200" –∑–∞–∑–≤–∏—á–∞–π –Ω–∞–ª–µ–∂–∏—Ç—å —Ç–æ–º—É –∂ —Å–ø—ñ–∫–µ—Ä—É, —â–æ —ñ –ø–æ–ø–µ—Ä–µ–¥–Ω—è –≤—ñ–¥–ø–æ–≤—ñ–¥—å ("Uh, per second"), –∞–±–æ —ñ–Ω—à–æ–º—É —Å–ø—ñ–∫–µ—Ä—É, —è–∫–∏–π –≤–∏–ø—Ä–∞–≤–ª—è—î.
   - –ê–ù–ê–õ–Ü–ó–£–ô –ö–û–ù–¢–ï–ö–°–¢: —è–∫—â–æ –ø–æ–ø–µ—Ä–µ–¥–Ω—è –≤—ñ–¥–ø–æ–≤—ñ–¥—å –±—É–ª–∞ –≤—ñ–¥ –Ω–µ–æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞, —Ç–æ "No" –º–æ–∂–µ –±—É—Ç–∏ –≤—ñ–¥ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ (–≤–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è), –∞–±–æ –≤—ñ–¥ —Ç–æ–≥–æ –∂ –Ω–µ–æ—Å–Ω–æ–≤–Ω–æ–≥–æ (—É—Ç–æ—á–Ω–µ–Ω–Ω—è).

7. –û–°–û–ë–õ–ò–í–Ü –í–ò–ü–ê–î–ö–ò:
   - –§—Ä–∞–∑–∏ —Ç–∏–ø—É "Hey, did you try to reset your modem?" –ø—ñ—Å–ª—è –∫–æ—Ä–æ—Ç–∫–æ—ó —Ä–µ–ø–ª—ñ–∫–∏ —ñ–Ω—à–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ –Ω–∞–ª–µ–∂–∞—Ç—å –æ—Å–Ω–æ–≤–Ω–æ–º—É —Å–ø—ñ–∫–µ—Ä—É (–ø—Ä–æ–¥–æ–≤–∂–µ–Ω–Ω—è –¥—ñ–∞–ª–æ–≥—É).

–ê–ù–ê–õ–Ü–ó–£–ô –ö–û–ù–¢–ï–ö–°–¢:
- –†–æ–∑–ø–æ–¥—ñ–ª —Å–ª—ñ–≤ –ø–æ —Å–ø—ñ–∫–µ—Ä–∞—Ö (—Ö—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç—å –±—ñ–ª—å—à–µ)
- –•—Ç–æ –∑–∞–¥–∞—î –ø–∏—Ç–∞–Ω–Ω—è —Ç–∞ –¥–∞—î —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—ó
- –•—Ç–æ –ø—Ä–æ–¥–æ–≤–∂—É—î –¥—ñ–∞–ª–æ–≥ –ø—ñ—Å–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ä–µ–ø–ª—ñ–∫
- –¢—Ä–∏–≤–∞–ª—ñ—Å—Ç—å —Ç–∞ gap –º—ñ–∂ —Å–µ–≥–º–µ–Ω—Ç–∞–º–∏
- –ú–æ–≤–Ω—ñ –ø–∞—Ç—Ç–µ—Ä–Ω–∏ (–ø–∏—Ç–∞–Ω–Ω—è, —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—ó, –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ)

–ü–æ–≤–µ—Ä–Ω–∏ –¢–Ü–õ–¨–ö–ò –Ω–æ–º–µ—Ä —Å–ø—ñ–∫–µ—Ä–∞ (0 –∞–±–æ 1) –±–µ–∑ –¥–æ–¥–∞—Ç–∫–æ–≤–∏—Ö –ø–æ—è—Å–Ω–µ–Ω—å."""

    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —î –ø–∏—Ç–∞–Ω–Ω—è –≤ –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ–º—É —Å–µ–≥–º–µ–Ω—Ç—ñ
    prev_text = prev_seg.get('text', '').strip()
    has_question = '?' in prev_text
    
    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ –ø–æ—Ç–æ—á–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç –ø–æ—á–∏–Ω–∞—î—Ç—å—Å—è –∑ –∑–∞–ø–µ—Ä–µ—á–µ–Ω–Ω—è
    current_text = current_seg.get('text', '').strip().lower()
    starts_with_negation = any(
        current_text.startswith(neg) for neg in ['no,', 'no ', 'nope,', 'nope ', 'nah,', 'nah ', 'not,', 'not ']
    )
    
    # –î–æ–¥–∞—î–º–æ —Å–ø–µ—Ü—ñ–∞–ª—å–Ω—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –¥–ª—è –≤–∏–ø–∞–¥–∫—ñ–≤ –∑ –∑–∞–ø–µ—Ä–µ—á–µ–Ω–Ω—è–º–∏
    negation_info = ""
    if has_question and starts_with_negation:
        negation_info = "\n\n‚ö†Ô∏è –í–ê–ñ–õ–ò–í–û: –ü–æ–ø–µ—Ä–µ–¥–Ω—ñ–π —Å–µ–≥–º–µ–Ω—Ç –º—ñ—Å—Ç–∏—Ç—å –ø–∏—Ç–∞–Ω–Ω—è, –∞ –ø–æ—Ç–æ—á–Ω–∏–π –ø–æ—á–∏–Ω–∞—î—Ç—å—Å—è –∑ –∑–∞–ø–µ—Ä–µ—á–µ–Ω–Ω—è (No/Nope/Nah/Not).\n–¶–µ –∑–∞–∑–≤–∏—á–∞–π –æ–∑–Ω–∞—á–∞—î, —â–æ –ø–æ—Ç–æ—á–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç - —Ü–µ –≤—ñ–¥–ø–æ–≤—ñ–¥—å/–≤–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è –≤—ñ–¥ –Ü–ù–®–û–ì–û —Å–ø—ñ–∫–µ—Ä–∞, –∞–ª–µ –ø—Ä–æ–∞–Ω–∞–ª—ñ–∑—É–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥—ñ–∞–ª–æ–≥—É, —â–æ–± –≤–∏–∑–Ω–∞—á–∏—Ç–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞."
    
    user_prompt = f"""–ü–æ–ø–µ—Ä–µ–¥–Ω—ñ–π —Å–µ–≥–º–µ–Ω—Ç:
- –°–ø—ñ–∫–µ—Ä: {prev_seg['speaker']}
- –¢–µ–∫—Å—Ç: "{prev_text}"
- –¢—Ä–∏–≤–∞–ª—ñ—Å—Ç—å: {prev_duration:.2f} —Å–µ–∫—É–Ω–¥
- –ß–∞—Å: {prev_seg['start']:.2f}s - {prev_seg['end']:.2f}s
- –ú—ñ—Å—Ç–∏—Ç—å –ø–∏—Ç–∞–Ω–Ω—è: {'–¢–∞–∫' if has_question else '–ù—ñ'}

–ü–æ—Ç–æ—á–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç:
- –°–ø—ñ–∫–µ—Ä (–ø–æ—Ç–æ—á–Ω–∏–π): {current_seg['speaker']}
- –¢–µ–∫—Å—Ç: "{current_seg.get('text', '')}"
- –¢—Ä–∏–≤–∞–ª—ñ—Å—Ç—å: {current_duration:.2f} —Å–µ–∫—É–Ω–¥
- –ß–∞—Å: {current_seg['start']:.2f}s - {current_seg['end']:.2f}s
- –ü–æ—á–∏–Ω–∞—î—Ç—å—Å—è –∑ –∑–∞–ø–µ—Ä–µ—á–µ–Ω–Ω—è: {'–¢–∞–∫' if starts_with_negation else '–ù—ñ'}

Gap –º—ñ–∂ —Å–µ–≥–º–µ–Ω—Ç–∞–º–∏: {gap_to_prev:.2f} —Å–µ–∫—É–Ω–¥{context_info}{negation_info}

–í–∏–∑–Ω–∞—á –ø—Ä–∞–≤–∏–ª—å–Ω–∏–π —Å–ø—ñ–∫–µ—Ä –¥–ª—è –ø–æ—Ç–æ—á–Ω–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –¥—ñ–∞–ª–æ–≥—É. –ü–æ–≤–µ—Ä–Ω–∏ –¢–Ü–õ–¨–ö–ò –Ω–æ–º–µ—Ä —Å–ø—ñ–∫–µ—Ä–∞ (0 –∞–±–æ 1)."""

    try:
        # –§–æ—Ä–º—É—î–º–æ URL –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ LLM
        if use_local_llm:
            # –î–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ LLM –¥–æ–¥–∞—î–º–æ /v1/chat/completions —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ
            if not llm_api_url.startswith('http'):
                llm_api_url = f"http://{llm_api_url}"
            llm_api_url = llm_api_url.rstrip('/')
            if not llm_api_url.endswith('/v1/chat/completions'):
                llm_api_url = f"{llm_api_url}/v1/chat/completions"
        
        # –§–æ—Ä–º—É—î–º–æ –∑–∞–≥–æ–ª–æ–≤–∫–∏
        headers = {"Content-Type": "application/json"}
        if llm_api_key:
            headers["Authorization"] = f"Bearer {llm_api_key}"
        if not use_local_llm:
            headers["HTTP-Referer"] = os.getenv('APP_URL', 'http://localhost:5005')
            headers["X-Title"] = "Speaker Correction"
        
        # –§–æ—Ä–º—É—î–º–æ payload
        payload = {
            "model": llm_model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            "temperature": 0,
            "max_tokens": 10  # –ü–æ—Ç—Ä—ñ–±–Ω–æ —Ç—ñ–ª—å–∫–∏ –Ω–æ–º–µ—Ä —Å–ø—ñ–∫–µ—Ä–∞
        }
        
        # –í–∏–∫–ª–∏–∫–∞—î–º–æ LLM
        timeout = 30 if use_local_llm else 10
        print(f"üì§ [LLM] –í—ñ–¥–ø—Ä–∞–≤–ª—è—î–º–æ –∑–∞–ø–∏—Ç –¥–æ LLM...")
        sys.stdout.flush()
        
        response = requests.post(llm_api_url, json=payload, headers=headers, timeout=timeout)
        
        print(f"üì• [LLM] –û—Ç—Ä–∏–º–∞–Ω–æ –≤—ñ–¥–ø–æ–≤—ñ–¥—å: —Å—Ç–∞—Ç—É—Å {response.status_code}")
        sys.stdout.flush()
        
        if response.status_code == 200:
            try:
                response_data = response.json()
            except ValueError as e:
                print(f"‚ö†Ô∏è [LLM] –ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É JSON –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ: {e}")
                print(f"üìù [LLM] –°–∏—Ä–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å: {response.text[:200]}")
                sys.stdout.flush()
                return None
            
            # –ë–µ–∑–ø–µ—á–Ω–µ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –∫–æ–Ω—Ç–µ–Ω—Ç—É –∑ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞–º–∏
            choices = response_data.get("choices", [])
            if not choices:
                print(f"‚ö†Ô∏è [LLM] –í—ñ–¥–ø–æ–≤—ñ–¥—å –Ω–µ –º—ñ—Å—Ç–∏—Ç—å choices: {response_data}")
                sys.stdout.flush()
                return None
            
            message = choices[0].get("message", {})
            if not message:
                print(f"‚ö†Ô∏è [LLM] Choice –Ω–µ –º—ñ—Å—Ç–∏—Ç—å message: {choices[0]}")
                sys.stdout.flush()
                return None
            
            content = message.get("content", "").strip()
            
            print(f"üìù [LLM] –í—ñ–¥–ø–æ–≤—ñ–¥—å LLM: '{content}' (–¥–æ–≤–∂–∏–Ω–∞: {len(content)})")
            sys.stdout.flush()
            
            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ –ø–æ—Ä–æ–∂–Ω—é –≤—ñ–¥–ø–æ–≤—ñ–¥—å
            if not content:
                print(f"‚ö†Ô∏è [LLM] LLM –ø–æ–≤–µ—Ä–Ω—É–≤ –ø–æ—Ä–æ–∂–Ω—é –≤—ñ–¥–ø–æ–≤—ñ–¥—å. –¶–µ –º–æ–∂–µ –±—É—Ç–∏ –ø–æ–º–∏–ª–∫–∞ LLM.")
                sys.stdout.flush()
                return None
            
            # –ü–∞—Ä—Å–∏–º–æ –≤—ñ–¥–ø–æ–≤—ñ–¥—å (–æ—á—ñ–∫—É—î–º–æ —Ç—ñ–ª—å–∫–∏ –Ω–æ–º–µ—Ä —Å–ø—ñ–∫–µ—Ä–∞)
            try:
                # –í–∏–¥–∞–ª—è—î–º–æ –≤—Å—ñ –Ω–µ—Ü–∏—Ñ—Ä–æ–≤—ñ —Å–∏–º–≤–æ–ª–∏
                speaker_str = ''.join(filter(str.isdigit, content))
                if speaker_str:
                    speaker = int(speaker_str[0])  # –ë–µ—Ä–µ–º–æ –ø–µ—Ä—à—É —Ü–∏—Ñ—Ä—É
                    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ –≤–∞–ª—ñ–¥–Ω—ñ—Å—Ç—å —Å–ø—ñ–∫–µ—Ä–∞ (0 –∞–±–æ 1)
                    if speaker not in [0, 1]:
                        print(f"‚ö†Ô∏è [LLM] LLM –ø–æ–≤–µ—Ä–Ω—É–≤ –Ω–µ–≤–∞–ª—ñ–¥–Ω–∏–π –Ω–æ–º–µ—Ä —Å–ø—ñ–∫–µ—Ä–∞: {speaker} (–æ—á—ñ–∫—É—î—Ç—å—Å—è 0 –∞–±–æ 1)")
                        sys.stdout.flush()
                        return None
                    print(f"‚úÖ [LLM] –í–∏–∑–Ω–∞—á–µ–Ω–æ —Å–ø—ñ–∫–µ—Ä–∞: {speaker} (–±—É–ª–æ: {current_seg['speaker']})")
                    sys.stdout.flush()
                    return speaker
                else:
                    print(f"‚ö†Ô∏è [LLM] LLM –ø–æ–≤–µ—Ä–Ω—É–≤ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –±–µ–∑ —Ü–∏—Ñ—Ä: '{content}'")
                    sys.stdout.flush()
                    return None
            except (ValueError, IndexError, TypeError) as e:
                print(f"‚ö†Ô∏è [LLM] –ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ LLM: {e}, –∫–æ–Ω—Ç–µ–Ω—Ç: '{content}'")
                sys.stdout.flush()
                return None
        else:
            print(f"‚ö†Ô∏è  LLM API –ø–æ–≤–µ—Ä–Ω—É–≤ —Å—Ç–∞—Ç—É—Å {response.status_code}: {response.text[:200]}")
            sys.stdout.flush()
            return None
            
    except requests.exceptions.Timeout:
        print(f"‚ö†Ô∏è  LLM API timeout (–±—ñ–ª—å—à–µ {timeout}s)")
        sys.stdout.flush()
        return None
    except Exception as e:
        print(f"‚ö†Ô∏è  LLM request error: {e}")
        import traceback
        traceback.print_exc()
        sys.stdout.flush()
        return None


def combine_diarization_and_transcription(diarization_segments, words, llm_mode='local'):
    """
    –û–±'—î–¥–Ω—É—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó —Ç–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó –Ω–∞ —Ä—ñ–≤–Ω—ñ —Å–ª—ñ–≤ –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç—ñ.
    
    Args:
        diarization_segments: —Å–µ–≥–º–µ–Ω—Ç–∏ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó [{'speaker': int, 'start': float, 'end': float}]
        words: —Å–ø–∏—Å–æ–∫ —Å–ª—ñ–≤ –∑ timestamps [{'word': str, 'start': float, 'end': float}]
        llm_mode: –†–µ–∂–∏–º LLM –¥–ª—è –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è –ø—Ä–∏–∑–Ω–∞—á–µ–Ω—å —Å–ø—ñ–∫–µ—Ä—ñ–≤ ('local', 'fast', 'smart', 'smart-2')
    
    Returns:
        combined: —Å–ø–∏—Å–æ–∫ –æ–±'—î–¥–Ω–∞–Ω–∏—Ö —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ –∑ —Å–ø—ñ–∫–µ—Ä–æ–º —Ç–∞ —Ç–µ–∫—Å—Ç–æ–º
    """
    if not words:
        print("‚ö†Ô∏è  No words provided for combination")
        return []
    
    if not diarization_segments:
        print("‚ö†Ô∏è  No diarization segments provided, using default speaker 0")
        # –Ø–∫—â–æ –Ω–µ–º–∞—î –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó, –ø–æ–≤–µ—Ä—Ç–∞—î–º–æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—é –∑ –æ–¥–Ω–∏–º —Å–ø—ñ–∫–µ—Ä–æ–º
        combined = []
        current_start = words[0]['start']
        current_words = []
        
        for word in words:
            if not word['word'].strip():
                continue
            current_words.append(word['word'])
            # –°—Ç–≤–æ—Ä—é—î–º–æ —Å–µ–≥–º–µ–Ω—Ç–∏ –ø–æ –ø—Ä–æ–º—ñ–∂–∫–∞—Ö >1 —Å–µ–∫
            if len(combined) == 0 or (word['start'] - combined[-1]['end'] > 1.0):
                if current_words:
                    combined.append({
                        'speaker': 0,
                        'start': round(current_start, 2),
                        'end': round(word['end'], 2),
                        'text': ' '.join(current_words).strip()
                    })
                    current_words = []
                    current_start = word['start']
        
        if current_words:
            combined.append({
                'speaker': 0,
                'start': round(current_start, 2),
                'end': round(words[-1]['end'], 2),
                'text': ' '.join(current_words).strip()
            })
        return combined
    
    print(f"üîó Combining {len(words)} words with {len(diarization_segments)} diarization segments")
    
    # –°–æ—Ä—Ç—É—î–º–æ —Å–µ–≥–º–µ–Ω—Ç–∏ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó –∑–∞ —á–∞—Å–æ–º –¥–ª—è —à–≤–∏–¥—à–æ–≥–æ –ø–æ—à—É–∫—É
    sorted_diar_segments = sorted(diarization_segments, key=lambda x: x['start'])
    
    # –¢–†–Ü–ó: –í–∏–∑–Ω–∞—á–∞—î–º–æ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—ó –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó —Ç–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó
    # –¶–µ –≤–∞–∂–ª–∏–≤–æ, –±–æ —è–∫—â–æ –≤–∏–∑–Ω–∞—á–∞—Ç–∏ –ø—ñ—Å–ª—è –æ–±'—î–¥–Ω–∞–Ω–Ω—è, –º–æ–∂–µ –±—É—Ç–∏ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∏–º, —è–∫—â–æ —Å–ª–æ–≤–∞ –≤–∂–µ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ –ø—Ä–∏–∑–Ω–∞—á–µ–Ω—ñ
    speaker_durations = {}
    for diar_seg in sorted_diar_segments:
        speaker = diar_seg['speaker']
        duration = diar_seg['end'] - diar_seg['start']
        speaker_durations[speaker] = speaker_durations.get(speaker, 0) + duration
    
    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —î —á—ñ—Ç–∫–∏–π –ø–µ—Ä–µ–≤–∞–∂–∞—é—á–∏–π —Å–ø—ñ–∫–µ—Ä –≤ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó
    if speaker_durations:
        max_duration = max(speaker_durations.values())
        total_duration = sum(speaker_durations.values())
        max_ratio = max_duration / total_duration if total_duration > 0 else 0
        
        # –Ø–∫—â–æ —î —á—ñ—Ç–∫–∏–π –ø–µ—Ä–µ–≤–∞–∂–∞—é—á–∏–π —Å–ø—ñ–∫–µ—Ä (>60%), –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—é
        if max_ratio > 0.6:
            main_speaker_from_diarization = max(speaker_durations.items(), key=lambda x: x[1])[0]
            print(f"üëë Main speaker from diarization: {main_speaker_from_diarization} (durations: {speaker_durations}, ratio: {max_ratio:.2%})")
        else:
            # –Ø–∫—â–æ –Ω–µ–º–∞—î —á—ñ—Ç–∫–æ–≥–æ –ø–µ—Ä–µ–≤–∞–∂–∞—é—á–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—é —è–∫ fallback
            # (–ø—ñ–∑–Ω—ñ—à–µ –æ–Ω–æ–≤–∏–º–æ –Ω–∞ –æ—Å–Ω–æ–≤—ñ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó)
            main_speaker_from_diarization = max(speaker_durations.items(), key=lambda x: x[1])[0]
            print(f"üëë Main speaker from diarization (fallback, ratio: {max_ratio:.2%}): {main_speaker_from_diarization} (durations: {speaker_durations})")
    else:
        main_speaker_from_diarization = 0
        print(f"üëë Main speaker from diarization: {main_speaker_from_diarization} (no diarization segments)")
    
    # –î–ª—è –∫–æ–∂–Ω–æ–≥–æ —Å–ª–æ–≤–∞ –∑–Ω–∞—Ö–æ–¥–∏–º–æ –Ω–∞–π–∫—Ä–∞—â–µ –ø–µ—Ä–µ–∫—Ä–∏—Ç—Ç—è –∑ —Å–µ–≥–º–µ–Ω—Ç–∞–º–∏ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó
    word_speakers = []
    for word_idx, word in enumerate(words):
        word_start = word['start']
        word_end = word['end']
        word_center = (word_start + word_end) / 2.0
        word_text = word['word']
        
        if not word_text.strip():
            continue
        
        word_duration = word_end - word_start
        
        # –ö–†–ò–¢–ò–ß–ù–û: –®—É–∫–∞—î–º–æ –≤—Å—ñ —Å–µ–≥–º–µ–Ω—Ç–∏, —è–∫—ñ –ø–µ—Ä–µ—Ç–∏–Ω–∞—é—Ç—å—Å—è –∑ —Å–ª–æ–≤–æ–º (–Ω–µ —Ç—ñ–ª—å–∫–∏ –ø–æ–≤–Ω—ñ—Å—Ç—é –º—ñ—Å—Ç—è—Ç—å)
        # –¶–µ –≤–∞–∂–ª–∏–≤–æ, –±–æ —Å–ª–æ–≤–∞ –º–æ–∂—É—Ç—å —á–∞—Å—Ç–∫–æ–≤–æ –ø–µ—Ä–µ—Ç–∏–Ω–∞—Ç–∏—Å—è –∑ —Å–µ–≥–º–µ–Ω—Ç–∞–º–∏ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó
        overlapping_segments = []
        for diar_seg in sorted_diar_segments:
            diar_start = diar_seg['start']
            diar_end = diar_seg['end']
            segment_duration = diar_end - diar_start
            
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –ø–µ—Ä–µ–∫—Ä–∏—Ç—Ç—è (–Ω–µ —Ç—ñ–ª—å–∫–∏ –ø–æ–≤–Ω–µ –≤–º—ñ—â–µ–Ω–Ω—è)
            overlap_start = max(word_start, diar_start)
            overlap_end = min(word_end, diar_end)
            overlap = max(0, overlap_end - overlap_start)
            
            if overlap > 0:
                overlap_ratio = overlap / word_duration if word_duration > 0 else 0
                diar_center = (diar_start + diar_end) / 2.0
                center_distance = abs(word_center - diar_center)
                
                # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —Å–ª–æ–≤–æ –ø–æ–≤–Ω—ñ—Å—Ç—é –≤ –º–µ–∂–∞—Ö (–Ω–∞–π–≤–∏—â–∏–π –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç)
                fully_contained = (word_start >= diar_start and word_end <= diar_end)
                
                # –¢–†–Ü–ó: –í–∏—è–≤–ª–µ–Ω–Ω—è –ø—Ä–æ–±–ª–µ–º–Ω–∏—Ö –≤–µ–ª–∏–∫–∏—Ö —Å–µ–≥–º–µ–Ω—Ç—ñ–≤
                # –°–µ–≥–º–µ–Ω—Ç–∏ >10 —Å–µ–∫—É–Ω–¥ –≤–≤–∞–∂–∞—é—Ç—å—Å—è "–ø—ñ–¥–æ–∑—Ä—ñ–ª–∏–º–∏" - –≤–æ–Ω–∏ –º–æ–∂—É—Ç—å "–ø–æ–≥–ª–∏–Ω–∞—Ç–∏" —Å–ª–æ–≤–∞
                is_suspicious_large = segment_duration > 10.0
                
                overlapping_segments.append({
                    'segment': diar_seg,
                    'speaker': diar_seg['speaker'],
                    'overlap': overlap,
                    'overlap_ratio': overlap_ratio,
                    'center_distance': center_distance,
                    'fully_contained': fully_contained,
                    'segment_duration': segment_duration,
                    'is_suspicious_large': is_suspicious_large
                })
        
        # –Ø–∫—â–æ —î —Å–µ–≥–º–µ–Ω—Ç–∏, —â–æ –ø–µ—Ä–µ—Ç–∏–Ω–∞—é—Ç—å—Å—è, –≤–∏–±–∏—Ä–∞—î–º–æ –Ω–∞–π–∫—Ä–∞—â–∏–π
        if overlapping_segments:
            # –¢–†–Ü–ó –†–Ü–®–ï–ù–ù–Ø: –í—ñ–¥–¥–∞—î–º–æ –ø–µ—Ä–µ–≤–∞–≥—É –Ω–∞–π–±–ª–∏–∂—á–æ–º—É —Å–µ–≥–º–µ–Ω—Ç—É –∑ –¥–æ—Å—Ç–∞—Ç–Ω—ñ–º –ø–µ—Ä–µ–∫—Ä–∏—Ç—Ç—è–º
            # –¶–µ –≤–∏—Ä—ñ—à—É—î –ø—Ä–æ–±–ª–µ–º—É "–ø–æ–≥–ª–∏–Ω–∞–Ω–Ω—è" —Å–ª—ñ–≤ –≤–µ–ª–∏–∫–∏–º–∏ —Å–µ–≥–º–µ–Ω—Ç–∞–º–∏
            
            # –ö–†–ò–¢–ò–ß–ù–û: –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –ø–æ–ø–µ—Ä–µ–¥–Ω—î —Å–ª–æ–≤–æ - —è–∫—â–æ –≤–æ–Ω–æ –≤–∂–µ –ø—Ä–∏–∑–Ω–∞—á–µ–Ω–µ —ñ–Ω—à–æ–º—É —Å–ø—ñ–∫–µ—Ä—É
            # —ñ –ø–æ—Ç–æ—á–Ω–µ —Å–ª–æ–≤–æ –¥—É–∂–µ –±–ª–∏–∑—å–∫–µ (<0.5s), –≤–æ–Ω–∏ –º–∞—é—Ç—å –Ω–∞–ª–µ–∂–∞—Ç–∏ –æ–¥–Ω–æ–º—É —Å–ø—ñ–∫–µ—Ä—É
            if len(word_speakers) > 0:
                prev_word = word_speakers[-1]
                gap_to_prev = word_start - prev_word['end']
                
                # –Ø–∫—â–æ –ø–æ–ø–µ—Ä–µ–¥–Ω—î —Å–ª–æ–≤–æ –¥—É–∂–µ –±–ª–∏–∑—å–∫–µ (<0.5s), –ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —î —Å–µ–≥–º–µ–Ω—Ç —Ç–æ–≥–æ –∂ —Å–ø—ñ–∫–µ—Ä–∞
                if gap_to_prev < 0.5:
                    prev_speaker = prev_word['speaker']
                    current_speakers = set(s['speaker'] for s in overlapping_segments)
                    
                    # –Ø–∫—â–æ –ø–æ–ø–µ—Ä–µ–¥–Ω—î —Å–ª–æ–≤–æ –Ω–∞–ª–µ–∂–∏—Ç—å —Å–ø—ñ–∫–µ—Ä—É, —è–∫–∏–π –Ω–µ –≤ –ø–æ—Ç–æ—á–Ω–∏—Ö overlapping_segments,
                    # –∞–ª–µ —î —Å–µ–≥–º–µ–Ω—Ç —Ç–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ –≤ overlapping_segments, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –π–æ–≥–æ
                    if prev_speaker not in current_speakers:
                        # –®—É–∫–∞—î–º–æ —Å–µ–≥–º–µ–Ω—Ç —Ç–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ –≤ overlapping_segments
                        matching_prev_speaker_segs = [
                            s for s in overlapping_segments 
                            if s['speaker'] == prev_speaker
                        ]
                        if matching_prev_speaker_segs:
                            # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Å–µ–≥–º–µ–Ω—Ç —Ç–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
                            best_seg = min(matching_prev_speaker_segs, key=lambda x: x['center_distance'])
                            speaker_id = best_seg['speaker']
                            word_speakers.append({
                                'word': word_text,
                                'start': word_start,
                                'end': word_end,
                                'speaker': speaker_id,
                                'triz_corrected': True
                            })
                            continue
                    else:
                        # –Ø–∫—â–æ –ø–æ–ø–µ—Ä–µ–¥–Ω—î —Å–ª–æ–≤–æ –Ω–∞–ª–µ–∂–∏—Ç—å —Å–ø—ñ–∫–µ—Ä—É, —è–∫–∏–π —î –≤ overlapping_segments,
                        # –∞–ª–µ –ø–æ—Ç–æ—á–Ω–∏–π –Ω–∞–π–±–ª–∏–∂—á–∏–π —Å–µ–≥–º–µ–Ω—Ç –≤—ñ–¥ —ñ–Ω—à–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ —ñ –ø—ñ–¥–æ–∑—Ä—ñ–ª–∏–π,
                        # –≤—ñ–¥–¥–∞—î–º–æ –ø–µ—Ä–µ–≤–∞–≥—É —Å–µ–≥–º–µ–Ω—Ç—É –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
                        matching_prev_speaker_segs = [
                            s for s in overlapping_segments 
                            if s['speaker'] == prev_speaker
                        ]
                        if matching_prev_speaker_segs:
                            closest_current = min(overlapping_segments, key=lambda x: x['center_distance'])
                            closest_prev = min(matching_prev_speaker_segs, key=lambda x: x['center_distance'])
                            
                            # –Ø–∫—â–æ –Ω–∞–π–±–ª–∏–∂—á–∏–π –ø–æ—Ç–æ—á–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç –ø—ñ–¥–æ–∑—Ä—ñ–ª–∏–π —ñ –≤—ñ–¥ —ñ–Ω—à–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞,
                            # –∞ —Å–µ–≥–º–µ–Ω—Ç –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ –Ω–µ –ø—ñ–¥–æ–∑—Ä—ñ–ª–∏–π, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –π–æ–≥–æ
                            if (closest_current['is_suspicious_large'] and 
                                closest_current['speaker'] != prev_speaker and
                                not closest_prev['is_suspicious_large']):
                                speaker_id = prev_speaker
                                word_speakers.append({
                                    'word': word_text,
                                    'start': word_start,
                                    'end': word_end,
                                    'speaker': speaker_id,
                                    'triz_corrected': True
                                })
                                continue
                        else:
                            # –Ø–∫—â–æ –Ω–µ–º–∞—î —Å–µ–≥–º–µ–Ω—Ç–∞ –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ –≤ overlapping_segments,
                            # –∞–ª–µ gap –¥—É–∂–µ –º–∞–ª–∏–π (<0.3s), –ø—Ä–æ—Å—Ç–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ç–æ–≥–æ –∂ —Å–ø—ñ–∫–µ—Ä–∞
                            # (—Ü–µ –æ–∑–Ω–∞—á–∞—î, —â–æ —Å–ª–æ–≤–∞ –π–¥—É—Ç—å –æ–¥—Ä–∞–∑—É –æ–¥–∏–Ω –∑–∞ –æ–¥–Ω–∏–º —ñ –Ω–∞–ª–µ–∂–∞—Ç—å –æ–¥–Ω–æ–º—É —Å–ø—ñ–∫–µ—Ä—É)
                            if gap_to_prev < 0.3:
                                speaker_id = prev_speaker
                                word_speakers.append({
                                    'word': word_text,
                                    'start': word_start,
                                    'end': word_end,
                                    'speaker': speaker_id,
                                    'triz_corrected': True
                                })
                                continue
            
            # –ö–†–ò–¢–ò–ß–ù–û: –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç - –≤–µ–ª–∏–∫—ñ –ø–∞—É–∑–∏ –º—ñ–∂ —Ä–µ–ø–ª—ñ–∫–∞–º–∏
            # –Ø–∫—â–æ —î –≤–µ–ª–∏–∫–∞ –ø–∞—É–∑–∞ (>3s) –ø–µ—Ä–µ–¥ –∞–±–æ –ø—ñ—Å–ª—è —Å–ª–æ–≤–∞, —ñ —î —Å–µ–≥–º–µ–Ω—Ç —ñ–Ω—à–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ –ø–æ–±–ª–∏–∑—É,
            # —Ü–µ –º–æ–∂–µ –±—É—Ç–∏ –æ–∫—Ä–µ–º–∞ —Ä–µ–ø–ª—ñ–∫–∞ –≤—ñ–¥ —ñ–Ω—à–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
            word_index = [w['word'] for w in words].index(word_text) if word_text in [w['word'] for w in words] else -1
            
            if word_index >= 0:
                # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –ø–∞—É–∑—É –ø–µ—Ä–µ–¥ —Å–ª–æ–≤–æ–º
                gap_before = 0
                if word_index > 0:
                    prev_word = words[word_index - 1]
                    gap_before = word_start - prev_word['end']
                
                # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –ø–∞—É–∑—É –ø—ñ—Å–ª—è —Å–ª–æ–≤–∞
                gap_after = 0
                if word_index < len(words) - 1:
                    next_word = words[word_index + 1]
                    gap_after = next_word['start'] - word_end
                
                # –Ø–∫—â–æ —î –≤–µ–ª–∏–∫–∞ –ø–∞—É–∑–∞ (>3s) –ø–µ—Ä–µ–¥ –∞–±–æ –ø—ñ—Å–ª—è, –ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Å–µ–≥–º–µ–Ω—Ç–∏ —ñ–Ω—à–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
                if gap_before > 3.0 or gap_after > 3.0:
                    current_speakers = set(s['speaker'] for s in overlapping_segments)
                    nearby_other_speaker_segments = []
                    
                    for diar_seg in sorted_diar_segments:
                        if diar_seg['speaker'] not in current_speakers:
                            diar_start = diar_seg['start']
                            diar_end = diar_seg['end']
                            
                            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —Å–µ–≥–º–µ–Ω—Ç –±–ª–∏–∑—å–∫–∏–π –¥–æ —Å–ª–æ–≤–∞ (–≤ –º–µ–∂–∞—Ö –ø–∞—É–∑–∏)
                            if gap_before > 3.0:
                                # –°–µ–≥–º–µ–Ω—Ç –ø–µ—Ä–µ–¥ —Å–ª–æ–≤–æ–º
                                distance_before = word_start - diar_end
                                if 0 <= distance_before < gap_before + 1.0:  # –í –º–µ–∂–∞—Ö –ø–∞—É–∑–∏ + 1s
                                    nearby_other_speaker_segments.append({
                                        'segment': diar_seg,
                                        'speaker': diar_seg['speaker'],
                                        'distance': distance_before,
                                        'type': 'before'
                                    })
                            
                            if gap_after > 3.0:
                                # –°–µ–≥–º–µ–Ω—Ç –ø—ñ—Å–ª—è —Å–ª–æ–≤–∞
                                distance_after = diar_start - word_end
                                if 0 <= distance_after < gap_after + 1.0:  # –í –º–µ–∂–∞—Ö –ø–∞—É–∑–∏ + 1s
                                    nearby_other_speaker_segments.append({
                                        'segment': diar_seg,
                                        'speaker': diar_seg['speaker'],
                                        'distance': distance_after,
                                        'type': 'after'
                                    })
                    
                    # –Ø–∫—â–æ —î —Å–µ–≥–º–µ–Ω—Ç–∏ —ñ–Ω—à–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ –≤ –º–µ–∂–∞—Ö –ø–∞—É–∑–∏, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —ó—Ö
                    if nearby_other_speaker_segments:
                        closest_other = min(nearby_other_speaker_segments, key=lambda x: x['distance'])
                        closest_current = min(overlapping_segments, key=lambda x: x['center_distance'])
                        
                        # –Ø–∫—â–æ —Å–µ–≥–º–µ–Ω—Ç —ñ–Ω—à–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ –¥—É–∂–µ –±–ª–∏–∑—å–∫–∏–π (<2s) —ñ –ø–æ—Ç–æ—á–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç –ø—ñ–¥–æ–∑—Ä—ñ–ª–∏–π,
                        # –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Å–µ–≥–º–µ–Ω—Ç —ñ–Ω—à–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
                        if (closest_other['distance'] < 2.0 and 
                            (closest_current['is_suspicious_large'] or closest_other['distance'] < 1.0)):
                            speaker_id = closest_other['speaker']
                            word_speakers.append({
                                'word': word_text,
                                'start': word_start,
                                'end': word_end,
                                'speaker': speaker_id,
                                'triz_corrected': True
                            })
                            continue
            
            # –ö–†–ò–¢–ò–ß–ù–û: –Ø–∫—â–æ –≤—Å—ñ overlapping —Å–µ–≥–º–µ–Ω—Ç–∏ –ø—ñ–¥–æ–∑—Ä—ñ–ª—ñ (–≤–µ–ª–∏–∫—ñ), –ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —î —Å–µ–≥–º–µ–Ω—Ç–∏ —ñ–Ω—à–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ –ø–æ–±–ª–∏–∑—É
            # (–Ω–∞–≤—ñ—Ç—å —è–∫—â–æ –≤–æ–Ω–∏ –Ω–µ –ø–µ—Ä–µ—Ç–∏–Ω–∞—é—Ç—å—Å—è –±–µ–∑–ø–æ—Å–µ—Ä–µ–¥–Ω—å–æ)
            all_suspicious = all(s['is_suspicious_large'] for s in overlapping_segments)
            if all_suspicious:
                # –®—É–∫–∞—î–º–æ —Å–µ–≥–º–µ–Ω—Ç–∏ —ñ–Ω—à–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ –ø–æ–±–ª–∏–∑—É (–Ω–∞–≤—ñ—Ç—å —è–∫—â–æ –Ω–µ –ø–µ—Ä–µ—Ç–∏–Ω–∞—é—Ç—å—Å—è)
                current_speakers = set(s['speaker'] for s in overlapping_segments)
                nearby_other_speaker_segments = []
                
                for diar_seg in sorted_diar_segments:
                    if diar_seg['speaker'] not in current_speakers:
                        diar_start = diar_seg['start']
                        diar_end = diar_seg['end']
                        diar_center = (diar_start + diar_end) / 2.0
                        
                        # –í—ñ–¥—Å—Ç–∞–Ω—å –≤—ñ–¥ —Ü–µ–Ω—Ç—Ä—É —Å–ª–æ–≤–∞ –¥–æ —Ü–µ–Ω—Ç—Ä—É —Å–µ–≥–º–µ–Ω—Ç–∞
                        center_distance = abs(word_center - diar_center)
                        
                        # –í—ñ–¥—Å—Ç–∞–Ω—å –≤—ñ–¥ –∫—ñ–Ω—Ü—è —Å–µ–≥–º–µ–Ω—Ç–∞ –¥–æ –ø–æ—á–∞—Ç–∫—É —Å–ª–æ–≤–∞ (—è–∫—â–æ —Å–µ–≥–º–µ–Ω—Ç –ø–µ—Ä–µ–¥ —Å–ª–æ–≤–æ–º)
                        distance_before = word_start - diar_end if word_start > diar_end else float('inf')
                        
                        # –í—ñ–¥—Å—Ç–∞–Ω—å –≤—ñ–¥ –∫—ñ–Ω—Ü—è —Å–ª–æ–≤–∞ –¥–æ –ø–æ—á–∞—Ç–∫—É —Å–µ–≥–º–µ–Ω—Ç–∞ (—è–∫—â–æ —Å–µ–≥–º–µ–Ω—Ç –ø—ñ—Å–ª—è —Å–ª–æ–≤–∞)
                        distance_after = diar_start - word_end if diar_start > word_end else float('inf')
                        
                        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —Å–µ–≥–º–µ–Ω—Ç –¥—É–∂–µ –±–ª–∏–∑—å–∫–∏–π (<2s)
                        if center_distance < 2.0 or distance_before < 2.0 or distance_after < 2.0:
                            nearby_other_speaker_segments.append({
                                'segment': diar_seg,
                                'speaker': diar_seg['speaker'],
                                'center_distance': center_distance,
                                'distance_before': distance_before,
                                'distance_after': distance_after,
                                'min_distance': min(center_distance, distance_before, distance_after)
                            })
                
                # –Ø–∫—â–æ —î –±–ª–∏–∑—å–∫—ñ —Å–µ–≥–º–µ–Ω—Ç–∏ —ñ–Ω—à–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –Ω–∞–π–±–ª–∏–∂—á–∏–π
                if nearby_other_speaker_segments:
                    closest_other = min(nearby_other_speaker_segments, key=lambda x: x['min_distance'])
                    closest_suspicious = min(overlapping_segments, key=lambda x: x['center_distance'])
                    
                    # –Ø–∫—â–æ —Å–µ–≥–º–µ–Ω—Ç —ñ–Ω—à–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ –¥—É–∂–µ –±–ª–∏–∑—å–∫–∏–π (<1.5s), –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –π–æ–≥–æ
                    # –û—Å–æ–±–ª–∏–≤–æ —è–∫—â–æ –ø—ñ–¥–æ–∑—Ä—ñ–ª–∏–π —Å–µ–≥–º–µ–Ω—Ç –º–∞—î –≤–µ–ª–∏–∫—É center_distance
                    if (closest_other['min_distance'] < 1.5 and 
                        closest_suspicious['center_distance'] > 1.0):
                        speaker_id = closest_other['speaker']
                        word_speakers.append({
                            'word': word_text,
                            'start': word_start,
                            'end': word_end,
                            'speaker': speaker_id,
                            'triz_corrected': True
                        })
                        continue
            
            # –ö–†–ò–¢–ò–ß–ù–û: –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç - —è–∫—â–æ –Ω–∞—Å—Ç—É–ø–Ω—ñ —Å–ª–æ–≤–∞ –Ω–∞–ª–µ–∂–∞—Ç—å —ñ–Ω—à–æ–º—É —Å–ø—ñ–∫–µ—Ä—É,
            # —ñ –ø–æ—Ç–æ—á–Ω–µ —Å–ª–æ–≤–æ –¥—É–∂–µ –±–ª–∏–∑—å–∫–æ –¥–æ –Ω–∏—Ö (<0.5s), —Ü–µ –º–æ–∂–µ –±—É—Ç–∏ —á–∞—Å—Ç–∏–Ω–∞ —Ä–µ–ø–ª—ñ–∫–∏ —Ç–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
            # –ê–ª–µ —è–∫—â–æ –Ω–∞—Å—Ç—É–ø–Ω—ñ —Å–ª–æ–≤–∞ –Ω–∞–ª–µ–∂–∞—Ç—å –ø–æ—Ç–æ—á–Ω–æ–º—É —Å–ø—ñ–∫–µ—Ä—É, –ø–æ—Ç–æ—á–Ω–µ —Å–ª–æ–≤–æ —Ç–µ–∂ –º–∞—î –Ω–∞–ª–µ–∂–∞—Ç–∏ –π–æ–º—É
            word_index = words.index(word)
            if word_index < len(words) - 1:
                next_word = words[word_index + 1]
                next_word_start = next_word['start']
                gap_to_next = next_word_start - word_end
                
                # –Ø–∫—â–æ –Ω–∞—Å—Ç—É–ø–Ω–µ —Å–ª–æ–≤–æ –¥—É–∂–µ –±–ª–∏–∑—å–∫–æ (<0.5s), –ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –π–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
                # (–∞–ª–µ –º–∏ —â–µ –Ω–µ –∑–Ω–∞—î–º–æ –π–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞, —Ç–æ–º—É –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—é)
                if gap_to_next < 0.5:
                    # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –Ω–∞–π–±–ª–∏–∂—á–∏–π —Å–µ–≥–º–µ–Ω—Ç –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó –¥–ª—è –Ω–∞—Å—Ç—É–ø–Ω–æ–≥–æ —Å–ª–æ–≤–∞
                    next_word_center = (next_word_start + next_word['end']) / 2.0
                    closest_next_seg = None
                    min_next_distance = float('inf')
                    for diar_seg in sorted_diar_segments:
                        if next_word_start < diar_seg['end'] and next_word['end'] > diar_seg['start']:
                            diar_center = (diar_seg['start'] + diar_seg['end']) / 2.0
                            distance = abs(next_word_center - diar_center)
                            if distance < min_next_distance:
                                min_next_distance = distance
                                closest_next_seg = diar_seg
                    
                    # –Ø–∫—â–æ –Ω–∞—Å—Ç—É–ø–Ω–µ —Å–ª–æ–≤–æ –Ω–∞–ª–µ–∂–∏—Ç—å —ñ–Ω—à–æ–º—É —Å–ø—ñ–∫–µ—Ä—É (–Ω–µ —Ç–æ–º—É, —â–æ –≤ overlapping_segments),
                    # —ñ –ø–æ—Ç–æ—á–Ω–µ —Å–ª–æ–≤–æ –ø–µ—Ä–µ—Ç–∏–Ω–∞—î—Ç—å—Å—è –∑ —Å–µ–≥–º–µ–Ω—Ç–æ–º —Ç–æ–≥–æ –∂ —Å–ø—ñ–∫–µ—Ä–∞, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –π–æ–≥–æ
                    if closest_next_seg:
                        next_speaker = closest_next_seg['speaker']
                        current_speakers = set(s['speaker'] for s in overlapping_segments)
                        
                        # –Ø–∫—â–æ –Ω–∞—Å—Ç—É–ø–Ω–µ —Å–ª–æ–≤–æ –Ω–∞–ª–µ–∂–∏—Ç—å —Å–ø—ñ–∫–µ—Ä—É, —è–∫–∏–π –Ω–µ –≤ –ø–æ—Ç–æ—á–Ω–∏—Ö overlapping_segments,
                        # –∞–ª–µ —î —Å–µ–≥–º–µ–Ω—Ç —Ç–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ –≤ overlapping_segments, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –π–æ–≥–æ
                        if next_speaker not in current_speakers:
                            # –®—É–∫–∞—î–º–æ —Å–µ–≥–º–µ–Ω—Ç —Ç–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ –≤ overlapping_segments
                            matching_next_speaker_segs = [
                                s for s in overlapping_segments 
                                if s['speaker'] == next_speaker
                            ]
                            if matching_next_speaker_segs:
                                # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Å–µ–≥–º–µ–Ω—Ç —Ç–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
                                best_seg = min(matching_next_speaker_segs, key=lambda x: x['center_distance'])
                                speaker_id = best_seg['speaker']
                                word_speakers.append({
                                    'word': word_text,
                                    'start': word_start,
                                    'end': word_end,
                                    'speaker': speaker_id,
                                    'triz_corrected': True
                                })
                                continue
            
            # –ö–†–ò–¢–ò–ß–ù–û: –î–ª—è —Å–ª—ñ–≤ –Ω–∞ –ø–æ—á–∞—Ç–∫—É —Ñ–∞–π–ª—É (<3.0s) –ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Å—É—Å—ñ–¥–Ω—ñ —Å–µ–≥–º–µ–Ω—Ç–∏ —ñ–Ω—à–∏—Ö —Å–ø—ñ–∫–µ—Ä—ñ–≤
            # –Ø–∫—â–æ —Å–ª–æ–≤–æ –Ω–∞ –ø–æ—á–∞—Ç–∫—É —ñ —î —Å–µ–≥–º–µ–Ω—Ç —ñ–Ω—à–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞, —è–∫–∏–π –ø–æ—á–∏–Ω–∞—î—Ç—å—Å—è –¥—É–∂–µ –±–ª–∏–∑—å–∫–æ (<0.5s –≤—ñ–¥ —Å–ª–æ–≤–∞),
            # —Ü–µ –º–æ–∂–µ –±—É—Ç–∏ —á–∞—Å—Ç–∏–Ω–∞ —Ä–µ–ø–ª—ñ–∫–∏ —Ç–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞, –Ω–∞–≤—ñ—Ç—å —è–∫—â–æ –ø–æ—Ç–æ—á–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç –º–∞—î overlap
            # –¢–†–Ü–ó: –ó–±—ñ–ª—å—à–µ–Ω–æ –ø–æ—Ä—ñ–≥ –∑ <1.0s –¥–æ <3.0s –¥–ª—è –∫—Ä–∞—â–æ–≥–æ –≤–∏—è–≤–ª–µ–Ω–Ω—è —Ñ—Ä–∞–∑ –ø—ñ—Å–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ä–µ–ø–ª—ñ–∫
            if word_start < 3.0:
                # –®—É–∫–∞—î–º–æ —Å–µ–≥–º–µ–Ω—Ç–∏ —ñ–Ω—à–∏—Ö —Å–ø—ñ–∫–µ—Ä—ñ–≤, —è–∫—ñ –ø–æ—á–∏–Ω–∞—é—Ç—å—Å—è –¥—É–∂–µ –±–ª–∏–∑—å–∫–æ –¥–æ —Å–ª–æ–≤–∞
                nearby_other_speaker_segments = []
                for diar_seg in sorted_diar_segments:
                    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Å–µ–≥–º–µ–Ω—Ç–∏, —è–∫—ñ –ø–æ—á–∏–Ω–∞—é—Ç—å—Å—è –≤ –º–µ–∂–∞—Ö 0.5s –≤—ñ–¥ –∫—ñ–Ω—Ü—è —Å–ª–æ–≤–∞
                    distance_to_start = diar_seg['start'] - word_end
                    if distance_to_start >= 0 and distance_to_start < 0.5:
                        # –¶–µ —Å–µ–≥–º–µ–Ω—Ç, —è–∫–∏–π –ø–æ—á–∏–Ω–∞—î—Ç—å—Å—è –ø—ñ—Å–ª—è —Å–ª–æ–≤–∞, –∞–ª–µ –¥—É–∂–µ –±–ª–∏–∑—å–∫–æ
                        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ –≤—ñ–Ω –≤—ñ–¥ —ñ–Ω—à–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ (–Ω–µ –≤—ñ–¥ —Ç–æ–≥–æ, —â–æ –≤–∂–µ –≤ overlapping)
                        current_speakers = set(s['speaker'] for s in overlapping_segments)
                        if diar_seg['speaker'] not in current_speakers:
                            # –¶–µ —Å–µ–≥–º–µ–Ω—Ç —ñ–Ω—à–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
                            nearby_other_speaker_segments.append({
                                'segment': diar_seg,
                                'speaker': diar_seg['speaker'],
                                'distance_to_start': distance_to_start,
                                'segment_start': diar_seg['start'],
                                'segment_end': diar_seg['end']
                            })
                
                # –Ø–∫—â–æ —î –±–ª–∏–∑—å–∫—ñ —Å–µ–≥–º–µ–Ω—Ç–∏ —ñ–Ω—à–∏—Ö —Å–ø—ñ–∫–µ—Ä—ñ–≤, –ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ –Ω–µ —î —Å–ª–æ–≤–æ —á–∞—Å—Ç–∏–Ω–æ—é —ó—Ö —Ä–µ–ø–ª—ñ–∫–∏
                if nearby_other_speaker_segments:
                    # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –Ω–∞–π–±–ª–∏–∂—á–∏–π —Å–µ–≥–º–µ–Ω—Ç —ñ–Ω—à–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
                    closest_other = min(nearby_other_speaker_segments, key=lambda x: x['distance_to_start'])
                    
                    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –ø–æ—Ç–æ—á–Ω–∏–π –Ω–∞–π–±–ª–∏–∂—á–∏–π —Å–µ–≥–º–µ–Ω—Ç
                    closest_current_seg = min(overlapping_segments, key=lambda x: x['center_distance'])
                    
                    # –¢–†–Ü–ó: –Ø–∫—â–æ –ø–æ—Ç–æ—á–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç –ø–æ—á–∏–Ω–∞—î—Ç—å—Å—è –∑ —Å–∞–º–æ–≥–æ –ø–æ—á–∞—Ç–∫—É —Ñ–∞–π–ª—É (start=0.0),
                    # —Ü–µ –º–æ–∂–µ –±—É—Ç–∏ –ø–æ–º–∏–ª–∫–∞ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó - –≤–æ–Ω–∞ —á–∞—Å—Ç–æ —Å—Ç–≤–æ—Ä—é—î —Å–µ–≥–º–µ–Ω—Ç–∏ –∑ 0.0
                    # –£ —Ç–∞–∫–æ–º—É –≤–∏–ø–∞–¥–∫—É –≤—ñ–¥–¥–∞—î–º–æ –ø–µ—Ä–µ–≤–∞–≥—É —Å–µ–≥–º–µ–Ω—Ç—É —ñ–Ω—à–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞, —è–∫–∏–π –ø–æ—á–∏–Ω–∞—î—Ç—å—Å—è –±–ª–∏–∑—å–∫–æ
                    current_seg_starts_at_zero = closest_current_seg['segment']['start'] < 0.1
                    
                    if closest_other['distance_to_start'] < 0.5:
                        # –¢–†–Ü–ó: –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ç–∏–ø —Ñ—Ä–∞–∑–∏ (–ø–∏—Ç–∞–Ω–Ω—è/—ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—è)
                        # –Ø–∫—â–æ —Å–ª–æ–≤–æ —î —á–∞—Å—Ç–∏–Ω–æ—é –ø–∏—Ç–∞–Ω–Ω—è/—ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—ó, –≤–æ–Ω–æ –º–∞—î –Ω–∞–ª–µ–∂–∞—Ç–∏ –æ—Å–Ω–æ–≤–Ω–æ–º—É —Å–ø—ñ–∫–µ—Ä—É
                        # –û—á–∏—â–∞—î–º–æ –ø—É–Ω–∫—Ç—É–∞—Ü—ñ—é –ø–µ—Ä–µ–¥ –ø–µ—Ä–µ–≤—ñ—Ä–∫–æ—é
                        word_text_clean = clean_punctuation(word_text).lower()
                        is_question_or_instruction = any([
                            word_text_clean.startswith('hey'),
                            word_text_clean.startswith('did'),
                            word_text_clean.startswith('can'),
                            word_text_clean.startswith('try'),
                            word_text_clean.startswith('what'),
                            word_text_clean.startswith('how'),
                            word_text_clean.startswith('why'),
                            word_text_clean.startswith('when'),
                            word_text_clean.startswith('where'),
                        ])
                        
                        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ç—Ä–∏–≤–∞–ª—ñ—Å—Ç—å –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞ —ñ–Ω—à–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
                        prev_seg_duration = closest_other['segment_end'] - closest_other['segment_start']
                        is_prev_short = prev_seg_duration < 1.0
                        
                        # –Ø–∫—â–æ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π —Å–µ–≥–º–µ–Ω—Ç –∫–æ—Ä–æ—Ç–∫–∏–π —ñ –ø–æ—Ç–æ—á–Ω–µ —Å–ª–æ–≤–æ —î –ø–∏—Ç–∞–Ω–Ω—è–º/—ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—î—é,
                        # –≤–æ–Ω–æ –º–∞—î –Ω–∞–ª–µ–∂–∞—Ç–∏ –æ—Å–Ω–æ–≤–Ω–æ–º—É —Å–ø—ñ–∫–µ—Ä—É
                        if is_prev_short and is_question_or_instruction:
                            speaker_id = main_speaker_from_diarization
                            word_speakers.append({
                                'word': word_text,
                                'start': word_start,
                                'end': word_end,
                                'speaker': speaker_id,
                                'triz_corrected': True  # –ü–æ–∑–Ω–∞—á–∫–∞: –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–æ –¢–†–Ü–ó –ª–æ–≥—ñ–∫–æ—é –¥–ª—è —Å–ª—ñ–≤ –Ω–∞ –ø–æ—á–∞—Ç–∫—É (–ø–∏—Ç–∞–Ω–Ω—è/—ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—è)
                            })
                            continue
                        
                        # –°–µ–≥–º–µ–Ω—Ç —ñ–Ω—à–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ –¥—É–∂–µ –±–ª–∏–∑—å–∫–æ - —Ü–µ –π–º–æ–≤—ñ—Ä–Ω–æ —á–∞—Å—Ç–∏–Ω–∞ –π–æ–≥–æ —Ä–µ–ø–ª—ñ–∫–∏
                        # –û—Å–æ–±–ª–∏–≤–æ —è–∫—â–æ –ø–æ—Ç–æ—á–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç –ø–æ—á–∏–Ω–∞—î—Ç—å—Å—è –∑ 0.0 (–ø–æ–º–∏–ª–∫–∞ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó)
                        # –∞–±–æ –ø—ñ–¥–æ–∑—Ä—ñ–ª–∏–π (–≤–µ–ª–∏–∫–∏–π)
                        if (current_seg_starts_at_zero or 
                            closest_current_seg['is_suspicious_large'] or 
                            closest_current_seg['center_distance'] > 0.2):
                            # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Å–µ–≥–º–µ–Ω—Ç —ñ–Ω—à–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
                            speaker_id = closest_other['speaker']
                            word_speakers.append({
                                'word': word_text,
                                'start': word_start,
                                'end': word_end,
                                'speaker': speaker_id,
                                'triz_corrected': True  # –ü–æ–∑–Ω–∞—á–∫–∞: –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–æ –¢–†–Ü–ó –ª–æ–≥—ñ–∫–æ—é –¥–ª—è —Å–ª—ñ–≤ –Ω–∞ –ø–æ—á–∞—Ç–∫—É
                            })
                            continue
            
            # –ö–†–ò–¢–ò–ß–ù–û: –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç - —è–∫—â–æ –Ω–∞—Å—Ç—É–ø–Ω—ñ —Å–ª–æ–≤–∞ –Ω–∞–ª–µ–∂–∞—Ç—å —ñ–Ω—à–æ–º—É —Å–ø—ñ–∫–µ—Ä—É,
            # —ñ –ø–æ—Ç–æ—á–Ω–µ —Å–ª–æ–≤–æ –¥—É–∂–µ –±–ª–∏–∑—å–∫–æ –¥–æ –Ω–∏—Ö (<0.5s), —Ü–µ –º–æ–∂–µ –±—É—Ç–∏ —á–∞—Å—Ç–∏–Ω–∞ —Ä–µ–ø–ª—ñ–∫–∏ —Ç–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
            word_index = [w['word'] for w in words].index(word_text) if word_text in [w['word'] for w in words] else -1
            if word_index >= 0 and word_index < len(words) - 1:
                next_word = words[word_index + 1]
                next_word_start = next_word['start']
                gap_to_next = next_word_start - word_end
                
                # –Ø–∫—â–æ –Ω–∞—Å—Ç—É–ø–Ω–µ —Å–ª–æ–≤–æ –¥—É–∂–µ –±–ª–∏–∑—å–∫–æ (<0.5s), –ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –π–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ —á–µ—Ä–µ–∑ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—é
                if gap_to_next < 0.5:
                    # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –Ω–∞–π–±–ª–∏–∂—á–∏–π —Å–µ–≥–º–µ–Ω—Ç –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó –¥–ª—è –Ω–∞—Å—Ç—É–ø–Ω–æ–≥–æ —Å–ª–æ–≤–∞
                    next_word_center = (next_word_start + next_word['end']) / 2.0
                    closest_next_seg = None
                    min_next_distance = float('inf')
                    for diar_seg in sorted_diar_segments:
                        if next_word_start < diar_seg['end'] and next_word['end'] > diar_seg['start']:
                            diar_center = (diar_seg['start'] + diar_seg['end']) / 2.0
                            distance = abs(next_word_center - diar_center)
                            if distance < min_next_distance:
                                min_next_distance = distance
                                closest_next_seg = diar_seg
                    
                    # –Ø–∫—â–æ –Ω–∞—Å—Ç—É–ø–Ω–µ —Å–ª–æ–≤–æ –Ω–∞–ª–µ–∂–∏—Ç—å —ñ–Ω—à–æ–º—É —Å–ø—ñ–∫–µ—Ä—É, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Å–µ–≥–º–µ–Ω—Ç —Ç–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
                    if closest_next_seg:
                        next_speaker = closest_next_seg['speaker']
                        # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –Ω–∞–π–±–ª–∏–∂—á–∏–π —Å–µ–≥–º–µ–Ω—Ç –¥–ª—è –ø–æ—Ç–æ—á–Ω–æ–≥–æ —Å–ª–æ–≤–∞
                        closest_current_seg = min(overlapping_segments, key=lambda x: x['center_distance'])
                        current_speaker = closest_current_seg['speaker']
                        
                        # –Ø–∫—â–æ –Ω–∞—Å—Ç—É–ø–Ω–µ —Å–ª–æ–≤–æ –Ω–∞–ª–µ–∂–∏—Ç—å —ñ–Ω—à–æ–º—É —Å–ø—ñ–∫–µ—Ä—É, —ñ —î —Å–µ–≥–º–µ–Ω—Ç —Ç–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ –≤ overlapping_segments,
                        # –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –π–æ–≥–æ (—Ü–µ –æ–∑–Ω–∞—á–∞—î, —â–æ –ø–æ—Ç–æ—á–Ω–µ —Å–ª–æ–≤–æ - —á–∞—Å—Ç–∏–Ω–∞ —Ä–µ–ø–ª—ñ–∫–∏ —Ç–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞)
                        if next_speaker != current_speaker:
                            # –®—É–∫–∞—î–º–æ —Å–µ–≥–º–µ–Ω—Ç —Ç–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ –≤ overlapping_segments
                            matching_next_speaker_segs = [
                                s for s in overlapping_segments 
                                if s['speaker'] == next_speaker
                            ]
                            if matching_next_speaker_segs:
                                # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Å–µ–≥–º–µ–Ω—Ç —Ç–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
                                best_seg = min(matching_next_speaker_segs, key=lambda x: x['center_distance'])
                                speaker_id = best_seg['speaker']
                                word_speakers.append({
                                    'word': word_text,
                                    'start': word_start,
                                    'end': word_end,
                                    'speaker': speaker_id,
                                    'triz_corrected': True
                                })
                                continue
            
            # –ü—Ä—ñ–æ—Ä–∏—Ç–µ—Ç 1: –ù–∞–π–±–ª–∏–∂—á—ñ —Å–µ–≥–º–µ–Ω—Ç–∏ (center_distance <0.5s) –∑ overlap_ratio >5%
            # –¶–µ –Ω–∞–π—Ç–æ—á–Ω—ñ—à—ñ —Å–µ–≥–º–µ–Ω—Ç–∏, —è–∫—ñ —Ç–æ—á–Ω–æ –Ω–∞–ª–µ–∂–∞—Ç—å —Å–ª–æ–≤—É
            close_segments = [
                s for s in overlapping_segments 
                if s['center_distance'] < 0.5 and s['overlap_ratio'] > 0.05
            ]
            
            if close_segments:
                # –Ø–∫—â–æ —î –±–ª–∏–∑—å–∫—ñ —Å–µ–≥–º–µ–Ω—Ç–∏, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –Ω–∞–π–±–ª–∏–∂—á–∏–π
                # –í—ñ–¥–¥–∞—î–º–æ –ø–µ—Ä–µ–≤–∞–≥—É –º–µ–Ω—à–∏–º —Å–µ–≥–º–µ–Ω—Ç–∞–º (–Ω–µ –ø—ñ–¥–æ–∑—Ä—ñ–ª–∏–º –≤–µ–ª–∏–∫–∏–º)
                non_suspicious_close = [s for s in close_segments if not s['is_suspicious_large']]
                if non_suspicious_close:
                    best_seg = min(non_suspicious_close, key=lambda x: x['center_distance'])
                else:
                    # –Ø–∫—â–æ –≤—Å—ñ –±–ª–∏–∑—å–∫—ñ –ø—ñ–¥–æ–∑—Ä—ñ–ª—ñ, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –Ω–∞–π–±–ª–∏–∂—á–∏–π
                    best_seg = min(close_segments, key=lambda x: x['center_distance'])
            else:
                # –ü—Ä—ñ–æ—Ä–∏—Ç–µ—Ç 2: –Ø–∫—â–æ –Ω–µ–º–∞—î –±–ª–∏–∑—å–∫–∏—Ö, –≤–∏–±–∏—Ä–∞—î–º–æ –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º –ø—ñ–¥–æ–∑—Ä—ñ–ª–∏—Ö –≤–µ–ª–∏–∫–∏—Ö —Å–µ–≥–º–µ–Ω—Ç—ñ–≤
                # –í—ñ–¥–¥–∞—î–º–æ –ø–µ—Ä–µ–≤–∞–≥—É –º–µ–Ω—à–∏–º —Å–µ–≥–º–µ–Ω—Ç–∞–º, –Ω–∞–≤—ñ—Ç—å —è–∫—â–æ overlap –º–µ–Ω—à–∏–π
                non_suspicious = [s for s in overlapping_segments if not s['is_suspicious_large']]
                
                if non_suspicious:
                    # –Ø–∫—â–æ —î –Ω–µ–ø—ñ–¥–æ–∑—Ä—ñ–ª—ñ —Å–µ–≥–º–µ–Ω—Ç–∏, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –Ω–∞–π–±–ª–∏–∂—á–∏–π –∑ –Ω–∏—Ö
                    # –ê–ª–µ —Ç—ñ–ª—å–∫–∏ —è–∫—â–æ –≤—ñ–Ω –º–∞—î overlap_ratio >5%
                    valid_non_suspicious = [s for s in non_suspicious if s['overlap_ratio'] > 0.05]
                    if valid_non_suspicious:
                        best_seg = min(valid_non_suspicious, key=lambda x: x['center_distance'])
                    else:
                        # –Ø–∫—â–æ –Ω–µ–º–∞—î –≤–∞–ª—ñ–¥–Ω–∏—Ö –Ω–µ–ø—ñ–¥–æ–∑—Ä—ñ–ª–∏—Ö, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –Ω–∞–π–±–ª–∏–∂—á–∏–π –Ω–µ–ø—ñ–¥–æ–∑—Ä—ñ–ª–∏–π
                        best_seg = min(non_suspicious, key=lambda x: x['center_distance'])
                else:
                    # –Ø–∫—â–æ –≤—Å—ñ –ø—ñ–¥–æ–∑—Ä—ñ–ª—ñ, –ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —î —Å–µ–≥–º–µ–Ω—Ç–∏ —ñ–Ω—à–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞, —è–∫—ñ –±–ª–∏–∂—á—ñ
                    # –¶–µ –≤–∞–∂–ª–∏–≤–æ –¥–ª—è –≤–∏–ø–∞–¥–∫—ñ–≤, –∫–æ–ª–∏ –≤–µ–ª–∏–∫–∏–π —Å–µ–≥–º–µ–Ω—Ç "–ø–æ–≥–ª–∏–Ω–∞—î" —Å–ª–æ–≤–∞ –≤—ñ–¥ —ñ–Ω—à–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
                    suspicious_segments = [s for s in overlapping_segments if s['is_suspicious_large']]
                    closest_suspicious = min(suspicious_segments, key=lambda x: x['center_distance'])
                    
                    # –®—É–∫–∞—î–º–æ —Å–µ–≥–º–µ–Ω—Ç–∏ —ñ–Ω—à–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ (–Ω–µ –ø—ñ–¥–æ–∑—Ä—ñ–ª—ñ), —è–∫—ñ –º–æ–∂—É—Ç—å –±—É—Ç–∏ –±–ª–∏–∂—á—ñ
                    other_speaker_segments = [
                        s for s in overlapping_segments 
                        if s['speaker'] != closest_suspicious['speaker'] and not s['is_suspicious_large']
                    ]
                    
                    if other_speaker_segments:
                        # –Ø–∫—â–æ —î —Å–µ–≥–º–µ–Ω—Ç–∏ —ñ–Ω—à–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞, –ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ –≤–æ–Ω–∏ –±–ª–∏–∂—á—ñ
                        closest_other = min(other_speaker_segments, key=lambda x: x['center_distance'])
                        
                        # –Ø–∫—â–æ —Å–µ–≥–º–µ–Ω—Ç —ñ–Ω—à–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ –±–ª–∏–∂—á–∏–π –∞–±–æ –º–∞—î –∫—Ä–∞—â—É –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—é center_distance + overlap,
                        # –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –π–æ–≥–æ (–æ—Å–æ–±–ª–∏–≤–æ —è–∫—â–æ —Ä—ñ–∑–Ω–∏—Ü—è –≤ center_distance –Ω–µ –¥—É–∂–µ –≤–µ–ª–∏–∫–∞)
                        distance_diff = closest_suspicious['center_distance'] - closest_other['center_distance']
                        
                        # –Ø–∫—â–æ —Å–µ–≥–º–µ–Ω—Ç —ñ–Ω—à–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ –±–ª–∏–∂—á–∏–π –Ω–∞ >0.5s –∞–±–æ –º–∞—î overlap_ratio >10%,
                        # –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –π–æ–≥–æ –∑–∞–º—ñ—Å—Ç—å –ø—ñ–¥–æ–∑—Ä—ñ–ª–æ–≥–æ –≤–µ–ª–∏–∫–æ–≥–æ
                        if (distance_diff > 0.5 or 
                            (closest_other['overlap_ratio'] > 0.10 and distance_diff > 0.2)):
                            best_seg = closest_other
                        else:
                            # –Ø–∫—â–æ —Ä—ñ–∑–Ω–∏—Ü—è –Ω–µ–≤–µ–ª–∏–∫–∞, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –Ω–∞–π–±–ª–∏–∂—á–∏–π –ø—ñ–¥–æ–∑—Ä—ñ–ª–∏–π
                            # –ê–ª–µ —Ç—ñ–ª—å–∫–∏ —è–∫—â–æ –≤—ñ–Ω –º–∞—î overlap_ratio >5% –∞–±–æ center_distance <1.0s
                            valid_suspicious = [
                                s for s in suspicious_segments 
                                if s['overlap_ratio'] > 0.05 or s['center_distance'] < 1.0
                            ]
                            if valid_suspicious:
                                best_seg = min(valid_suspicious, key=lambda x: x['center_distance'])
                            else:
                                best_seg = closest_suspicious
                    else:
                        # –Ø–∫—â–æ –Ω–µ–º–∞—î —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ —ñ–Ω—à–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –Ω–∞–π–±–ª–∏–∂—á–∏–π –ø—ñ–¥–æ–∑—Ä—ñ–ª–∏–π
                        # –ê–ª–µ —Ç—ñ–ª—å–∫–∏ —è–∫—â–æ –≤—ñ–Ω –º–∞—î overlap_ratio >5% –∞–±–æ center_distance <1.0s
                        valid_suspicious = [
                            s for s in suspicious_segments 
                            if s['overlap_ratio'] > 0.05 or s['center_distance'] < 1.0
                        ]
                        if valid_suspicious:
                            best_seg = min(valid_suspicious, key=lambda x: x['center_distance'])
                        else:
                            # –û—Å—Ç–∞–Ω–Ω—ñ–π fallback: –Ω–∞–π–±–ª–∏–∂—á–∏–π —Å–µ–≥–º–µ–Ω—Ç
                            best_seg = min(overlapping_segments, key=lambda x: x['center_distance'])
            
            speaker_id = best_seg['speaker']
            word_speakers.append({
                'word': word_text,
                'start': word_start,
                'end': word_end,
                'speaker': speaker_id,
                'triz_corrected': False  # –ü–æ–∑–Ω–∞—á–∫–∞ –¥–ª—è —Å–ª—ñ–≤, –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–∏—Ö –¢–†–Ü–ó –ª–æ–≥—ñ–∫–æ—é
            })
            continue
        
        # –Ø–∫—â–æ –Ω–µ–º–∞—î —Å–µ–≥–º–µ–Ω—Ç—ñ–≤, —â–æ –ø–µ—Ä–µ—Ç–∏–Ω–∞—é—Ç—å—Å—è, —à—É–∫–∞—î–º–æ –Ω–∞–π–±–ª–∏–∂—á–∏–π –∑–∞ —á–∞—Å–æ–º
        best_speaker = None
        best_overlap_ratio = 0
        best_center_distance = float('inf')
        
        for diar_seg in sorted_diar_segments:
            diar_start = diar_seg['start']
            diar_end = diar_seg['end']
            diar_center = (diar_start + diar_end) / 2.0
            
            # –í—ñ–¥—Å—Ç–∞–Ω—å –º—ñ–∂ —Ü–µ–Ω—Ç—Ä–∞–º–∏
            center_distance = abs(word_center - diar_center)
            
            # –í—ñ–¥–¥–∞—î–º–æ –ø–µ—Ä–µ–≤–∞–≥—É –Ω–∞–π–±–ª–∏–∂—á–æ–º—É —Å–µ–≥–º–µ–Ω—Ç—É
            if center_distance < best_center_distance:
                best_center_distance = center_distance
                best_speaker = diar_seg['speaker']
        
        # –Ø–∫—â–æ –Ω–µ–º–∞—î –ø–µ—Ä–µ–∫—Ä–∏—Ç—Ç—è, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –Ω–∞–π–±–ª–∏–∂—á–∏–π —Å–µ–≥–º–µ–Ω—Ç –∑–∞ —á–∞—Å–æ–º
        # –ê–ª–µ —Ç—ñ–ª—å–∫–∏ —è–∫—â–æ –≤—ñ–Ω –¥—É–∂–µ –±–ª–∏–∑—å–∫–∏–π (<1 —Å–µ–∫)
        if best_speaker is not None and best_center_distance < 1.0:
            speaker_id = best_speaker
        elif len(word_speakers) > 0:
            # Fallback: –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Å–ø—ñ–∫–µ—Ä–∞ –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ–≥–æ —Å–ª–æ–≤–∞, —è–∫—â–æ –≤–æ–Ω–æ –¥—É–∂–µ –±–ª–∏–∑—å–∫–µ (<0.3 —Å–µ–∫)
            last_word = word_speakers[-1]
            if (word_start - last_word['end']) < 0.3:
                speaker_id = last_word['speaker']
            else:
                # –Ø–∫—â–æ –ø–æ–ø–µ—Ä–µ–¥–Ω—î —Å–ª–æ–≤–æ –¥–∞–ª–µ–∫–æ, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –Ω–∞–π–±–ª–∏–∂—á–∏–π —Å–µ–≥–º–µ–Ω—Ç, –Ω–∞–≤—ñ—Ç—å —è–∫—â–æ –¥–∞–ª–µ–∫–æ
                speaker_id = best_speaker if best_speaker is not None else 0
        else:
            speaker_id = best_speaker if best_speaker is not None else 0
        
        word_speakers.append({
            'word': word_text,
            'start': word_start,
            'end': word_end,
            'speaker': speaker_id,
            'triz_corrected': False  # –ü–æ–∑–Ω–∞—á–∫–∞ –¥–ª—è –∑–≤–∏—á–∞–π–Ω–∏—Ö —Å–ª—ñ–≤
        })
    
    # –¢–†–Ü–ó: –í–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è –Ω–∞ —Ä—ñ–≤–Ω—ñ —Å–ª—ñ–≤ –¥–ª—è —Ñ—Ä–∞–∑ –ø—ñ—Å–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ä–µ–ø–ª—ñ–∫
    # –ù–ï–ó–ê–õ–ï–ñ–ù–û –≤—ñ–¥ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó —Ç–∞ overlapping_segments
    # –Ø–∫—â–æ –ø–æ–ø–µ—Ä–µ–¥–Ω—î —Å–ª–æ–≤–æ –Ω–∞–ª–µ–∂–∏—Ç—å —ñ–Ω—à–æ–º—É —Å–ø—ñ–∫–µ—Ä—É (–Ω–µ –ø–æ—Ç–æ—á–Ω–æ–º—É), —ñ gap < 3.0s,
    # —ñ –ø–æ–ø–µ—Ä–µ–¥–Ω—î —Å–ª–æ–≤–æ –∫–æ—Ä–æ—Ç–∫–µ, —ñ –ø–æ—Ç–æ—á–Ω–µ —Å–ª–æ–≤–æ —î —á–∞—Å—Ç–∏–Ω–æ—é –ø–∏—Ç–∞–Ω–Ω—è/—ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—ó,
    # —Ç–æ –ø–æ—Ç–æ—á–Ω–µ —Å–ª–æ–≤–æ –º–∞—î –Ω–∞–ª–µ–∂–∞—Ç–∏ —ñ–Ω—à–æ–º—É —Å–ø—ñ–∫–µ—Ä—É (–Ω–µ –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ–º—É)
    print(f"üîß –¢–†–Ü–ó: Applying word-level corrections for phrases after short replies (independent of diarization)...")
    for iteration in range(3):
        changes_made = False
        for i in range(1, len(word_speakers) - 1):
            current_word = word_speakers[i]
            prev_word = word_speakers[i - 1]
            
            current_speaker = current_word['speaker']
            prev_speaker = prev_word['speaker']
            
            gap_to_prev = current_word['start'] - prev_word['end']
            
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ –ø–æ–ø–µ—Ä–µ–¥–Ω—î —Å–ª–æ–≤–æ –Ω–∞–ª–µ–∂–∏—Ç—å –∫–æ—Ä–æ—Ç–∫–æ–º—É —Å–µ–≥–º–µ–Ω—Ç—É –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó
            prev_word_duration = prev_word['end'] - prev_word['start']
            prev_seg_duration = None
            for diar_seg in sorted_diar_segments:
                if (prev_word['start'] >= diar_seg['start'] and 
                    prev_word['end'] <= diar_seg['end']):
                    prev_seg_duration = diar_seg['end'] - diar_seg['start']
                    break
            
            is_prev_short = prev_word_duration < 0.5 or (prev_seg_duration and prev_seg_duration < 1.0)
            
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ –ø–æ—Ç–æ—á–Ω–µ —Å–ª–æ–≤–æ —î —á–∞—Å—Ç–∏–Ω–æ—é –ø–∏—Ç–∞–Ω–Ω—è/—ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—ó
            # –¢–†–Ü–ó: –û—á–∏—â–∞—î–º–æ –ø—É–Ω–∫—Ç—É–∞—Ü—ñ—é –ø–µ—Ä–µ–¥ –ø–µ—Ä–µ–≤—ñ—Ä–∫–æ—é —Ç–∏–ø—É —Ñ—Ä–∞–∑–∏
            current_text_clean = clean_punctuation(current_word['word']).lower()
            is_question_start = any([
                current_text_clean.startswith('hey'),
                current_text_clean.startswith('did'),
                current_text_clean.startswith('can'),
                current_text_clean.startswith('try'),
                current_text_clean.startswith('what'),
                current_text_clean.startswith('how'),
                current_text_clean.startswith('why'),
                current_text_clean.startswith('when'),
                current_text_clean.startswith('where'),
                current_text_clean.startswith('you'),
            ])
            
            # –¢–†–Ü–ó: –Ø–∫—â–æ –ø–æ–ø–µ—Ä–µ–¥–Ω—î —Å–ª–æ–≤–æ –Ω–∞–ª–µ–∂–∏—Ç—å —ñ–Ω—à–æ–º—É —Å–ø—ñ–∫–µ—Ä—É (–Ω–µ –ø–æ—Ç–æ—á–Ω–æ–º—É), —ñ gap < 3.0s,
            # —ñ –ø–æ–ø–µ—Ä–µ–¥–Ω—î —Å–ª–æ–≤–æ –∫–æ—Ä–æ—Ç–∫–µ, —ñ –ø–æ—Ç–æ—á–Ω–µ —Å–ª–æ–≤–æ —î —á–∞—Å—Ç–∏–Ω–æ—é –ø–∏—Ç–∞–Ω–Ω—è/—ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—ó,
            # —Ç–æ –ø–æ—Ç–æ—á–Ω–µ —Å–ª–æ–≤–æ –º–∞—î –Ω–∞–ª–µ–∂–∞—Ç–∏ —ñ–Ω—à–æ–º—É —Å–ø—ñ–∫–µ—Ä—É (–Ω–µ –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ–º—É),
            # –ù–ï–ó–ê–õ–ï–ñ–ù–û –≤—ñ–¥ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó —Ç–∞ overlapping_segments
            if (prev_speaker != current_speaker and
                gap_to_prev < 3.0 and
                is_prev_short and
                is_question_start):
                # –í–∏–∑–Ω–∞—á–∞—î–º–æ —ñ–Ω—à–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ —è–∫ —Ç–æ–≥–æ, —Ö—Ç–æ –Ω–µ —î –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–º —Å–ø—ñ–∫–µ—Ä–æ–º
                all_speakers = set(w['speaker'] for w in word_speakers)
                other_speakers = all_speakers - {prev_speaker}
                
                if other_speakers:
                    # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Å–ø—ñ–∫–µ—Ä–∞ –∑ –Ω–∞–π–±—ñ–ª—å—à–æ—é –∫—ñ–ª—å–∫—ñ—Å—Ç—é —Å–ª—ñ–≤ (—è–∫—â–æ —î –∫—ñ–ª—å–∫–∞)
                    speaker_word_counts = {}
                    for w in word_speakers:
                        speaker = w['speaker']
                        if speaker in other_speakers:
                            speaker_word_counts[speaker] = speaker_word_counts.get(speaker, 0) + 1
                    
                    if speaker_word_counts:
                        target_speaker = max(speaker_word_counts.items(), key=lambda x: x[1])[0]
                    else:
                        # Fallback: –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –ø–µ—Ä—à–æ–≥–æ —ñ–Ω—à–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
                        target_speaker = list(other_speakers)[0]
                else:
                    # –Ø–∫—â–æ –Ω–µ–º–∞—î —ñ–Ω—à–∏—Ö —Å–ø—ñ–∫–µ—Ä—ñ–≤, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ 1 - prev_speaker
                    target_speaker = 1 - prev_speaker if prev_speaker == 0 else 0
                
                # –í–∏–ø—Ä–∞–≤–ª—è—î–º–æ –ø—Ä–∏–∑–Ω–∞—á–µ–Ω–Ω—è
                if current_speaker != target_speaker:
                    word_speakers[i]['speaker'] = target_speaker
                    word_speakers[i]['triz_corrected'] = True
                    changes_made = True
                    print(f"üîß –¢–†–Ü–ó (word-level, independent): –í–∏–ø—Ä–∞–≤–ª–µ–Ω–æ —Å–ª–æ–≤–æ '{current_word['word']}' "
                          f"({current_word['start']:.2f}-{current_word['end']:.2f}s): "
                          f"–°–ø—ñ–∫–µ—Ä {current_speaker} ‚Üí {target_speaker} "
                          f"(–ø—ñ—Å–ª—è –∫–æ—Ä–æ—Ç–∫–æ—ó —Ä–µ–ø–ª—ñ–∫–∏ —Å–ø—ñ–∫–µ—Ä–∞ {prev_speaker}, –ø–∏—Ç–∞–Ω–Ω—è/—ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—è, –Ω–µ–∑–∞–ª–µ–∂–Ω–æ –≤—ñ–¥ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó)")
        
        if not changes_made:
            break
    
    # –î—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞: –ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ä–æ–∑–ø–æ–¥—ñ–ª —Å–ø—ñ–∫–µ—Ä—ñ–≤
    speakers_found = set(w['speaker'] for w in word_speakers)
    print(f"üìä Word-level speakers (before filtering): {len(speakers_found)} unique speakers found: {sorted(speakers_found)}")
    
    # –¢–†–Ü–ó –ü–Ü–î–•–Ü–î: –î—ñ–∞—Ä–∏–∑–∞—Ü—ñ—è - –¥–∂–µ—Ä–µ–ª–æ –ø—Ä–∞–≤–¥–∏ –ø—Ä–æ —Å–ø—ñ–∫–µ—Ä—ñ–≤
    # –ó–∞–º—ñ—Å—Ç—å majority voting, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –ø—Ä—è–º–∏–π –º–∞–ø—ñ–Ω–≥ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó
    # –Ø–∫—â–æ —Å–ª–æ–≤–æ –Ω–µ –º–∞—î –¥–æ—Å—Ç–∞—Ç–Ω—å–æ–≥–æ –ø–µ—Ä–µ–∫—Ä–∏—Ç—Ç—è –∑ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—î—é, —à—É–∫–∞—î–º–æ –Ω–∞–π–±–ª–∏–∂—á–∏–π —Å–µ–≥–º–µ–Ω—Ç –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó
    
    filtered_word_speakers = []
    
    for i, word_info in enumerate(word_speakers):
        word_start = word_info['start']
        word_end = word_info['end']
        word_center = (word_start + word_end) / 2.0
        word_speaker = word_info['speaker']
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –ø–µ—Ä–µ–∫—Ä–∏—Ç—Ç—è –∑ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—î—é –¥–ª—è —Ü—å–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
        best_overlap = 0
        best_overlap_ratio = 0
        for diar_seg in sorted_diar_segments:
            if diar_seg['speaker'] == word_speaker:
                overlap_start = max(word_start, diar_seg['start'])
                overlap_end = min(word_end, diar_seg['end'])
                overlap = max(0, overlap_end - overlap_start)
                word_duration = word_end - word_start
                overlap_ratio = overlap / word_duration if word_duration > 0 else 0
                if overlap_ratio > best_overlap_ratio:
                    best_overlap_ratio = overlap_ratio
                    best_overlap = overlap
        
        word_duration = word_end - word_start
        
        # –ö–†–ò–¢–ò–ß–ù–û: –ù–µ –∑–º—ñ–Ω—é—î–º–æ —Å–ø—ñ–∫–µ—Ä–∞, —è–∫—â–æ –≤—ñ–Ω –±—É–≤ –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–∏–π –¢–†–Ü–ó –ª–æ–≥—ñ–∫–æ—é –¥–ª—è —Å–ª—ñ–≤ –Ω–∞ –ø–æ—á–∞—Ç–∫—É —Ñ–∞–π–ª—É
        # –¶–µ –∑–∞–ø–æ–±—ñ–≥–∞—î –ø–µ—Ä–µ–∑–∞–ø–∏—Å—É –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –ø—Ä–∏–∑–Ω–∞—á–µ–Ω–Ω—è —Å–ø—ñ–∫–µ—Ä–∞
        if word_info.get('triz_corrected', False):
            # –°–ª–æ–≤–æ –≤–∂–µ –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–æ –¢–†–Ü–ó –ª–æ–≥—ñ–∫–æ—é - –Ω–µ –∑–º—ñ–Ω—é—î–º–æ —Å–ø—ñ–∫–µ—Ä–∞
            filtered_word_speakers.append({
                'word': word_info['word'],
                'start': word_start,
                'end': word_end,
                'speaker': word_speaker
            })
            continue
        
        # –¢–†–Ü–ó: –Ø–∫—â–æ –ø–µ—Ä–µ–∫—Ä–∏—Ç—Ç—è –∑ –ø–æ—Ç–æ—á–Ω–∏–º —Å–ø—ñ–∫–µ—Ä–æ–º –∑–∞–Ω–∞–¥—Ç–æ –º–∞–ª–µ, —à—É–∫–∞—î–º–æ –Ω–∞–π–±–ª–∏–∂—á–∏–π —Å–µ–≥–º–µ–Ω—Ç –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó
        # –∑–∞–º—ñ—Å—Ç—å –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è majority voting (—è–∫–∏–π –º–æ–∂–µ –∑–º—ñ—â—É–≤–∞—Ç–∏ –¥–æ –¥–æ–º—ñ–Ω—É—é—á–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞)
        if best_overlap_ratio < 0.3:  # –ó–Ω–∏–∂–µ–Ω–æ –ø–æ—Ä—ñ–≥ –¥–ª—è –±—ñ–ª—å—à –∞–≥—Ä–µ—Å–∏–≤–Ω–æ—ó –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏
            # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –Ω–∞–π–±–ª–∏–∂—á–∏–π —Å–µ–≥–º–µ–Ω—Ç –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó (–∑–∞ —Ü–µ–Ω—Ç—Ä–æ–º —Å–ª–æ–≤–∞)
            closest_seg = None
            min_distance = float('inf')
            
            # –°–ø–æ—á–∞—Ç–∫—É —à—É–∫–∞—î–º–æ —Å–µ–≥–º–µ–Ω—Ç–∏, —è–∫—ñ –ø–µ—Ä–µ—Ç–∏–Ω–∞—é—Ç—å—Å—è –∑ —Å–ª–æ–≤–æ–º (–Ω–∞–≤—ñ—Ç—å —á–∞—Å—Ç–∫–æ–≤–æ)
            overlapping_segs = []
            for diar_seg in sorted_diar_segments:
                if word_start < diar_seg['end'] and word_end > diar_seg['start']:
                    diar_center = (diar_seg['start'] + diar_seg['end']) / 2.0
                    distance = abs(word_center - diar_center)
                    overlapping_segs.append((diar_seg, distance))
            
            # –Ø–∫—â–æ —î —Å–µ–≥–º–µ–Ω—Ç–∏, —â–æ –ø–µ—Ä–µ—Ç–∏–Ω–∞—é—Ç—å—Å—è, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –Ω–∞–π–±–ª–∏–∂—á–∏–π
            if overlapping_segs:
                overlapping_segs.sort(key=lambda x: x[1])
                closest_seg, min_distance = overlapping_segs[0]
            else:
                # –Ø–∫—â–æ –Ω–µ–º–∞—î –ø–µ—Ä–µ—Ç–∏–Ω–∞—é—á–∏—Ö—Å—è, —à—É–∫–∞—î–º–æ –Ω–∞–π–±–ª–∏–∂—á–∏–π –∑–∞ —á–∞—Å–æ–º
                for diar_seg in sorted_diar_segments:
                    diar_center = (diar_seg['start'] + diar_seg['end']) / 2.0
                    distance = abs(word_center - diar_center)
                    
                    if distance < min_distance and distance < 0.5:
                        min_distance = distance
                        closest_seg = diar_seg
            
            # –Ø–∫—â–æ –∑–Ω–∞–π—à–ª–∏ –±–ª–∏–∑—å–∫–∏–π —Å–µ–≥–º–µ–Ω—Ç, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –π–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
            if closest_seg and min_distance < 0.5:
                new_speaker = closest_seg['speaker']
                # –ö–†–ò–¢–ò–ß–ù–û: –ù–µ –∑–º—ñ–Ω—é—î–º–æ —Å–ø—ñ–∫–µ—Ä–∞, —è–∫—â–æ –≤—ñ–Ω –≤–∂–µ –ø—Ä–∞–≤–∏–ª—å–Ω–∏–π –∞–±–æ —è–∫—â–æ —Ü–µ –∑–º—ñ–Ω–∏—Ç—å —Å–ø—ñ–∫–µ—Ä–∞ –Ω–∞ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ
                # –¢—ñ–ª—å–∫–∏ —è–∫—â–æ –Ω–æ–≤–∏–π —Å–ø—ñ–∫–µ—Ä –≤—ñ–¥—Ä—ñ–∑–Ω—è—î—Ç—å—Å—è –≤—ñ–¥ –ø–æ—Ç–æ—á–Ω–æ–≥–æ
                if new_speaker != word_speaker:
                    word_speaker = new_speaker
                    if i < 5:  # –õ–æ–≥—É—î–º–æ —Ç—ñ–ª—å–∫–∏ –ø–µ—Ä—à—ñ 5 –¥–ª—è –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
                        print(f"üîß Word '{word_info['word']}' at {word_center:.2f}s: low overlap ({best_overlap_ratio:.2f}), "
                              f"using closest diarization segment (speaker {word_speaker}, distance: {min_distance:.2f}s)")
        
        filtered_word_speakers.append({
            'word': word_info['word'],
            'start': word_start,
            'end': word_end,
            'speaker': word_speaker
        })
    
    # –¢–†–Ü–ó –†–Ü–®–ï–ù–ù–Ø: –í–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è –ø—Ä–∏–∑–Ω–∞—á–µ–Ω—å –Ω–∞ –æ—Å–Ω–æ–≤—ñ –¥–≤–æ—Å—Ç–æ—Ä–æ–Ω–Ω—å–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É
    # –Ø–∫—â–æ –ø–æ–ø–µ—Ä–µ–¥–Ω—î —ñ –Ω–∞—Å—Ç—É–ø–Ω–µ —Å–ª–æ–≤–∞ –Ω–∞–ª–µ–∂–∞—Ç—å –æ–¥–Ω–æ–º—É —Å–ø—ñ–∫–µ—Ä—É, —ñ gap < 2s,
    # –ø–æ—Ç–æ—á–Ω–µ —Å–ª–æ–≤–æ —Ç–µ–∂ –º–∞—î –Ω–∞–ª–µ–∂–∞—Ç–∏ —Ç–æ–º—É —Å–ø—ñ–∫–µ—Ä—É
    # –¶–µ –≤–∏—Ä—ñ—à—É—î –ø—Ä–æ–±–ª–µ–º—É –∑ —Ñ—Ä–∞–∑–∞–º–∏ –º—ñ–∂ —Ä–µ–ø–ª—ñ–∫–∞–º–∏ –æ–¥–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ (gap 1-2 —Å–µ–∫—É–Ω–¥–∏)
    # –ü—Ä–æ—Ö–æ–¥–∏–º–æ –∫—ñ–ª—å–∫–∞ —Ä–∞–∑—ñ–≤ –¥–ª—è –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è –≤—Å—ñ—Ö –≤–∏–ø–∞–¥–∫—ñ–≤
    for iteration in range(3):  # –î–æ 3 —ñ—Ç–µ—Ä–∞—Ü—ñ–π –¥–ª—è –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è –≤—Å—ñ—Ö –≤–∏–ø–∞–¥–∫—ñ–≤
        changes_made = False
        for i in range(1, len(filtered_word_speakers) - 1):
            current_word = filtered_word_speakers[i]
            prev_word = filtered_word_speakers[i - 1]
            next_word = filtered_word_speakers[i + 1]
            
            current_speaker = current_word['speaker']
            prev_speaker = prev_word['speaker']
            next_speaker = next_word['speaker']
            
            gap_to_prev = current_word['start'] - prev_word['end']
            gap_to_next = next_word['start'] - current_word['end']
            
            # –ü–†–Ü–û–†–ò–¢–ï–¢ 1: –Ø–∫—â–æ –ø–æ–ø–µ—Ä–µ–¥–Ω—î —ñ –Ω–∞—Å—Ç—É–ø–Ω–µ —Å–ª–æ–≤–∞ –Ω–∞–ª–µ–∂–∞—Ç—å –æ–¥–Ω–æ–º—É —Å–ø—ñ–∫–µ—Ä—É (–Ω–µ –ø–æ—Ç–æ—á–Ω–æ–º—É),
            # —ñ gap –¥—É–∂–µ –º–∞–ª—ñ (<2s), –ø–æ—Ç–æ—á–Ω–µ —Å–ª–æ–≤–æ —Ç–µ–∂ –º–∞—î –Ω–∞–ª–µ–∂–∞—Ç–∏ —Ç–æ–º—É —Å–ø—ñ–∫–µ—Ä—É
            # –¶–µ –≤–∏—Ä—ñ—à—É—î –ø—Ä–æ–±–ª–µ–º—É –∑ —Ñ—Ä–∞–∑–∞–º–∏ –º—ñ–∂ —Ä–µ–ø–ª—ñ–∫–∞–º–∏ –æ–¥–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, "No, it should be 200.")
            if (prev_speaker == next_speaker and 
                prev_speaker != current_speaker and
                gap_to_prev < 2.0 and 
                gap_to_next < 2.0):
                # –í–∏–ø—Ä–∞–≤–ª—è—î–º–æ –ø—Ä–∏–∑–Ω–∞—á–µ–Ω–Ω—è
                filtered_word_speakers[i]['speaker'] = prev_speaker
                filtered_word_speakers[i]['triz_corrected'] = True
                changes_made = True
            # –ü–†–Ü–û–†–ò–¢–ï–¢ 2: –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ç—ñ–ª—å–∫–∏ –ø–æ–ø–µ—Ä–µ–¥–Ω—î —Å–ª–æ–≤–æ, —è–∫—â–æ gap < 1s
            # –ê–ª–µ —Ç—ñ–ª—å–∫–∏ —è–∫—â–æ –Ω–∞—Å—Ç—É–ø–Ω–µ —Å–ª–æ–≤–æ –Ω–µ –Ω–∞–ª–µ–∂–∏—Ç—å —ñ–Ω—à–æ–º—É —Å–ø—ñ–∫–µ—Ä—É (–∞–±–æ gap –≤–µ–ª–∏–∫–∏–π)
            # –¶–µ –∑–∞–ø–æ–±—ñ–≥–∞—î –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–º—É –ø—Ä–∏–∑–Ω–∞—á–µ–Ω–Ω—é, –∫–æ–ª–∏ –Ω–∞—Å—Ç—É–ø–Ω–µ —Å–ª–æ–≤–æ –≤—ñ–¥ —ñ–Ω—à–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
            elif (prev_speaker != current_speaker and
                  gap_to_prev < 1.0 and
                  (next_speaker == prev_speaker or gap_to_next > 2.0)):
                # –Ø–∫—â–æ –ø–æ–ø–µ—Ä–µ–¥–Ω—î —Å–ª–æ–≤–æ –Ω–∞–ª–µ–∂–∏—Ç—å —ñ–Ω—à–æ–º—É —Å–ø—ñ–∫–µ—Ä—É —ñ gap –¥—É–∂–µ –º–∞–ª–∏–π,
                # —ñ –Ω–∞—Å—Ç—É–ø–Ω–µ —Å–ª–æ–≤–æ –Ω–µ –Ω–∞–ª–µ–∂–∏—Ç—å —ñ–Ω—à–æ–º—É —Å–ø—ñ–∫–µ—Ä—É (–∞–±–æ gap –≤–µ–ª–∏–∫–∏–π),
                # –ø–æ—Ç–æ—á–Ω–µ —Å–ª–æ–≤–æ —Ç–µ–∂ –º–∞—î –Ω–∞–ª–µ–∂–∞—Ç–∏ –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ–º—É —Å–ø—ñ–∫–µ—Ä—É
                filtered_word_speakers[i]['speaker'] = prev_speaker
                filtered_word_speakers[i]['triz_corrected'] = True
                changes_made = True
            # –ü–†–Ü–û–†–ò–¢–ï–¢ 3: –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ç—ñ–ª—å–∫–∏ –ø–æ–ø–µ—Ä–µ–¥–Ω—î —Å–ª–æ–≤–æ, —è–∫—â–æ gap –¥—É–∂–µ –º–∞–ª–∏–π (<0.3s)
            # –¶–µ –¥–ª—è –≤–∏–ø–∞–¥–∫—ñ–≤, –∫–æ–ª–∏ —Å–ª–æ–≤–∞ –π–¥—É—Ç—å –æ–¥—Ä–∞–∑—É –æ–¥–∏–Ω –∑–∞ –æ–¥–Ω–∏–º
            elif (prev_speaker != current_speaker and
                  gap_to_prev < 0.3):
                # –Ø–∫—â–æ –ø–æ–ø–µ—Ä–µ–¥–Ω—î —Å–ª–æ–≤–æ –Ω–∞–ª–µ–∂–∏—Ç—å —ñ–Ω—à–æ–º—É —Å–ø—ñ–∫–µ—Ä—É —ñ gap –¥—É–∂–µ –º–∞–ª–∏–π,
                # –ø–æ—Ç–æ—á–Ω–µ —Å–ª–æ–≤–æ —Ç–µ–∂ –º–∞—î –Ω–∞–ª–µ–∂–∞—Ç–∏ —Ç–æ–º—É —Å–ø—ñ–∫–µ—Ä—É
                filtered_word_speakers[i]['speaker'] = prev_speaker
                filtered_word_speakers[i]['triz_corrected'] = True
                changes_made = True
        
        # –Ø–∫—â–æ –Ω–µ –±—É–ª–æ –∑–º—ñ–Ω, –≤–∏—Ö–æ–¥–∏–º–æ
        if not changes_made:
            break
    
    # –î—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø—ñ—Å–ª—è —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó
    speakers_found_after = set(w['speaker'] for w in filtered_word_speakers)
    print(f"üìä Word-level speakers (after filtering): {len(speakers_found_after)} unique speakers found: {sorted(speakers_found_after)}")
    
    # –î–æ–¥–∞—Ç–∫–æ–≤–∞ –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞: –ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ä–æ–∑–ø–æ–¥—ñ–ª —Å–ª—ñ–≤ –ø–æ —Å–ø—ñ–∫–µ—Ä–∞—Ö
    speaker_word_counts = {}
    for w in filtered_word_speakers:
        speaker = w['speaker']
        speaker_word_counts[speaker] = speaker_word_counts.get(speaker, 0) + 1
    print(f"üìä Word distribution by speaker: {speaker_word_counts}")
    
    # –ì—Ä—É–ø—É—î–º–æ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω—ñ —Å–ª–æ–≤–∞ –æ–¥–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ –≤ —Å–µ–≥–º–µ–Ω—Ç–∏
    # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ word-level —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –¥–ª—è –ø–æ–¥–∞–ª—å—à–æ–≥–æ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è —Å–µ–≥–º–µ–Ω—Ç—ñ–≤
    combined = []
    if not filtered_word_speakers:
        return combined
    
    current_speaker = filtered_word_speakers[0]['speaker']
    current_start = filtered_word_speakers[0]['start']
    current_words = [filtered_word_speakers[0]['word']]
    current_word_infos = [filtered_word_speakers[0]]  # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –ø–æ–≤–Ω—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ —Å–ª–æ–≤–∞
    
    for i in range(1, len(filtered_word_speakers)):
        word_info = filtered_word_speakers[i]
        
        # –Ø–∫—â–æ —Å–ø—ñ–∫–µ—Ä –∑–º—ñ–Ω–∏–≤—Å—è –∞–±–æ –≤–µ–ª–∏–∫–∏–π –ø—Ä–æ–º—ñ–∂–æ–∫ (>1 —Å–µ–∫), —Å—Ç–≤–æ—Ä—é—î–º–æ –Ω–æ–≤–∏–π —Å–µ–≥–º–µ–Ω—Ç
        if (word_info['speaker'] != current_speaker or 
            word_info['start'] - filtered_word_speakers[i-1]['end'] > 1.0):
            
            # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –ø–æ—Ç–æ—á–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç –∑ word-level —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—î—é
            combined.append({
                'speaker': current_speaker,
                'start': round(current_start, 2),
                'end': round(filtered_word_speakers[i-1]['end'], 2),
                'text': ' '.join(current_words).strip(),
                'words': current_word_infos.copy()  # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ word-level —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é
            })
            
            # –ü–æ—á–∏–Ω–∞—î–º–æ –Ω–æ–≤–∏–π —Å–µ–≥–º–µ–Ω—Ç
            current_speaker = word_info['speaker']
            current_start = word_info['start']
            current_words = [word_info['word']]
            current_word_infos = [word_info]
        else:
            # –î–æ–¥–∞—î–º–æ —Å–ª–æ–≤–æ –¥–æ –ø–æ—Ç–æ—á–Ω–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞
            current_words.append(word_info['word'])
            current_word_infos.append(word_info)
    
    # –î–æ–¥–∞—î–º–æ –æ—Å—Ç–∞–Ω–Ω—ñ–π —Å–µ–≥–º–µ–Ω—Ç
    if current_words:
        combined.append({
            'speaker': current_speaker,
            'start': round(current_start, 2),
            'end': round(filtered_word_speakers[-1]['end'], 2),
            'text': ' '.join(current_words).strip(),
            'words': current_word_infos.copy()  # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ word-level —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é
        })
    
    # –ê–õ–ì–û–†–ò–¢–ú: –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è —Å–µ–≥–º–µ–Ω—Ç—ñ–≤, —è–∫—ñ –º—ñ—Å—Ç—è—Ç—å —Å–ª–æ–≤–∞ –≤—ñ–¥ —Ä—ñ–∑–Ω–∏—Ö —Å–ø—ñ–∫–µ—Ä—ñ–≤
    # –¶–µ –≤–∏—Ä—ñ—à—É—î –ø—Ä–æ–±–ª–µ–º—É, –∫–æ–ª–∏ –≤ –æ–¥–Ω–æ–º—É —Å–µ–≥–º–µ–Ω—Ç—ñ –æ–±'—î–¥–Ω–∞–Ω—ñ —Ñ—Ä–∞–∑–∏ –≤—ñ–¥ —Ä—ñ–∑–Ω–∏—Ö —Å–ø—ñ–∫–µ—Ä—ñ–≤
    # (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, "What speed does it show? Uh, per second.")
    print(f"üîç [Segment Splitting] –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ {len(combined)} —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ –Ω–∞ –Ω–∞—è–≤–Ω—ñ—Å—Ç—å —Å–ª—ñ–≤ –≤—ñ–¥ —Ä—ñ–∑–Ω–∏—Ö —Å–ø—ñ–∫–µ—Ä—ñ–≤...")
    sys.stdout.flush()
    
    split_combined = []
    for seg_idx, seg in enumerate(combined):
        if 'words' not in seg or len(seg['words']) == 0:
            # –Ø–∫—â–æ –Ω–µ–º–∞—î word-level —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó, –¥–æ–¥–∞—î–º–æ —Å–µ–≥–º–µ–Ω—Ç —è–∫ —î
            split_combined.append(seg)
            continue
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —î –≤ —Å–µ–≥–º–µ–Ω—Ç—ñ —Å–ª–æ–≤–∞ –≤—ñ–¥ —Ä—ñ–∑–Ω–∏—Ö —Å–ø—ñ–∫–µ—Ä—ñ–≤
        word_speakers = set(w['speaker'] for w in seg['words'])
        
        if len(word_speakers) == 1:
            # –í—Å—ñ —Å–ª–æ–≤–∞ –≤—ñ–¥ –æ–¥–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ - –¥–æ–¥–∞—î–º–æ —Å–µ–≥–º–µ–Ω—Ç —è–∫ —î
            # –í–∏–¥–∞–ª—è—î–º–æ 'words' –¥–ª—è –µ–∫–æ–Ω–æ–º—ñ—ó –ø–∞–º'—è—Ç—ñ
            seg_clean = {k: v for k, v in seg.items() if k != 'words'}
            split_combined.append(seg_clean)
        else:
            # –Ñ —Å–ª–æ–≤–∞ –≤—ñ–¥ —Ä—ñ–∑–Ω–∏—Ö —Å–ø—ñ–∫–µ—Ä—ñ–≤ - —Ä–æ–∑–¥—ñ–ª—è—î–º–æ —Å–µ–≥–º–µ–Ω—Ç
            print(f"üîß [Segment Splitting] –°–µ–≥–º–µ–Ω—Ç {seg_idx}: '{seg['text'][:50]}...' –º—ñ—Å—Ç–∏—Ç—å —Å–ª–æ–≤–∞ –≤—ñ–¥ {len(word_speakers)} —Å–ø—ñ–∫–µ—Ä—ñ–≤: {sorted(word_speakers)}")
            sys.stdout.flush()
            
            # –ì—Ä—É–ø—É—î–º–æ —Å–ª–æ–≤–∞ –∑–∞ —Å–ø—ñ–∫–µ—Ä–æ–º
            current_split_speaker = seg['words'][0]['speaker']
            current_split_start = seg['words'][0]['start']
            current_split_words = [seg['words'][0]['word']]
            
            for word_idx in range(1, len(seg['words'])):
                word = seg['words'][word_idx]
                
                # –Ø–∫—â–æ —Å–ø—ñ–∫–µ—Ä –∑–º—ñ–Ω–∏–≤—Å—è, —Å—Ç–≤–æ—Ä—é—î–º–æ –Ω–æ–≤–∏–π –ø—ñ–¥—Å–µ–≥–º–µ–Ω—Ç
                if word['speaker'] != current_split_speaker:
                    # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π –ø—ñ–¥—Å–µ–≥–º–µ–Ω—Ç
                    prev_word = seg['words'][word_idx - 1]
                    split_combined.append({
                        'speaker': current_split_speaker,
                        'start': round(current_split_start, 2),
                        'end': round(prev_word['end'], 2),
                        'text': ' '.join(current_split_words).strip()
                    })
                    
                    # –ü–æ—á–∏–Ω–∞—î–º–æ –Ω–æ–≤–∏–π –ø—ñ–¥—Å–µ–≥–º–µ–Ω—Ç
                    current_split_speaker = word['speaker']
                    current_split_start = word['start']
                    current_split_words = [word['word']]
                else:
                    # –î–æ–¥–∞—î–º–æ —Å–ª–æ–≤–æ –¥–æ –ø–æ—Ç–æ—á–Ω–æ–≥–æ –ø—ñ–¥—Å–µ–≥–º–µ–Ω—Ç–∞
                    current_split_words.append(word['word'])
            
            # –î–æ–¥–∞—î–º–æ –æ—Å—Ç–∞–Ω–Ω—ñ–π –ø—ñ–¥—Å–µ–≥–º–µ–Ω—Ç
            if current_split_words:
                last_word = seg['words'][-1]
                split_combined.append({
                    'speaker': current_split_speaker,
                    'start': round(current_split_start, 2),
                    'end': round(last_word['end'], 2),
                    'text': ' '.join(current_split_words).strip()
                })
            
            print(f"   ‚úÖ –†–æ–∑–¥—ñ–ª–µ–Ω–æ –Ω–∞ {len([s for s in split_combined if s['start'] >= seg['start'] and s['end'] <= seg['end']])} –ø—ñ–¥—Å–µ–≥–º–µ–Ω—Ç—ñ–≤")
            sys.stdout.flush()
    
    # –ó–∞–º—ñ–Ω—é—î–º–æ combined –Ω–∞ split_combined
    combined = split_combined
    print(f"‚úÖ [Segment Splitting] –ü—ñ—Å–ª—è —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è: {len(combined)} —Å–µ–≥–º–µ–Ω—Ç—ñ–≤")
    sys.stdout.flush()
    
    # –ê–õ–ì–û–†–ò–¢–ú 2: –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è LLM –¥–ª—è –≤–∏—è–≤–ª–µ–Ω–Ω—è —Ç–∞ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ –∑ –ø–∏—Ç–∞–Ω–Ω—è–º + –≤—ñ–¥–ø–æ–≤—ñ–¥–¥—é
    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –∫–æ–∂–µ–Ω —Å–µ–≥–º–µ–Ω—Ç –Ω–∞ –Ω–∞—è–≤–Ω—ñ—Å—Ç—å –ø–∏—Ç–∞–Ω–Ω—è + –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ
    print(f"ü§ñ [LLM Segment Analysis] –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ –Ω–∞ –ø–∏—Ç–∞–Ω–Ω—è + –≤—ñ–¥–ø–æ–≤—ñ–¥—å...")
    sys.stdout.flush()
    
    llm_split_combined = []
    for seg_idx, seg in enumerate(combined):
        seg_text = seg.get('text', '').strip()
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —Å–µ–≥–º–µ–Ω—Ç –º—ñ—Å—Ç–∏—Ç—å –ø–∏—Ç–∞–Ω–Ω—è (?) —Ç–∞ –º–æ–∂–ª–∏–≤—É –≤—ñ–¥–ø–æ–≤—ñ–¥—å
        has_question = '?' in seg_text
        has_potential_answer = any(
            phrase in seg_text.lower() for phrase in [
                'uh', 'um', 'well', 'yes', 'no', 'yeah', 'sure', 'okay', 'ok',
                'i did', 'i do', 'i have', 'i will', 'i can', 'i am', "i'm",
                'per second', 'per minute', 'per hour'
            ]
        )
        
        # –Ø–∫—â–æ —î –ø–∏—Ç–∞–Ω–Ω—è —ñ –ø–æ—Ç–µ–Ω—Ü—ñ–π–Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å, –≤—ñ–¥–ø—Ä–∞–≤–ª—è—î–º–æ –Ω–∞ LLM
        if has_question and has_potential_answer and len(seg_text.split()) > 5:
            print(f"üîç [LLM Segment Analysis] –°–µ–≥–º–µ–Ω—Ç {seg_idx}: '{seg_text[:60]}...' - –≤–∏—è–≤–ª–µ–Ω–æ –ø–∏—Ç–∞–Ω–Ω—è + –º–æ–∂–ª–∏–≤–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å")
            sys.stdout.flush()
            
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —î —è–≤–Ω—ñ –º–∞—Ä–∫–µ—Ä–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ ("Uh", "per second", —Ç–æ—â–æ)
            explicit_answer_markers = ['uh,', 'um,', 'well,', 'per second', 'per minute', 'per hour']
            has_explicit_answer = any(marker in seg_text.lower() for marker in explicit_answer_markers)
            
            # –Ø–∫—â–æ —î —è–≤–Ω—ñ –º–∞—Ä–∫–µ—Ä–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ, —Å–ø—Ä–æ–±—É—î–º–æ —Ä–æ–∑–¥—ñ–ª–∏—Ç–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ
            if has_explicit_answer:
                # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –ø–æ–∑–∏—Ü—ñ—é –ø–∏—Ç–∞–Ω–Ω—è (?) —Ç–∞ –º–∞—Ä–∫–µ—Ä–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ
                question_pos = seg_text.find('?')
                if question_pos > 0:
                    # –†–æ–∑–¥—ñ–ª—è—î–º–æ –Ω–∞ –ø–∏—Ç–∞–Ω–Ω—è —Ç–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å
                    question_text = seg_text[:question_pos + 1].strip()
                    answer_text = seg_text[question_pos + 1:].strip()
                    
                    # –í–∏–∑–Ω–∞—á–∞—î–º–æ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
                    speaker_word_counts = {}
                    for s in combined:
                        speaker = s['speaker']
                        word_count = len(s.get('text', '').split())
                        speaker_word_counts[speaker] = speaker_word_counts.get(speaker, 0) + word_count
                    main_speaker = max(speaker_word_counts.items(), key=lambda x: x[1])[0] if speaker_word_counts else 0
                    other_speaker = 1 if main_speaker == 0 else 0
                    
                    # –†–æ–∑–¥—ñ–ª—è—î–º–æ —Å–µ–≥–º–µ–Ω—Ç
                    total_duration = seg['end'] - seg['start']
                    question_ratio = len(question_text) / len(seg_text) if len(seg_text) > 0 else 0.6
                    
                    question_duration = total_duration * question_ratio
                    answer_duration = total_duration * (1 - question_ratio)
                    
                    print(f"üîß [Auto Split] –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è —Å–µ–≥–º–µ–Ω—Ç–∞ {seg_idx} (—è–≤–Ω—ñ –º–∞—Ä–∫–µ—Ä–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ)")
                    print(f"   –ü–∏—Ç–∞–Ω–Ω—è: '{question_text}' ‚Üí –°–ø—ñ–∫–µ—Ä {main_speaker}")
                    print(f"   –í—ñ–¥–ø–æ–≤—ñ–¥—å: '{answer_text}' ‚Üí –°–ø—ñ–∫–µ—Ä {other_speaker}")
                    sys.stdout.flush()
                    
                    llm_split_combined.append({
                        'speaker': main_speaker,
                        'start': seg['start'],
                        'end': seg['start'] + question_duration,
                        'text': question_text
                    })
                    llm_split_combined.append({
                        'speaker': other_speaker,
                        'start': seg['start'] + question_duration,
                        'end': seg['end'],
                        'text': answer_text
                    })
                    continue
            
            # –Ø–∫—â–æ –Ω–µ–º–∞—î —è–≤–Ω–∏—Ö –º–∞—Ä–∫–µ—Ä—ñ–≤, –≤—ñ–¥–ø—Ä–∞–≤–ª—è—î–º–æ –Ω–∞ LLM
            split_result = call_llm_for_segment_splitting(seg, all_segments_context=combined, mode=llm_mode)
            
            if split_result and split_result.get('should_split') and split_result.get('parts'):
                # –†–æ–∑–¥—ñ–ª—è—î–º–æ —Å–µ–≥–º–µ–Ω—Ç –Ω–∞ —á–∞—Å—Ç–∏–Ω–∏
                print(f"‚úÖ [LLM Segment Analysis] –°–µ–≥–º–µ–Ω—Ç {seg_idx} —Ä–æ–∑–¥—ñ–ª–µ–Ω–æ –Ω–∞ {len(split_result['parts'])} —á–∞—Å—Ç–∏–Ω")
                for part_idx, part in enumerate(split_result['parts']):
                    print(f"   –ß–∞—Å—Ç–∏–Ω–∞ {part_idx + 1}: '{part['text'][:50]}...' ‚Üí –°–ø—ñ–∫–µ—Ä {part['speaker']}")
                sys.stdout.flush()
                
                llm_split_combined.extend(split_result['parts'])
            else:
                # –ù–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ —Ä–æ–∑–¥—ñ–ª—è—Ç–∏ - –¥–æ–¥–∞—î–º–æ —Å–µ–≥–º–µ–Ω—Ç —è–∫ —î
                llm_split_combined.append(seg)
        else:
            # –ù–µ –ø—ñ–¥—Ö–æ–¥–∏—Ç—å –ø—ñ–¥ –∫—Ä–∏—Ç–µ—Ä—ñ—ó - –¥–æ–¥–∞—î–º–æ —Å–µ–≥–º–µ–Ω—Ç —è–∫ —î
            llm_split_combined.append(seg)
    
    # –ó–∞–º—ñ–Ω—é—î–º–æ combined –Ω–∞ llm_split_combined
    combined = llm_split_combined
    print(f"‚úÖ [LLM Segment Analysis] –ü—ñ—Å–ª—è LLM –∞–Ω–∞–ª—ñ–∑—É: {len(combined)} —Å–µ–≥–º–µ–Ω—Ç—ñ–≤")
    sys.stdout.flush()
    
    # –¢–†–Ü–ó: –î–æ–¥–∞—Ç–∫–æ–≤–∞ –ø—ñ—Å–ª—è–æ–±—Ä–æ–±–∫–∞ –Ω–∞ —Ä—ñ–≤–Ω—ñ —Å–µ–≥–º–µ–Ω—Ç—ñ–≤
    # –Ø–∫—â–æ —Å–µ–≥–º–µ–Ω—Ç –∑–Ω–∞—Ö–æ–¥–∏—Ç—å—Å—è –º—ñ–∂ —Å–µ–≥–º–µ–Ω—Ç–∞–º–∏ –æ–¥–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ –∑ –º–∞–ª–∏–º–∏ gap,
    # –≤—ñ–Ω –º–∞—î –Ω–∞–ª–µ–∂–∞—Ç–∏ —Ç–æ–º—É –∂ —Å–ø—ñ–∫–µ—Ä—É
    # –ü—Ä–æ—Ö–æ–¥–∏–º–æ –∫—ñ–ª—å–∫–∞ —Ä–∞–∑—ñ–≤ –¥–ª—è –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è –≤—Å—ñ—Ö –≤–∏–ø–∞–¥–∫—ñ–≤
    for iteration in range(3):
        changes_made = False
        for i in range(1, len(combined) - 1):
            current_seg = combined[i]
            prev_seg = combined[i - 1]
            next_seg = combined[i + 1]
            
            current_speaker = current_seg['speaker']
            prev_speaker = prev_seg['speaker']
            next_speaker = next_seg['speaker']
            
            gap_to_prev = current_seg['start'] - prev_seg['end']
            gap_to_next = next_seg['start'] - current_seg['end']
            
            # –ü–†–Ü–û–†–ò–¢–ï–¢ 1: –Ø–∫—â–æ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π —ñ –Ω–∞—Å—Ç—É–ø–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç–∏ –Ω–∞–ª–µ–∂–∞—Ç—å –æ–¥–Ω–æ–º—É —Å–ø—ñ–∫–µ—Ä—É (–Ω–µ –ø–æ—Ç–æ—á–Ω–æ–º—É),
            # —ñ gap –¥—É–∂–µ –º–∞–ª—ñ (<2s), –ø–æ—Ç–æ—á–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç —Ç–µ–∂ –º–∞—î –Ω–∞–ª–µ–∂–∞—Ç–∏ —Ç–æ–º—É —Å–ø—ñ–∫–µ—Ä—É
            # –¶–µ –≤–∏—Ä—ñ—à—É—î –ø—Ä–æ–±–ª–µ–º—É –∑ —Ñ—Ä–∞–∑–∞–º–∏ –º—ñ–∂ —Ä–µ–ø–ª—ñ–∫–∞–º–∏ –æ–¥–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, "No, it should be 200.")
            if (prev_speaker == next_speaker and 
                prev_speaker != current_speaker and
                gap_to_prev < 2.0 and 
                gap_to_next < 2.0):
                # –í–∏–ø—Ä–∞–≤–ª—è—î–º–æ –ø—Ä–∏–∑–Ω–∞—á–µ–Ω–Ω—è
                combined[i]['speaker'] = prev_speaker
                changes_made = True
            # –ü–†–Ü–û–†–ò–¢–ï–¢ 2: –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ç—ñ–ª—å–∫–∏ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π —Å–µ–≥–º–µ–Ω—Ç, —è–∫—â–æ gap < 1s
            # –ê–ª–µ —Ç—ñ–ª—å–∫–∏ —è–∫—â–æ –Ω–∞—Å—Ç—É–ø–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç –Ω–µ –Ω–∞–ª–µ–∂–∏—Ç—å —ñ–Ω—à–æ–º—É —Å–ø—ñ–∫–µ—Ä—É (–∞–±–æ gap –≤–µ–ª–∏–∫–∏–π)
            elif (prev_speaker != current_speaker and
                  gap_to_prev < 1.0 and
                  (next_speaker == prev_speaker or gap_to_next > 2.0)):
                # –Ø–∫—â–æ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π —Å–µ–≥–º–µ–Ω—Ç –Ω–∞–ª–µ–∂–∏—Ç—å —ñ–Ω—à–æ–º—É —Å–ø—ñ–∫–µ—Ä—É —ñ gap –¥—É–∂–µ –º–∞–ª–∏–π,
                # —ñ –Ω–∞—Å—Ç—É–ø–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç –Ω–µ –Ω–∞–ª–µ–∂–∏—Ç—å —ñ–Ω—à–æ–º—É —Å–ø—ñ–∫–µ—Ä—É (–∞–±–æ gap –≤–µ–ª–∏–∫–∏–π),
                # –ø–æ—Ç–æ—á–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç —Ç–µ–∂ –º–∞—î –Ω–∞–ª–µ–∂–∞—Ç–∏ –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ–º—É —Å–ø—ñ–∫–µ—Ä—É
                combined[i]['speaker'] = prev_speaker
                changes_made = True
        
        if not changes_made:
            break
    
    # –¢–†–Ü–ó: –í–∏—è–≤–ª–µ–Ω–Ω—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ä–µ–ø–ª—ñ–∫ —Ç–∞ –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è –ø—Ä–∏–∑–Ω–∞—á–µ–Ω—å —Å–ø—ñ–∫–µ—Ä—ñ–≤
    # –Ø–∫—â–æ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π —Å–µ–≥–º–µ–Ω—Ç –∫–æ—Ä–æ—Ç–∫–∏–π (<1s) —ñ –Ω–∞–ª–µ–∂–∏—Ç—å —ñ–Ω—à–æ–º—É —Å–ø—ñ–∫–µ—Ä—É,
    # —ñ –ø–æ—Ç–æ—á–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç —î –ø–∏—Ç–∞–Ω–Ω—è–º/—ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—î—é, –≤—ñ–Ω –º–∞—î –Ω–∞–ª–µ–∂–∞—Ç–∏ –æ—Å–Ω–æ–≤–Ω–æ–º—É —Å–ø—ñ–∫–µ—Ä—É
    # –¶–µ –≤–∏—Ä—ñ—à—É—î –ø—Ä–æ–±–ª–µ–º—É –∑ —Ñ—Ä–∞–∑–∞–º–∏ –ø—ñ—Å–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ä–µ–ø–ª—ñ–∫ —ñ–Ω—à–∏—Ö —Å–ø—ñ–∫–µ—Ä—ñ–≤
    # (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, "Hey, did you try to reset your modem?" –ø—ñ—Å–ª—è "dropping.")
    # –¢–†–Ü–ó: –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ main_speaker_from_diarization (–≤–∏–∑–Ω–∞—á–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤—ñ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó),
    # –∞ –Ω–µ main_speaker (–≤–∏–∑–Ω–∞—á–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤—ñ –æ–±'—î–¥–Ω–∞–Ω–æ—ó —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó)
    
    if len(combined) > 1:
        print(f"üîç [Segment Processing] –ü–æ—á–∞—Ç–æ–∫ –æ–±—Ä–æ–±–∫–∏ {len(combined)} —Å–µ–≥–º–µ–Ω—Ç—ñ–≤, llm_mode={llm_mode}")
        sys.stdout.flush()
        
        # –Ü—Ç–µ—Ä–∞—Ç–∏–≤–Ω–∞ –ø—ñ—Å–ª—è–æ–±—Ä–æ–±–∫–∞ –¥–ª—è –≤–∏—è–≤–ª–µ–Ω–Ω—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ä–µ–ø–ª—ñ–∫
        for iteration in range(3):
            changes_made = False
            print(f"üîÑ [Iteration {iteration + 1}] –û–±—Ä–æ–±–∫–∞ —Å–µ–≥–º–µ–Ω—Ç—ñ–≤...")
            sys.stdout.flush()
            
            for i in range(1, len(combined) - 1):
                current_seg = combined[i]
                prev_seg = combined[i - 1]
                next_seg = combined[i + 1] if i + 1 < len(combined) else None
                
                current_speaker = current_seg['speaker']
                prev_speaker = prev_seg['speaker']
                next_speaker = next_seg['speaker'] if next_seg else None
                
                gap_to_prev = current_seg['start'] - prev_seg['end']
                gap_to_next = next_seg['start'] - current_seg['end'] if next_seg else float('inf')
                
                prev_duration = prev_seg['end'] - prev_seg['start']
                current_duration = current_seg['end'] - current_seg['start']
                current_text_raw = current_seg.get('text', '').strip()
                current_text_lower = current_text_raw.lower().strip()
                
                # –í–∏–∑–Ω–∞—á–∞—î–º–æ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ (—Ç–æ–π, —Ö—Ç–æ –º–∞—î –±—ñ–ª—å—à–µ —Å–ª—ñ–≤)
                all_speakers = set(seg['speaker'] for seg in combined)
                speaker_word_counts = {}
                for seg in combined:
                    speaker = seg['speaker']
                    word_count = len(seg.get('text', '').split())
                    speaker_word_counts[speaker] = speaker_word_counts.get(speaker, 0) + word_count
                main_speaker = max(speaker_word_counts.items(), key=lambda x: x[1])[0] if speaker_word_counts else 0
                
                # –ê–õ–ì–û–†–ò–¢–ú 1: –í–∏—è–≤–ª–µ–Ω–Ω—è –∫–æ—Ä–æ—Ç–∫–∏—Ö –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π –Ω–µ–≥–æ–ª–æ–≤–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ –º—ñ–∂ —Ä–µ–ø–ª—ñ–∫–∞–º–∏ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ
                # –°—Ü–µ–Ω–∞—Ä—ñ–π: [–û—Å–Ω–æ–≤–Ω–∏–π —Å–ø—ñ–∫–µ—Ä] -> [–ö–æ—Ä–æ—Ç–∫–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å] -> [–û—Å–Ω–æ–≤–Ω–∏–π —Å–ø—ñ–∫–µ—Ä]
                if (prev_seg and next_seg and 
                    prev_speaker == main_speaker and 
                    next_speaker == main_speaker):
                    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ –ø–æ—Ç–æ—á–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç - –∫–æ—Ä–æ—Ç–∫–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å
                    is_short_duration = current_duration < 2.0
                    word_count = len(current_text_raw.split())
                    is_short_phrase = word_count <= 3
                    
                    # –°–ø–∏—Å–æ–∫ —Ç–∏–ø–æ–≤–∏—Ö –∫–æ—Ä–æ—Ç–∫–∏—Ö –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π
                    short_replies = [
                        'i did', 'i do', 'i have', 'i will', 'i can', 'i am', "i'm",
                        'yes', 'yeah', 'yep', 'yup', 'sure', 'okay', 'ok', 'alright', 'right',
                        'no', 'nope', 'nah', 'not', "don't", "didn't", "won't", "can't",
                        'thanks', 'thank you', 'please', 'sorry', 'excuse me',
                        'uh huh', 'mm hmm', 'hmm', 'ah', 'oh', 'well', 'um', 'uh'
                    ]
                    
                    is_short_reply = any(
                        current_text_lower.startswith(reply) or 
                        current_text_lower == reply or
                        current_text_lower.startswith(reply + ' ')
                        for reply in short_replies
                    )
                    
                    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ gap –º—ñ–∂ —Å–µ–≥–º–µ–Ω—Ç–∞–º–∏ (–Ω–µ–≤–µ–ª–∏–∫–∏–π gap = –ø—Ä–∏—Ä–æ–¥–Ω–∞ –ø–∞—É–∑–∞ –≤ –¥—ñ–∞–ª–æ–∑—ñ)
                    gap_ok = gap_to_prev < 3.0 and gap_to_next < 3.0
                    
                    # –î—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ –º—ñ–∂ —Ä–µ–ø–ª—ñ–∫–∞–º–∏ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
                    if is_short_duration and is_short_phrase:
                        print(f"üîç [Short Reply Check] Segment {i}: '{current_text_raw}' "
                              f"(duration={current_duration:.2f}s, words={word_count}, "
                              f"speaker={current_speaker}, main={main_speaker}, "
                              f"prev={prev_speaker}, next={next_speaker}, "
                              f"is_short_reply={is_short_reply}, gap_ok={gap_ok})")
                        sys.stdout.flush()
                    
                    if is_short_duration and is_short_phrase and (is_short_reply or word_count <= 2) and gap_ok:
                        # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –Ω–µ–≥–æ–ª–æ–≤–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
                        other_speakers = all_speakers - {main_speaker}
                        if other_speakers:
                            other_speaker = list(other_speakers)[0]
                            
                            # –í–∏–ø—Ä–∞–≤–ª—è—î–º–æ –ø—Ä–∏–∑–Ω–∞—á–µ–Ω–Ω—è —Å–ø—ñ–∫–µ—Ä–∞
                            combined[i]['speaker'] = other_speaker
                            changes_made = True
                            print(f"üîß [Algorithm] ‚úÖ –í–∏—è–≤–ª–µ–Ω–æ –∫–æ—Ä–æ—Ç–∫—É –≤—ñ–¥–ø–æ–≤—ñ–¥—å –º—ñ–∂ —Ä–µ–ø–ª—ñ–∫–∞–º–∏ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞: "
                                  f"'{current_text_raw}' ({current_seg['start']:.2f}-{current_seg['end']:.2f}s, "
                                  f"{current_duration:.2f}s, {word_count} words) "
                                  f"‚Üí –°–ø—ñ–∫–µ—Ä {current_speaker} ‚Üí {other_speaker} "
                                  f"(–º—ñ–∂ —Ä–µ–ø–ª—ñ–∫–∞–º–∏ —Å–ø—ñ–∫–µ—Ä–∞ {main_speaker})")
                            sys.stdout.flush()
                            continue  # –ü–µ—Ä–µ—Ö–æ–¥–∏–º–æ –¥–æ –Ω–∞—Å—Ç—É–ø–Ω–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞
                
                # –ê–õ–ì–û–†–ò–¢–ú 2: –í–∏—è–≤–ª–µ–Ω–Ω—è –∑–∞–ø–µ—Ä–µ—á–µ–Ω—å —Ç–∏–ø—É "No, it should be..." –ø—ñ—Å–ª—è –ø–∏—Ç–∞–Ω–Ω—è/—Ä–µ–ø–ª—ñ–∫–∏
                # –Ø–∫—â–æ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π —Å–µ–≥–º–µ–Ω—Ç –º—ñ—Å—Ç–∏—Ç—å –ø–∏—Ç–∞–Ω–Ω—è –∞–±–æ —Ä–µ–ø–ª—ñ–∫—É, –∞ –ø–æ—Ç–æ—á–Ω–∏–π –ø–æ—á–∏–Ω–∞—î—Ç—å—Å—è –∑ "No" (–∞–±–æ –ø–æ–¥—ñ–±–Ω–∏—Ö –∑–∞–ø–µ—Ä–µ—á–µ–Ω—å),
                # —Ü–µ –∑ –≤–µ–ª–∏–∫–æ—é –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—é –Ω–æ–≤–∏–π —Å–ø—ñ–∫–µ—Ä, –∞–ª–µ –ø–æ—Ç—Ä—ñ–±–µ–Ω LLM –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É –∫–æ–Ω—Ç–µ–∫—Å—Ç—É
                prev_text_lower = prev_seg.get('text', '').strip().lower()
                has_question_in_prev = '?' in prev_seg.get('text', '')
                has_statement_in_prev = any(
                    phrase in prev_text_lower for phrase in [
                        'it shows', 'it should', 'it is', 'it was', 'it can', 'it will',
                        'speed', 'shows', 'should be', 'per second', 'per minute', 'per hour',
                        'uh,', 'um,', 'well,'
                    ]
                )
                
                # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ç–∞–∫–æ–∂ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π-–ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π —Å–µ–≥–º–µ–Ω—Ç (i-2), —è–∫—â–æ gap –º—ñ–∂ –Ω–∏–º–∏ –Ω–µ–≤–µ–ª–∏–∫–∏–π
                # –¶–µ –≤–∏—Ä—ñ—à—É—î –≤–∏–ø–∞–¥–æ–∫: "What speed does it show?" (i-2) -> "Uh, per second." (i-1) -> "No, it should be 200." (i)
                has_question_in_prev_prev = False
                if i >= 2:
                    prev_prev_seg = combined[i - 2]
                    prev_prev_text = prev_prev_seg.get('text', '').strip()
                    has_question_in_prev_prev = '?' in prev_prev_text
                    gap_to_prev_prev = prev_seg['start'] - prev_prev_seg['end']
                    # –Ø–∫—â–æ –º—ñ–∂ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–º-–ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–º —ñ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–º gap –Ω–µ–≤–µ–ª–∏–∫–∏–π (< 2 —Å–µ–∫—É–Ω–¥–∏), –≤—Ä–∞—Ö–æ–≤—É—î–º–æ –ø–∏—Ç–∞–Ω–Ω—è
                    if has_question_in_prev_prev and gap_to_prev_prev < 2.0:
                        has_question_in_prev = True  # –í—Ä–∞—Ö–æ–≤—É—î–º–æ –ø–∏—Ç–∞–Ω–Ω—è –∑ –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ–≥–æ-–ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞
                
                # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ –ø–æ—Ç–æ—á–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç –ø–æ—á–∏–Ω–∞—î—Ç—å—Å—è –∑ –∑–∞–ø–µ—Ä–µ—á–µ–Ω–Ω—è
                negation_starters = ['no,', 'no ', 'nope,', 'nope ', 'nah,', 'nah ', 'not,', 'not ']
                starts_with_negation = any(
                    current_text_lower.startswith(neg) for neg in negation_starters
                )
                
                # –Ø–∫—â–æ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π —Å–µ–≥–º–µ–Ω—Ç –º—ñ—Å—Ç–∏—Ç—å –ø–∏—Ç–∞–Ω–Ω—è/—Ä–µ–ø–ª—ñ–∫—É, –∞ –ø–æ—Ç–æ—á–Ω–∏–π –ø–æ—á–∏–Ω–∞—î—Ç—å—Å—è –∑ –∑–∞–ø–µ—Ä–µ—á–µ–Ω–Ω—è,
                # —ñ gap –Ω–µ–≤–µ–ª–∏–∫–∏–π (< 3 —Å–µ–∫—É–Ω–¥–∏), –Ω–∞–ø—Ä–∞–≤–ª—è—î–º–æ –Ω–∞ LLM
                if (has_question_in_prev or has_statement_in_prev) and starts_with_negation and gap_to_prev < 3.0:
                    print(f"üîç [Negation Detection] –°–µ–≥–º–µ–Ω—Ç {i}: –í–∏—è–≤–ª–µ–Ω–æ –∑–∞–ø–µ—Ä–µ—á–µ–Ω–Ω—è –ø—ñ—Å–ª—è –ø–∏—Ç–∞–Ω–Ω—è/—Ä–µ–ø–ª—ñ–∫–∏")
                    print(f"   –ü–æ–ø–µ—Ä–µ–¥–Ω—ñ–π: '{prev_seg.get('text', '')[:50]}...' (speaker={prev_speaker})")
                    if i >= 2:
                        prev_prev_seg = combined[i - 2]
                        print(f"   –ü–æ–ø–µ—Ä–µ–¥–Ω—ñ–π-–ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π: '{prev_prev_seg.get('text', '')[:50]}...' (speaker={prev_prev_seg['speaker']})")
                    print(f"   –ü–æ—Ç–æ—á–Ω–∏–π: '{current_text_raw[:50]}...' (speaker={current_speaker})")
                    print(f"   Gap: {gap_to_prev:.2f}s")
                    print(f"   has_question_in_prev: {has_question_in_prev}, has_statement_in_prev: {has_statement_in_prev}, starts_with_negation: {starts_with_negation}")
                    sys.stdout.flush()
                    
                    # –ù–∞–ø—Ä–∞–≤–ª—è—î–º–æ –Ω–∞ LLM –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É –∫–æ–Ω—Ç–µ–∫—Å—Ç—É
                    llm_speaker = call_llm_for_speaker_correction(
                        prev_seg,
                        current_seg,
                        gap_to_prev,
                        all_segments_context=combined,
                        mode=llm_mode
                    )
                    
                    if llm_speaker is not None:
                        # LLM —É—Å–ø—ñ—à–Ω–æ –≤–∏–∑–Ω–∞—á–∏–ª–∞ —Å–ø—ñ–∫–µ—Ä–∞
                        if current_speaker != llm_speaker:
                            combined[i]['speaker'] = llm_speaker
                            changes_made = True
                            print(f"ü§ñ [LLM Negation] ‚úÖ –í–∏–ø—Ä–∞–≤–ª–µ–Ω–æ —Å–µ–≥–º–µ–Ω—Ç '{current_text_raw[:50]}...' "
                                  f"({current_seg['start']:.2f}-{current_seg['end']:.2f}s): "
                                  f"–°–ø—ñ–∫–µ—Ä {current_speaker} ‚Üí {llm_speaker} "
                                  f"(–∑–∞–ø–µ—Ä–µ—á–µ–Ω–Ω—è –ø—ñ—Å–ª—è –ø–∏—Ç–∞–Ω–Ω—è/—Ä–µ–ø–ª—ñ–∫–∏, LLM —Ä—ñ—à–µ–Ω–Ω—è)")
                            sys.stdout.flush()
                        else:
                            print(f"‚ÑπÔ∏è [LLM Negation] LLM –ø—ñ–¥—Ç–≤–µ—Ä–¥–∏–≤ –ø–æ—Ç–æ—á–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ {current_speaker} –¥–ª—è '{current_text_raw[:50]}...'")
                            sys.stdout.flush()
                    else:
                        # LLM –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ –∞–±–æ –ø–æ–≤–µ—Ä–Ω—É–ª–∞ –Ω–µ–∫–æ—Ä–µ–∫—Ç–Ω—É –≤—ñ–¥–ø–æ–≤—ñ–¥—å - –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –∞–ª–≥–æ—Ä–∏—Ç–º—ñ—á–Ω–∏–π fallback
                        print(f"‚ö†Ô∏è [LLM Negation] LLM –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ –∞–±–æ –ø–æ–≤–µ—Ä–Ω—É–ª–∞ –Ω–µ–∫–æ—Ä–µ–∫—Ç–Ω—É –≤—ñ–¥–ø–æ–≤—ñ–¥—å –¥–ª—è '{current_text_raw[:50]}...'")
                        print(f"   –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –∞–ª–≥–æ—Ä–∏—Ç–º—ñ—á–Ω–∏–π fallback –Ω–∞ –æ—Å–Ω–æ–≤—ñ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É...")
                        sys.stdout.flush()
                        
                        # –ê–õ–ì–û–†–ò–¢–ú–Ü–ß–ù–ò–ô FALLBACK –¥–ª—è –∑–∞–ø–µ—Ä–µ—á–µ–Ω—å
                        # –ü—Ä–∞–≤–∏–ª–æ: –Ø–∫—â–æ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π-–ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π —Å–µ–≥–º–µ–Ω—Ç –º—ñ—Å—Ç–∏—Ç—å –ø–∏—Ç–∞–Ω–Ω—è –≤—ñ–¥ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞,
                        # –∞ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π —Å–µ–≥–º–µ–Ω—Ç –º—ñ—Å—Ç–∏—Ç—å –≤—ñ–¥–ø–æ–≤—ñ–¥—å –≤—ñ–¥ –Ω–µ–æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞,
                        # —Ç–æ –∑–∞–ø–µ—Ä–µ—á–µ–Ω–Ω—è "No, it should be..." –∑–∞–∑–≤–∏—á–∞–π –Ω–∞–ª–µ–∂–∏—Ç—å –æ—Å–Ω–æ–≤–Ω–æ–º—É —Å–ø—ñ–∫–µ—Ä—É (–≤–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è)
                        
                        # –í–∏–∑–Ω–∞—á–∞—î–º–æ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
                        all_speakers = set(seg['speaker'] for seg in combined)
                        speaker_word_counts = {}
                        for seg in combined:
                            speaker = seg['speaker']
                            word_count = len(seg.get('text', '').split())
                            speaker_word_counts[speaker] = speaker_word_counts.get(speaker, 0) + word_count
                        main_speaker = max(speaker_word_counts.items(), key=lambda x: x[1])[0] if speaker_word_counts else 0
                        other_speaker = 1 if main_speaker == 0 else 0
                        
                        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç: –ø–∏—Ç–∞–Ω–Ω—è (–æ—Å–Ω–æ–≤–Ω–∏–π) ‚Üí –≤—ñ–¥–ø–æ–≤—ñ–¥—å (–Ω–µ–æ—Å–Ω–æ–≤–Ω–∏–π) ‚Üí –∑–∞–ø–µ—Ä–µ—á–µ–Ω–Ω—è
                        algorithmic_speaker = None
                        if i >= 2:
                            prev_prev_seg = combined[i - 2]
                            prev_prev_speaker = prev_prev_seg['speaker']
                            prev_prev_text = prev_prev_seg.get('text', '').strip()
                            has_question_in_prev_prev = '?' in prev_prev_text
                            
                            # –Ø–∫—â–æ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π-–ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π —Å–µ–≥–º–µ–Ω—Ç - –ø–∏—Ç–∞–Ω–Ω—è –≤—ñ–¥ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞,
                            # –∞ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π —Å–µ–≥–º–µ–Ω—Ç - –≤—ñ–¥–ø–æ–≤—ñ–¥—å –≤—ñ–¥ –Ω–µ–æ—Å–Ω–æ–≤–Ω–æ–≥–æ,
                            # —Ç–æ –∑–∞–ø–µ—Ä–µ—á–µ–Ω–Ω—è –Ω–∞–ª–µ–∂–∏—Ç—å –æ—Å–Ω–æ–≤–Ω–æ–º—É —Å–ø—ñ–∫–µ—Ä—É (–≤—ñ–Ω –≤–∏–ø—Ä–∞–≤–ª—è—î –≤—ñ–¥–ø–æ–≤—ñ–¥—å)
                            if (has_question_in_prev_prev and 
                                prev_prev_speaker == main_speaker and 
                                prev_speaker == other_speaker):
                                algorithmic_speaker = main_speaker
                                print(f"   üìä [Fallback] –ö–æ–Ω—Ç–µ–∫—Å—Ç: –ø–∏—Ç–∞–Ω–Ω—è (—Å–ø—ñ–∫–µ—Ä {main_speaker}) ‚Üí "
                                      f"–≤—ñ–¥–ø–æ–≤—ñ–¥—å (—Å–ø—ñ–∫–µ—Ä {other_speaker}) ‚Üí –∑–∞–ø–µ—Ä–µ—á–µ–Ω–Ω—è ‚Üí —Å–ø—ñ–∫–µ—Ä {main_speaker} (–≤–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è)")
                            # –Ø–∫—â–æ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π-–ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π —Å–µ–≥–º–µ–Ω—Ç - –ø–∏—Ç–∞–Ω–Ω—è –≤—ñ–¥ –Ω–µ–æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞,
                            # –∞ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π —Å–µ–≥–º–µ–Ω—Ç - –≤—ñ–¥–ø–æ–≤—ñ–¥—å –≤—ñ–¥ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ,
                            # —Ç–æ –∑–∞–ø–µ—Ä–µ—á–µ–Ω–Ω—è –º–æ–∂–µ –Ω–∞–ª–µ–∂–∞—Ç–∏ –Ω–µ–æ—Å–Ω–æ–≤–Ω–æ–º—É —Å–ø—ñ–∫–µ—Ä—É (–≤—ñ–Ω –≤–∏–ø—Ä–∞–≤–ª—è—î)
                            elif (has_question_in_prev_prev and 
                                  prev_prev_speaker == other_speaker and 
                                  prev_speaker == main_speaker):
                                algorithmic_speaker = other_speaker
                                print(f"   üìä [Fallback] –ö–æ–Ω—Ç–µ–∫—Å—Ç: –ø–∏—Ç–∞–Ω–Ω—è (—Å–ø—ñ–∫–µ—Ä {other_speaker}) ‚Üí "
                                      f"–≤—ñ–¥–ø–æ–≤—ñ–¥—å (—Å–ø—ñ–∫–µ—Ä {main_speaker}) ‚Üí –∑–∞–ø–µ—Ä–µ—á–µ–Ω–Ω—è ‚Üí —Å–ø—ñ–∫–µ—Ä {other_speaker} (–≤–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è)")
                            # –Ø–∫—â–æ –æ–±–∏–¥–≤–∞ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ —Å–µ–≥–º–µ–Ω—Ç–∏ –≤—ñ–¥ –æ–¥–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞, –∑–∞–ø–µ—Ä–µ—á–µ–Ω–Ω—è –Ω–∞–ª–µ–∂–∏—Ç—å —ñ–Ω—à–æ–º—É
                            elif prev_prev_speaker == prev_speaker:
                                algorithmic_speaker = other_speaker if prev_speaker == main_speaker else main_speaker
                                print(f"   üìä [Fallback] –ö–æ–Ω—Ç–µ–∫—Å—Ç: –æ–±–∏–¥–≤–∞ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ —Å–µ–≥–º–µ–Ω—Ç–∏ –≤—ñ–¥ —Å–ø—ñ–∫–µ—Ä–∞ {prev_speaker} ‚Üí "
                                      f"–∑–∞–ø–µ—Ä–µ—á–µ–Ω–Ω—è ‚Üí —Å–ø—ñ–∫–µ—Ä {algorithmic_speaker} (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∏–π —Å–ø—ñ–∫–µ—Ä)")
                        
                        # –Ø–∫—â–æ –∞–ª–≥–æ—Ä–∏—Ç–º—ñ—á–Ω–µ —Ä—ñ—à–µ–Ω–Ω—è –∑–Ω–∞–π–¥–µ–Ω–æ —ñ –≤–æ–Ω–æ –≤—ñ–¥—Ä—ñ–∑–Ω—è—î—Ç—å—Å—è –≤—ñ–¥ –ø–æ—Ç–æ—á–Ω–æ–≥–æ
                        if algorithmic_speaker is not None and current_speaker != algorithmic_speaker:
                            combined[i]['speaker'] = algorithmic_speaker
                            changes_made = True
                            print(f"üîß [Algorithmic Fallback] ‚úÖ –í–∏–ø—Ä–∞–≤–ª–µ–Ω–æ —Å–µ–≥–º–µ–Ω—Ç '{current_text_raw[:50]}...' "
                                  f"({current_seg['start']:.2f}-{current_seg['end']:.2f}s): "
                                  f"–°–ø—ñ–∫–µ—Ä {current_speaker} ‚Üí {algorithmic_speaker} "
                                  f"(–∑–∞–ø–µ—Ä–µ—á–µ–Ω–Ω—è –ø—ñ—Å–ª—è –ø–∏—Ç–∞–Ω–Ω—è/—Ä–µ–ø–ª—ñ–∫–∏, –∞–ª–≥–æ—Ä–∏—Ç–º—ñ—á–Ω–µ —Ä—ñ—à–µ–Ω–Ω—è)")
                            sys.stdout.flush()
                        elif algorithmic_speaker is None:
                            print(f"‚ö†Ô∏è [Algorithmic Fallback] –ù–µ –≤–¥–∞–ª–æ—Å—è –≤–∏–∑–Ω–∞—á–∏—Ç–∏ —Å–ø—ñ–∫–µ—Ä–∞ –∞–ª–≥–æ—Ä–∏—Ç–º—ñ—á–Ω–æ –¥–ª—è '{current_text_raw[:50]}...'")
                            sys.stdout.flush()
                        else:
                            print(f"‚ÑπÔ∏è [Algorithmic Fallback] –ê–ª–≥–æ—Ä–∏—Ç–º—ñ—á–Ω–µ —Ä—ñ—à–µ–Ω–Ω—è –ø—ñ–¥—Ç–≤–µ—Ä–¥–∏–ª–æ –ø–æ—Ç–æ—á–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ {current_speaker}")
                            sys.stdout.flush()
                    
                    continue  # –ü–µ—Ä–µ—Ö–æ–¥–∏–º–æ –¥–æ –Ω–∞—Å—Ç—É–ø–Ω–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞
                
                # –î—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞
                if i <= 3 or prev_duration < 1.0:  # –õ–æ–≥—É—î–º–æ –ø–µ—Ä—à—ñ 3 –∞–±–æ –∫–æ—Ä–æ—Ç–∫—ñ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ —Å–µ–≥–º–µ–Ω—Ç–∏
                    print(f"  üìä [Segment {i}] prev: speaker={prev_speaker}, duration={prev_duration:.2f}s, text='{prev_seg.get('text', '')[:30]}...'")
                    print(f"     current: speaker={current_speaker}, gap={gap_to_prev:.2f}s, text='{current_text_raw[:30]}...'")
                    sys.stdout.flush()
                
                # –¢–†–Ü–ó: –û—á–∏—â–∞—î–º–æ –ø—É–Ω–∫—Ç—É–∞—Ü—ñ—é –ø–µ—Ä–µ–¥ –ø–µ—Ä–µ–≤—ñ—Ä–∫–æ—é —Ç–∏–ø—É —Ñ—Ä–∞–∑–∏
                # –ë–µ—Ä–µ–º–æ –ø–µ—Ä—à–µ —Å–ª–æ–≤–æ –∑ –æ—á–∏—â–µ–Ω–æ—é –ø—É–Ω–∫—Ç—É–∞—Ü—ñ—î—é –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏
                first_word = current_text_raw.split()[0] if current_text_raw.split() else ''
                current_text_clean = clean_punctuation(first_word).lower() if first_word else ''
                current_text_lower = current_text_raw.lower()
                
                # –í–∏—è–≤–ª—è—î–º–æ, —á–∏ –ø–æ—Ç–æ—á–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç —î –ø–∏—Ç–∞–Ω–Ω—è–º/—ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—î—é
                is_question_or_instruction = any([
                    current_text_clean.startswith('hey'),
                    current_text_lower.startswith('hey '),
                    current_text_lower.startswith('hey,'),
                    current_text_lower.startswith('did you'),
                    current_text_lower.startswith('can you'),
                    current_text_lower.startswith('try to'),
                    current_text_lower.startswith('you should'),
                    current_text_lower.startswith('you can'),
                    current_text_lower.startswith('you need'),
                    '?' in current_text_raw,
                    current_text_clean.startswith('what'),
                    current_text_clean.startswith('how'),
                    current_text_clean.startswith('why'),
                    current_text_clean.startswith('when'),
                    current_text_clean.startswith('where'),
                ])
                
                # –î—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ —É–º–æ–≤
                condition_prev_short = prev_duration < 1.0
                condition_diff_speaker = prev_speaker != current_speaker
                condition_gap_ok = gap_to_prev < 3.0
                
                # –ü–†–Ü–û–†–ò–¢–ï–¢ 1: –°–ø–æ—á–∞—Ç–∫—É –∞–ª–≥–æ—Ä–∏—Ç–º –Ω–∞–º–∞–≥–∞—î—Ç—å—Å—è –≤–∏—Ä—ñ—à–∏—Ç–∏
                # –Ø–∫—â–æ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π —Å–µ–≥–º–µ–Ω—Ç –∫–æ—Ä–æ—Ç–∫–∏–π (<1s) —ñ –ø–æ—Ç–æ—á–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç —î –ø–∏—Ç–∞–Ω–Ω—è–º/—ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—î—é, —ñ gap < 3s
                # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –Ω–∞–≤—ñ—Ç—å —è–∫—â–æ —Å–ø—ñ–∫–µ—Ä–∏ –æ–¥–Ω–∞–∫–æ–≤—ñ (—Ü–µ –º–æ–∂–µ –±—É—Ç–∏ –ø–æ–º–∏–ª–∫–∞ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó)
                should_check = (condition_prev_short and 
                               is_question_or_instruction and
                               condition_gap_ok)
                
                if should_check:
                    print(f"  üîç [Segment {i}] –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —É–º–æ–≤: prev_short={condition_prev_short}, "
                          f"diff_speaker={condition_diff_speaker}, gap_ok={condition_gap_ok}, "
                          f"is_question={is_question_or_instruction}")
                    sys.stdout.flush()
                    
                    print(f"üîç [Segment {i}] –£–º–æ–≤–∏ –≤–∏–∫–æ–Ω–∞–Ω—ñ: prev_duration={prev_duration:.2f}s, "
                          f"prev_speaker={prev_speaker}, current_speaker={current_speaker}, "
                          f"gap={gap_to_prev:.2f}s, is_question={is_question_or_instruction}, "
                          f"text='{current_text_raw[:50]}...'")
                    sys.stdout.flush()
                    
                    # –°–ø–æ—á–∞—Ç–∫—É –∞–ª–≥–æ—Ä–∏—Ç–º –Ω–∞–º–∞–≥–∞—î—Ç—å—Å—è –≤–∏–∑–Ω–∞—á–∏—Ç–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
                    # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ (—Ç–æ–π, —Ö—Ç–æ –º–∞—î –±—ñ–ª—å—à–µ —Å–ª—ñ–≤)
                    all_speakers = set(seg['speaker'] for seg in combined)
                    speaker_word_counts = {}
                    for seg in combined:
                        speaker = seg['speaker']
                        word_count = len(seg.get('text', '').split())
                        speaker_word_counts[speaker] = speaker_word_counts.get(speaker, 0) + word_count
                    
                    main_speaker = max(speaker_word_counts.items(), key=lambda x: x[1])[0] if speaker_word_counts else 0
                    other_speakers = all_speakers - {prev_speaker}
                    
                    # –ê–ª–≥–æ—Ä–∏—Ç–º—ñ—á–Ω–µ —Ä—ñ—à–µ–Ω–Ω—è: –ø–∏—Ç–∞–Ω–Ω—è/—ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—è –ø—ñ—Å–ª—è –∫–æ—Ä–æ—Ç–∫–æ—ó —Ä–µ–ø–ª—ñ–∫–∏ –º–∞—î –Ω–∞–ª–µ–∂–∞—Ç–∏ –æ—Å–Ω–æ–≤–Ω–æ–º—É —Å–ø—ñ–∫–µ—Ä—É
                    algorithmic_speaker = main_speaker
                    
                    # –í–∏–∑–Ω–∞—á–∞—î–º–æ –≤–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤—ñ —Ç–æ–≥–æ, —á–∏ –ø–æ—Ç–æ—á–Ω–∏–π —Å–ø—ñ–∫–µ—Ä –≤—ñ–¥—Ä—ñ–∑–Ω—è—î—Ç—å—Å—è –≤—ñ–¥ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ
                    # –Ø–∫—â–æ –ø–æ—Ç–æ—á–Ω–∏–π —Å–ø—ñ–∫–µ—Ä = –æ—Å–Ω–æ–≤–Ω–æ–º—É, –∞–ª–µ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π –∫–æ—Ä–æ—Ç–∫–∏–π —ñ –ø–æ—Ç–æ—á–Ω–∏–π - –ø–∏—Ç–∞–Ω–Ω—è,
                    # —Ü–µ –º–æ–∂–µ –±—É—Ç–∏ –ø–æ–º–∏–ª–∫–∞ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó (–æ—Å–æ–±–ª–∏–≤–æ —è–∫—â–æ —Å–ø—ñ–∫–µ—Ä–∏ –æ–¥–Ω–∞–∫–æ–≤—ñ)
                    if current_speaker == main_speaker and prev_speaker == current_speaker:
                        # –ü—ñ–¥–æ–∑—Ä—ñ–ª–∏–π –≤–∏–ø–∞–¥–æ–∫: –∫–æ—Ä–æ—Ç–∫–∞ —Ä–µ–ø–ª—ñ–∫–∞ —ñ –ø–∏—Ç–∞–Ω–Ω—è –º–∞—é—Ç—å –æ–¥–Ω–∞–∫–æ–≤–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
                        # –∞–ª–µ –ø–∏—Ç–∞–Ω–Ω—è –º–∞—î –Ω–∞–ª–µ–∂–∞—Ç–∏ –æ—Å–Ω–æ–≤–Ω–æ–º—É —Å–ø—ñ–∫–µ—Ä—É
                        confidence = 0.4  # –ù–∏–∑—å–∫–∞ –≤–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—å - –ø–æ—Ç—Ä—ñ–±–µ–Ω LLM
                    elif current_speaker != main_speaker:
                        # –ü–æ—Ç–æ—á–Ω–∏–π —Å–ø—ñ–∫–µ—Ä –Ω–µ –æ—Å–Ω–æ–≤–Ω–∏–π - –≤–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—å —Å–µ—Ä–µ–¥–Ω—è
                        confidence = 0.6
                    else:
                        # –ü–æ—Ç–æ—á–Ω–∏–π —Å–ø—ñ–∫–µ—Ä = –æ—Å–Ω–æ–≤–Ω–æ–º—É —ñ –≤–æ–Ω–∏ —Ä—ñ–∑–Ω—ñ –≤—ñ–¥ –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ–≥–æ - –≤–∏—Å–æ–∫–∞ –≤–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—å
                        confidence = 0.9
                    
                    # –í–∏–∫–ª–∏–∫–∞—î–º–æ LLM —è–∫—â–æ:
                    # 1. –í–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—å –Ω–∏–∑—å–∫–∞ (< 0.7)
                    # 2. –ê–±–æ –ø–æ—Ç–æ—á–Ω–∏–π —Å–ø—ñ–∫–µ—Ä –Ω–µ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î –∞–ª–≥–æ—Ä–∏—Ç–º—ñ—á–Ω–æ–º—É —Ä—ñ—à–µ–Ω–Ω—é
                    # 3. –ê–±–æ —Å–ø—ñ–∫–µ—Ä–∏ –æ–¥–Ω–∞–∫–æ–≤—ñ (–º–æ–∂–ª–∏–≤–∞ –ø–æ–º–∏–ª–∫–∞ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó)
                    use_llm = (confidence < 0.7 or 
                              algorithmic_speaker != current_speaker or
                              (prev_speaker == current_speaker and condition_prev_short))
                    
                    if use_llm:
                        print(f"üîç [LLM Check] –°–∫–ª–∞–¥–Ω–∏–π –≤–∏–ø–∞–¥–æ–∫ –≤–∏—è–≤–ª–µ–Ω–æ: prev_speaker={prev_speaker}, current_speaker={current_speaker}, "
                              f"algorithmic_speaker={algorithmic_speaker}, confidence={confidence:.2f}, mode={llm_mode}")
                        sys.stdout.flush()
                        
                        # –í–∏–∫–ª–∏–∫–∞—î–º–æ LLM –¥–ª—è –≤–∏—Ä—ñ—à–µ–Ω–Ω—è —Å–∫–ª–∞–¥–Ω–æ–≥–æ –≤–∏–ø–∞–¥–∫—É
                        llm_speaker = call_llm_for_speaker_correction(
                            prev_seg, 
                            current_seg, 
                            gap_to_prev, 
                            all_segments_context=combined,
                            mode=llm_mode
                        )
                        
                        if llm_speaker is not None:
                            # LLM –≤–∏–∑–Ω–∞—á–∏–≤ —Å–ø—ñ–∫–µ—Ä–∞ - –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –π–æ–≥–æ
                            if current_speaker != llm_speaker:
                                combined[i]['speaker'] = llm_speaker
                                changes_made = True
                                print(f"ü§ñ LLM (segment-level): –í–∏–ø—Ä–∞–≤–ª–µ–Ω–æ —Å–µ–≥–º–µ–Ω—Ç '{current_seg.get('text', '')[:50]}...' "
                                      f"({current_seg['start']:.2f}-{current_seg['end']:.2f}s): "
                                      f"–°–ø—ñ–∫–µ—Ä {current_speaker} ‚Üí {llm_speaker} "
                                      f"(–ø—ñ—Å–ª—è –∫–æ—Ä–æ—Ç–∫–æ—ó —Ä–µ–ø–ª—ñ–∫–∏ —Å–ø—ñ–∫–µ—Ä–∞ {prev_speaker}, –ø–∏—Ç–∞–Ω–Ω—è/—ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—è)")
                        else:
                            # LLM –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π - –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –∞–ª–≥–æ—Ä–∏—Ç–º—ñ—á–Ω–µ —Ä—ñ—à–µ–Ω–Ω—è
                            print(f"‚ö†Ô∏è LLM –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –∞–ª–≥–æ—Ä–∏—Ç–º—ñ—á–Ω–µ —Ä—ñ—à–µ–Ω–Ω—è: {algorithmic_speaker}")
                            if current_speaker != algorithmic_speaker:
                                combined[i]['speaker'] = algorithmic_speaker
                                changes_made = True
                                print(f"üîß Algorithmic (segment-level): –í–∏–ø—Ä–∞–≤–ª–µ–Ω–æ —Å–µ–≥–º–µ–Ω—Ç '{current_seg.get('text', '')[:50]}...' "
                                      f"({current_seg['start']:.2f}-{current_seg['end']:.2f}s): "
                                      f"–°–ø—ñ–∫–µ—Ä {current_speaker} ‚Üí {algorithmic_speaker} "
                                      f"(–ø—ñ—Å–ª—è –∫–æ—Ä–æ—Ç–∫–æ—ó —Ä–µ–ø–ª—ñ–∫–∏ —Å–ø—ñ–∫–µ—Ä–∞ {prev_speaker}, –ø–∏—Ç–∞–Ω–Ω—è/—ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—è, confidence={confidence:.2f})")
                    else:
                        # –ê–ª–≥–æ—Ä–∏—Ç–º –≤–ø–µ–≤–Ω–µ–Ω–∏–π - –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –π–æ–≥–æ —Ä—ñ—à–µ–Ω–Ω—è –±–µ–∑ LLM
                        if current_speaker != algorithmic_speaker:
                            combined[i]['speaker'] = algorithmic_speaker
                            changes_made = True
                            print(f"‚úÖ Algorithmic (segment-level, high confidence): –í–∏–ø—Ä–∞–≤–ª–µ–Ω–æ —Å–µ–≥–º–µ–Ω—Ç '{current_seg.get('text', '')[:50]}...' "
                                  f"({current_seg['start']:.2f}-{current_seg['end']:.2f}s): "
                                  f"–°–ø—ñ–∫–µ—Ä {current_speaker} ‚Üí {algorithmic_speaker} "
                                  f"(–ø—ñ—Å–ª—è –∫–æ—Ä–æ—Ç–∫–æ—ó —Ä–µ–ø–ª—ñ–∫–∏ —Å–ø—ñ–∫–µ—Ä–∞ {prev_speaker}, –ø–∏—Ç–∞–Ω–Ω—è/—ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—è, confidence={confidence:.2f})")
            
            if not changes_made:
                break
    
    # –î—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞: –ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ñ—ñ–Ω–∞–ª—å–Ω–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    final_speakers = set(seg['speaker'] for seg in combined)
    print(f"‚úÖ Combined result: {len(combined)} segments, {len(final_speakers)} unique speakers: {sorted(final_speakers)}")
    
    return combined


def process_audio_background(job_id, filepath, num_speakers, language, segment_duration, overlap, processing_mode='fast'):
    """–û–±—Ä–æ–±–∫–∞ –∞—É–¥—ñ–æ –≤ —Ñ–æ–Ω–æ–≤–æ–º—É –ø–æ—Ç–æ—Ü—ñ"""
    try:
        with jobs_lock:
            jobs[job_id]['status'] = 'processing'
            include_transcription = jobs[job_id].get('include_transcription', True)
        
        print(f"üîÑ [Job {job_id}] Starting background processing (mode: {processing_mode})...")
        import sys
        sys.stdout.flush()
        
        # –û–±—á–∏—Å–ª—é—î–º–æ —Ç—Ä–∏–≤–∞–ª—ñ—Å—Ç—å –∞—É–¥—ñ–æ
        try:
            audio_duration = librosa.get_duration(path=filepath)
            print(f"‚è±Ô∏è  [Job {job_id}] Audio duration: {audio_duration:.2f} seconds")
        except Exception as e:
            print(f"‚ö†Ô∏è  [Job {job_id}] Could not determine audio duration: {e}")
            audio_duration = 0
        
        # –í–∏–∑–Ω–∞—á–∞—î–º–æ –º–æ–≤—É –¥–ª—è Speechmatics
        lang_code = 'en'
        if language:
            lang_map = {
                'english': 'en', 'en': 'en',
                'ukrainian': 'uk', 'uk': 'uk',
                'arabic': 'ar', 'ar': 'ar',
                'russian': 'ru', 'ru': 'ru'
            }
            lang_code = lang_map.get(language.lower(), 'en')
        
        # –û–±—Ä–æ–±–∫–∞ –∑–∞–ª–µ–∂–Ω–æ –≤—ñ–¥ —Ä–µ–∂–∏–º—É
        if processing_mode == 'smart':
            # Smart mode: Speechmatics (—Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—è + –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—è)
            print(f"üéØ [Job {job_id}] Using Smart mode: Speechmatics")
            sys.stdout.flush()
            
            if include_transcription:
                try:
                    transcription, transcription_segments, words = transcribe_with_speechmatics(filepath, language=lang_code)
                    
                    print(f"üìä [Job {job_id}] Speechmatics result:")
                    print(f"   - transcription type: {type(transcription)}, length: {len(transcription) if transcription else 0}")
                    print(f"   - transcription_segments: {len(transcription_segments) if transcription_segments else 0} segments")
                    print(f"   - words: {len(words) if words else 0} words")
                    sys.stdout.flush()
                    
                    if not transcription or not words:
                        print(f"‚ö†Ô∏è  [Job {job_id}] Speechmatics transcription failed or empty")
                        sys.stdout.flush()
                        with jobs_lock:
                            jobs[job_id]['status'] = 'processing'
                            jobs[job_id]['error'] = 'Speechmatics transcription is still processing or failed. Please wait or retry.'
                        return
                    
                    # Speechmatics –≤–∂–µ –º—ñ—Å—Ç–∏—Ç—å –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—é –≤ words (speaker labels)
                    # –°—Ç–≤–æ—Ä—é—î–º–æ —Å–µ–≥–º–µ–Ω—Ç–∏ –∑ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—î—é –∑—ñ —Å–ª—ñ–≤
                    diarization_segments = []
                    current_speaker = None
                    current_start = None
                    current_end = None
                    current_text = []
                    
                    for word in words:
                        word_speaker = word.get('speaker', 0)
                        word_start = word.get('start', 0)
                        word_end = word.get('end', 0)
                        word_text = word.get('word', '')
                        
                        if current_speaker is None:
                            current_speaker = word_speaker
                            current_start = word_start
                            current_text = [word_text]
                        elif word_speaker == current_speaker:
                            current_text.append(word_text)
                        else:
                            # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π —Å–µ–≥–º–µ–Ω—Ç
                            if current_start is not None and current_end is not None:
                                diarization_segments.append({
                                    'speaker': current_speaker,
                                    'start': round(current_start, 2),
                                    'end': round(current_end, 2),
                                    'text': ' '.join(current_text)
                                })
                            # –ü–æ—á–∏–Ω–∞—î–º–æ –Ω–æ–≤–∏–π —Å–µ–≥–º–µ–Ω—Ç
                            current_speaker = word_speaker
                            current_start = word_start
                            current_text = [word_text]
                        
                        current_end = word_end
                    
                    # –î–æ–¥–∞—î–º–æ –æ—Å—Ç–∞–Ω–Ω—ñ–π —Å–µ–≥–º–µ–Ω—Ç
                    if current_speaker is not None and current_start is not None and current_end is not None:
                        diarization_segments.append({
                            'speaker': current_speaker,
                            'start': round(current_start, 2),
                            'end': round(current_end, 2),
                            'text': ' '.join(current_text)
                        })
                    
                    # –§–æ—Ä–º—É—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                    result = {
                        'success': True,
                        'duration': round(audio_duration, 2),
                        'diarization': {
                            'segments': diarization_segments,
                            'num_speakers': len(set(seg.get('speaker', 0) for seg in diarization_segments)) if diarization_segments else 0
                        },
                        'transcription': {
                            'full_text': transcription,
                            'segments': transcription_segments or []
                        },
                        'combined': {
                            'segments': diarization_segments,  # –í–∂–µ –º—ñ—Å—Ç–∏—Ç—å —Ç–µ–∫—Å—Ç
                            'num_speakers': len(set(seg.get('speaker', 0) for seg in diarization_segments)) if diarization_segments else 0,
                            'num_segments': len(diarization_segments)
                        }
                    }
                    
                except Exception as e:
                    print(f"‚ùå [Job {job_id}] Error during Speechmatics transcription: {e}")
                    import traceback
                    traceback.print_exc()
                    sys.stdout.flush()
                    with jobs_lock:
                        jobs[job_id]['status'] = 'failed'
                        jobs[job_id]['error'] = f'Speechmatics error: {str(e)}. Please check your internet connection and API key, then retry.'
                        jobs[job_id]['code'] = 'SPEECHMATICS_ERROR'
                    try:
                        if os.path.exists(filepath):
                            os.remove(filepath)
                    except:
                        pass
                    return
            else:
                # –ë–µ–∑ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó - –≤—Å–µ –æ–¥–Ω–æ –ø–æ—Ç—Ä—ñ–±–Ω–∞ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—è
                result = {
                    'success': True,
                    'duration': round(audio_duration, 2),
                    'diarization': {
                        'segments': [],
                        'num_speakers': 0
                    }
                }
        else:
            # Fast mode: Whisper + PyAnnote (–æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∞ –ª–æ–≥—ñ–∫–∞)
            print(f"‚ö° [Job {job_id}] Using Fast mode: Whisper + PyAnnote")
            sys.stdout.flush()
            
            # –í–∏—Ç—è–≥—É—î–º–æ –µ–º–±–µ–¥–¥–∏–Ω–≥–∏
            print(f"üîÑ [Job {job_id}] Extracting speaker embeddings...")
            sys.stdout.flush()
            embeddings, timestamps = extract_speaker_embeddings(
                filepath, 
                segment_duration=segment_duration, 
                overlap=overlap
            )
            
            if embeddings is None:
                with jobs_lock:
                    jobs[job_id]['status'] = 'failed'
                    jobs[job_id]['error'] = 'Failed to extract speaker embeddings. Audio may be corrupted or unsupported format.'
                    jobs[job_id]['code'] = 'EMBEDDING_EXTRACTION_FAILED'
                os.remove(filepath)
                return
            
            if len(embeddings) == 0:
                with jobs_lock:
                    jobs[job_id]['status'] = 'failed'
                    jobs[job_id]['error'] = f'Audio too short (duration: {audio_duration:.2f}s). Minimum recommended: 2 seconds.'
                    jobs[job_id]['code'] = 'AUDIO_TOO_SHORT'
                os.remove(filepath)
                return
            
            # –î—ñ–∞—Ä–∏–∑–∞—Ü—ñ—è
            print(f"üîÑ [Job {job_id}] Performing diarization...")
            sys.stdout.flush()
            diarization_segments = diarize_audio(embeddings, timestamps, num_speakers)
            
            if not diarization_segments:
                with jobs_lock:
                    jobs[job_id]['status'] = 'failed'
                    jobs[job_id]['error'] = 'Diarization failed. Could not identify speakers.'
                    jobs[job_id]['code'] = 'DIARIZATION_FAILED'
                os.remove(filepath)
                return
            
            # –§–æ—Ä–º—É—î–º–æ –±–∞–∑–æ–≤–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            result = {
                'success': True,
                'duration': round(audio_duration, 2),
                'diarization': {
                    'segments': diarization_segments,
                    'num_speakers': len(set(seg.get('speaker', 0) for seg in diarization_segments)) if diarization_segments else 0
                }
            }
            
            # –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—è (—è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–∞)
            if include_transcription:
                print(f"üîÑ [Job {job_id}] Transcribing audio...")
                sys.stdout.flush()
                try:
                    transcription, transcription_segments, words = transcribe_audio(filepath, language=language, transcription_provider='whisper')
                    
                    print(f"üìä [Job {job_id}] Transcription result:")
                    print(f"   - transcription type: {type(transcription)}, length: {len(transcription) if transcription else 0}")
                    print(f"   - transcription_segments: {len(transcription_segments) if transcription_segments else 0} segments")
                    print(f"   - words: {len(words) if words else 0} words")
                    if transcription:
                        print(f"   - transcription preview (first 100 chars): {transcription[:100]}")
                    sys.stdout.flush()
                    
                    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—è —É—Å–ø—ñ—à–Ω–∞
                    if not transcription or not words:
                        print(f"‚ö†Ô∏è  [Job {job_id}] Transcription failed or empty - keeping status 'processing'")
                        print(f"   - transcription is None/empty: {transcription is None or not transcription}")
                        print(f"   - words is None/empty: {words is None or not words}")
                        sys.stdout.flush()
                        # –ó–∞–ª–∏—à–∞—î–º–æ —Å—Ç–∞—Ç—É—Å 'processing', —â–æ–± Shortcut –ø—Ä–æ–¥–æ–≤–∂—É–≤–∞–≤ –ø–æ–ª—ñ–Ω–≥
                        with jobs_lock:
                            jobs[job_id]['status'] = 'processing'
                            jobs[job_id]['error'] = 'Transcription is still processing or failed. Please wait or retry.'
                        return  # –ù–µ –∑–∞–≤–µ—Ä—à—É—î–º–æ –æ–±—Ä–æ–±–∫—É, –∑–∞–ª–∏—à–∞—î–º–æ —Å—Ç–∞—Ç—É—Å 'processing'
                    
                    # –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—è —É—Å–ø—ñ—à–Ω–∞ - –ø—Ä–æ–¥–æ–≤–∂—É—î–º–æ
                    result['transcription'] = {
                        'full_text': transcription,
                        'segments': transcription_segments or []
                    }
                    
                    # –û–±'—î–¥–Ω–∞–Ω–Ω—è –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó —Ç–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó
                    print(f"üîÑ [Job {job_id}] Combining results...")
                    print(f"   - Diarization segments: {len(diarization_segments)}")
                    print(f"   - Words for combination: {len(words)}")
                    sys.stdout.flush()
                    # –û—Ç—Ä–∏–º—É—î–º–æ llm_mode –∑ jobs –∞–±–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ 'local' –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º
                    llm_mode = jobs[job_id].get('llm_mode', 'local')
                    # –ù–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ —Ä–µ–∂–∏–º
                    if llm_mode == 'smart2':
                        llm_mode = 'smart-2'
                    combined_segments = combine_diarization_and_transcription(
                        diarization_segments, 
                        words,
                        llm_mode=llm_mode
                    )
                    
                    print(f"‚úÖ [Job {job_id}] Combined result: {len(combined_segments) if combined_segments else 0} segments")
                    sys.stdout.flush()
                    
                    result['combined'] = {
                        'segments': combined_segments if combined_segments else [],
                        'num_speakers': len(set(seg.get('speaker', 0) for seg in combined_segments)) if combined_segments else 0,
                        'num_segments': len(combined_segments) if combined_segments else 0
                    }
                except Exception as e:
                    print(f"‚ùå [Job {job_id}] Error during transcription: {e}")
                    import traceback
                    traceback.print_exc()
                    sys.stdout.flush()
                    # –ó–∞–ª–∏—à–∞—î–º–æ —Å—Ç–∞—Ç—É—Å 'processing' –ø—Ä–∏ –ø–æ–º–∏–ª—Ü—ñ, —â–æ–± –º–æ–∂–Ω–∞ –±—É–ª–æ —Å–ø—Ä–æ–±—É–≤–∞—Ç–∏ –∑–Ω–æ–≤—É
                    with jobs_lock:
                        jobs[job_id]['status'] = 'processing'
                        jobs[job_id]['error'] = f'Transcription error: {str(e)}. Please retry.'
                    return  # –ù–µ –∑–∞–≤–µ—Ä—à—É—î–º–æ –æ–±—Ä–æ–±–∫—É
            else:
                result['transcription'] = None
                result['combined'] = None
        
        # –í—Å—Ç–∞–Ω–æ–≤–ª—é—î–º–æ —Å—Ç–∞—Ç—É—Å 'completed' —Ç—ñ–ª—å–∫–∏ —è–∫—â–æ –≤—Å–µ —É—Å–ø—ñ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–æ
        with jobs_lock:
            jobs[job_id]['status'] = 'completed'
            jobs[job_id]['result'] = result
        
        # –í–∏–¥–∞–ª—è—î–º–æ —Ñ–∞–π–ª
        try:
            os.remove(filepath)
        except:
            pass
        
        print(f"‚úÖ [Job {job_id}] Processing complete!")
        sys.stdout.flush()
        
    except Exception as e:
        print(f"‚ùå [Job {job_id}] Error: {e}")
        import traceback
        import sys
        traceback.print_exc()
        sys.stdout.flush()
        sys.stderr.flush()
        with jobs_lock:
            jobs[job_id]['status'] = 'failed'
            jobs[job_id]['error'] = str(e)
        try:
            if os.path.exists(filepath):
                os.remove(filepath)
        except:
            pass


def allowed_file(filename):
    """–ü–µ—Ä–µ–≤—ñ—Ä—è—î, —á–∏ –¥–æ–∑–≤–æ–ª–µ–Ω–∏–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª—É"""
    return '.' in filename and \
           filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS


def detect_audio_format_from_base64(base64_data):
    """
    –í–∏–∑–Ω–∞—á–∞—î —Ñ–æ—Ä–º–∞—Ç –∞—É–¥—ñ–æ —Ñ–∞–π–ª—É –∑ base64 –¥–∞–Ω–∏—Ö –∑–∞ —Å–∏–≥–Ω–∞—Ç—É—Ä–æ—é (magic bytes).
    –ü–æ–≤–µ—Ä—Ç–∞—î —Ä–æ–∑—à–∏—Ä–µ–Ω–Ω—è —Ñ–∞–π–ª—É (–±–µ–∑ –∫—Ä–∞–ø–∫–∏) –∞–±–æ None, —è–∫—â–æ –Ω–µ –≤–¥–∞–ª–æ—Å—è –≤–∏–∑–Ω–∞—á–∏—Ç–∏.
    """
    try:
        import base64
        # –û—á–∏—â–∞—î–º–æ base64 —Ä—è–¥–æ–∫
        base64_clean = str(base64_data).strip()
        
        # –í–∏–¥–∞–ª—è—î–º–æ data URI –ø—Ä–µ—Ñ—ñ–∫—Å (—è–∫—â–æ —î)
        if ',' in base64_clean:
            base64_clean = base64_clean.split(',', 1)[1]
        
        # –í–∏–¥–∞–ª—è—î–º–æ –≤—Å—ñ –ø—Ä–æ–±—ñ–ª–∏, –ø–µ—Ä–µ–Ω–æ—Å–∏ —Ä—è–¥–∫—ñ–≤
        base64_clean = base64_clean.replace('\n', '').replace('\r', '').replace(' ', '').replace('\t', '')
        
        # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ base64url (URL-safe) –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏–π base64
        if '-' in base64_clean or '_' in base64_clean:
            base64_clean = base64_clean.replace('-', '+').replace('_', '/')
        
        # –í–∏–¥–∞–ª—è—î–º–æ –∫—Ä–∞–ø–∫–∏ (–Ω–µ–≤–∞–ª—ñ–¥–Ω—ñ –≤ base64)
        if '.' in base64_clean:
            base64_clean = base64_clean.replace('.', '')
        
        # –î–æ–¥–∞—î–º–æ padding, —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ
        missing_padding = len(base64_clean) % 4
        if missing_padding:
            base64_clean += '=' * (4 - missing_padding)
        
        # –î–µ–∫–æ–¥—É—î–º–æ –ø–µ—Ä—à—ñ 20 –±–∞–π—Ç –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ —Å–∏–≥–Ω–∞—Ç—É—Ä–∏
        decoded = base64.b64decode(base64_clean[:100], validate=False)  # –ü–µ—Ä—à—ñ ~75 —Å–∏–º–≤–æ–ª—ñ–≤ base64 = ~50 –±–∞–π—Ç
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Å–∏–≥–Ω–∞—Ç—É—Ä–∏ —Ñ–æ—Ä–º–∞—Ç—ñ–≤
        if decoded.startswith(b'RIFF') and b'WAVE' in decoded[:12]:
            return 'wav'
        elif decoded.startswith(b'\xff\xfb') or decoded.startswith(b'\xff\xf3') or decoded.startswith(b'\xff\xf2'):
            return 'mp3'
        elif decoded.startswith(b'\xff\xf1') or decoded.startswith(b'\xff\xf9'):
            return 'aac'
        elif decoded.startswith(b'fLaC'):
            return 'flac'
        elif decoded.startswith(b'OggS'):
            return 'ogg'
        elif b'ftyp' in decoded[:20]:
            # M4A/M4V/MP4 –º–∞—é—Ç—å ftyp –Ω–∞ –ø–æ—á–∞—Ç–∫—É
            if b'm4a' in decoded[:30] or b'M4A' in decoded[:30]:
                return 'm4a'
            elif b'mp4' in decoded[:30] or b'MP4' in decoded[:30]:
                return 'm4a'  # MP4 –∞—É–¥—ñ–æ —Ç–∞–∫–æ–∂ –æ–±—Ä–æ–±–ª—è—î–º–æ —è–∫ m4a
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –±—ñ–ª—å—à –¥–µ—Ç–∞–ª—å–Ω–æ –¥–ª—è M4A
            if b'ftypM4A' in decoded[:30] or b'ftypmp4' in decoded[:30]:
                return 'm4a'
            # –Ø–∫—â–æ —î ftyp, –∞–ª–µ –Ω–µ –≤–∏–∑–Ω–∞—á–∏–ª–∏ —Ç–æ—á–Ω–æ, —Å–ø—Ä–æ–±—É—î–º–æ m4a (–Ω–∞–π–ø–æ—à–∏—Ä–µ–Ω—ñ—à–∏–π –¥–ª—è iOS)
            return 'm4a'
        
        return None
    except Exception as e:
        print(f"‚ö†Ô∏è Error detecting format from base64: {e}")
        return None


@app.errorhandler(400)
def handle_bad_request(e):
    """–û–±—Ä–æ–±–∫–∞ –ø–æ–º–∏–ª–æ–∫ 400 Bad Request"""
    print(f"‚ùå 400 Bad Request error: {e}")
    print(f"   Request method: {request.method}")
    print(f"   Content-Type: {request.content_type}")
    print(f"   Headers: {dict(request.headers)}")
    import sys
    sys.stdout.flush()
    return jsonify({
        'success': False,
        'error': f'Bad Request: {str(e)}. Make sure to send file as multipart/form-data.',
        'code': 'BAD_REQUEST',
        'debug_info': {
            'content_type': request.content_type,
            'method': request.method
        }
    }), 400


@app.route('/api/health', methods=['GET'])
def health():
    """–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Å—Ç–∞–Ω—É —Å–µ—Ä–≤–µ—Ä–∞"""
    return jsonify({
        'status': 'ok',
        'speaker_model_loaded': speaker_model is not None,
        'whisper_model_loaded': whisper_model is not None
    })


@app.route('/enhance-main-speaker', methods=['GET'])
def enhance_main_speaker_page():
    """–°—Ç–æ—Ä—ñ–Ω–∫–∞ –¥–ª—è –≤–∏–¥—ñ–ª–µ–Ω–Ω—è –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞"""
    return send_from_directory('.', 'enhance-main-speaker.html')


@app.route('/api/diarize', methods=['POST', 'OPTIONS'])
def api_diarize():
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∏–π API –µ–Ω–¥–ø–æ—ñ–Ω—Ç –¥–ª—è –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó —Ç–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó.
    –ü—Ä–∏–π–º–∞—î JSON –∑ base64-encoded —Ñ–∞–π–ª–æ–º (—à–≤–∏–¥—à–µ, –Ω—ñ–∂ multipart/form-data).
    –ü–æ–≤–µ—Ä—Ç–∞—î job_id –æ–¥—Ä–∞–∑—É, –æ–±—Ä–æ–±–∫–∞ –≤–∏–∫–æ–Ω—É—î—Ç—å—Å—è –≤ —Ñ–æ–Ω—ñ.
    –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ GET /api/diarize/{job_id}/status –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ —Å—Ç–∞—Ç—É—Å—É.
    """
    import sys
    import base64
    print(f"üîµ [API] /api/diarize called - Method: {request.method}, Remote: {request.remote_addr}")
    sys.stdout.flush()
    
    # –û–±—Ä–æ–±–∫–∞ OPTIONS –¥–ª—è preflight –∑–∞–ø–∏—Ç—ñ–≤ (CORS)
    if request.method == 'OPTIONS':
        print("‚úÖ OPTIONS preflight request received from", request.remote_addr)
        response = jsonify({})
        response.headers.add('Access-Control-Allow-Origin', '*')
        response.headers.add('Access-Control-Allow-Headers', 'Content-Type')
        response.headers.add('Access-Control-Allow-Methods', 'POST, OPTIONS')
        sys.stdout.flush()
        return response
    
    print(f"üì• POST /api/diarize request received from {request.remote_addr}")
    print(f"üìã Request headers: {dict(request.headers)}")
    print(f"üìã Request method: {request.method}")
    print(f"üìã Request content type: {request.content_type}")
    print(f"üìã Request content length: {request.content_length} bytes")
    sys.stdout.flush()
    
    # –ì–µ–Ω–µ—Ä—É—î–º–æ job_id –î–û try –±–ª–æ–∫—É
    job_id = str(uuid.uuid4())
    filepath = None
    
    try:
        print(f"üìã Step 1: Checking Content-Type: {request.content_type}")
        sys.stdout.flush()
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —Ü–µ JSON (base64) –∞–±–æ multipart/form-data (legacy)
        is_json = request.is_json or (request.content_type and 'application/json' in request.content_type)
        
        if is_json:
            print(f"üì¶ Step 2: Parsing JSON request (fast, non-blocking)...")
            print(f"üìã Step 2.1: Content-Type: {request.content_type}")
            print(f"üìã Step 2.2: Content-Length: {request.content_length} bytes")
            sys.stdout.flush()
            
            # JSON –ø–∞—Ä—Å–∏—Ç—å—Å—è —à–≤–∏–¥–∫–æ, –Ω–µ –±–ª–æ–∫—É—î
            try:
                data = request.get_json()
                if not data:
                    raise ValueError("No JSON data received")
                
                print(f"üì¶ Step 2.3: JSON parsed successfully")
                print(f"üìã Step 2.4: JSON keys: {list(data.keys()) if isinstance(data, dict) else 'not a dict'}")
                
                # –õ–æ–≥—É—î–º–æ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ —Ñ–∞–π–ª (–±–µ–∑ —Å–∞–º–æ–≥–æ base64, –±–æ –≤—ñ–Ω –≤–µ–ª–∏–∫–∏–π)
                if 'file' in data:
                    file_data = data['file']
                    if isinstance(file_data, str):
                        print(f"üìã Step 2.5: File base64 length: {len(file_data)} characters")
                        print(f"üìã Step 2.6: File base64 preview (first 100 chars): {file_data[:100]}")
                        print(f"üìã Step 2.7: File base64 preview (last 50 chars): {file_data[-50:]}")
                        print(f"üìã Step 2.8: File data type check: {type(file_data)}")
                        print(f"üìã Step 2.9: Contains spaces: {file_data.count(' ')}, Contains newlines: {file_data.count(chr(10))}")
                        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –Ω–∞ data URI –ø—Ä–µ—Ñ—ñ–∫—Å
                        if file_data.startswith('data:'):
                            print(f"‚ö†Ô∏è Step 2.10: WARNING - base64 starts with data URI prefix!")
                        if ',' in file_data:
                            print(f"‚ö†Ô∏è Step 2.11: WARNING - base64 contains comma (possible data URI)")
                        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —Ü–µ –Ω–µ –ø—Ä–æ—Å—Ç–æ —Ç–µ–∫—Å—Ç/–Ω–∞–∑–≤–∞ —Ñ–∞–π–ª—É
                        if len(file_data) < 50:
                            print(f"‚ùå Step 2.12: ERROR - Base64 string is too short! This might be a filename, not base64 data!")
                            print(f"   Full content: '{file_data}'")
                        elif ' ' in file_data and file_data.count(' ') > len(file_data) * 0.1:
                            print(f"‚ùå Step 2.13: ERROR - Base64 string contains too many spaces! This might be a filename, not base64 data!")
                    else:
                        print(f"‚ö†Ô∏è Step 2.5: File data is not a string: {type(file_data)}")
                        print(f"   Value: {file_data}")
                
                if 'filename' in data:
                    print(f"üìã Step 2.10: Filename: {data['filename']}")
                
                sys.stdout.flush()
                
                # –û—Ç—Ä–∏–º—É—î–º–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ (–≤—Å—ñ –º–∞—é—Ç—å –∑–Ω–∞—á–µ–Ω–Ω—è –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º)
                num_speakers = data.get('num_speakers', 2)  # –ó–∞–≤–∂–¥–∏ 2
                language = data.get('language', 'English')  # –ó–∞–≤–∂–¥–∏ English
                if language and language.lower() == 'auto':
                    language = None
                segment_duration = float(data.get('segment_duration', 2.5))  # –ó–∞–≤–∂–¥–∏ 2.5
                overlap = float(data.get('overlap', 0.4))  # –ó–∞–≤–∂–¥–∏ 0.4
                include_transcription = data.get('include_transcription', True)  # –ó–∞–≤–∂–¥–∏ True
                processing_mode = data.get('mode', 'fast')  # 'smart' –∞–±–æ 'fast'
                
                # –û—Ç—Ä–∏–º—É—î–º–æ base64 —Ñ–∞–π–ª
                file_base64 = data.get('file')
                filename = data.get('filename', 'audio.wav')
                
                if not file_base64:
                    return jsonify({
                        'success': False,
                        'error': 'No file data provided. Send file as base64 string in "file" field.',
                        'code': 'NO_FILE'
                    }), 400
                
                # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —Ü–µ –¥—ñ–π—Å–Ω–æ base64 (–∞ –Ω–µ –ø—Ä–æ—Å—Ç–æ —Ç–µ–∫—Å—Ç)
                file_base64_str = str(file_base64).strip()
                # Base64 –∑–∞–∑–≤–∏—á–∞–π –¥–æ–≤—à–∏–π –∑–∞ 100 —Å–∏–º–≤–æ–ª—ñ–≤ –¥–ª—è –∞—É–¥—ñ–æ —Ñ–∞–π–ª—ñ–≤
                # –Ü –º—ñ—Å—Ç–∏—Ç—å –±–∞–≥–∞—Ç–æ –±—É–∫–≤/—Ü–∏—Ñ—Ä, –∞ –Ω–µ –ø—Ä–æ—Å—Ç–æ —Ç–µ–∫—Å—Ç
                if len(file_base64_str) < 50:
                    print(f"‚ö†Ô∏è [Job {job_id}] WARNING: Base64 string is very short ({len(file_base64_str)} chars). This might be a filename instead of base64 data!")
                    print(f"üìã [Job {job_id}] Received data: {file_base64_str}")
                    return jsonify({
                        'success': False,
                        'error': f'Invalid base64 data: string is too short ({len(file_base64_str)} chars). Expected base64-encoded audio file, but received: "{file_base64_str[:50]}...". Make sure you use "Encode Media" action with Base64 format in Shortcut.',
                        'code': 'INVALID_BASE64_TOO_SHORT'
                    }), 400
                
                # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —Ü–µ –Ω–µ –ø—Ä–æ—Å—Ç–æ —Ç–µ–∫—Å—Ç (—è–∫—â–æ –º—ñ—Å—Ç–∏—Ç—å –±–∞–≥–∞—Ç–æ –ø—Ä–æ–±—ñ–ª—ñ–≤ –∞–±–æ –Ω–µ –º—ñ—Å—Ç–∏—Ç—å —Ç–∏–ø–æ–≤–∏—Ö base64 —Å–∏–º–≤–æ–ª—ñ–≤)
                if ' ' in file_base64_str and file_base64_str.count(' ') > len(file_base64_str) * 0.1:
                    print(f"‚ö†Ô∏è [Job {job_id}] WARNING: Base64 string contains many spaces. This might be a filename instead of base64 data!")
                    print(f"üìã [Job {job_id}] Received data: {file_base64_str}")
                    return jsonify({
                        'success': False,
                        'error': f'Invalid base64 data: contains too many spaces. This looks like a filename, not base64-encoded audio. Received: "{file_base64_str[:100]}...". Make sure you use "Encode Media" action with Base64 format in Shortcut.',
                        'code': 'INVALID_BASE64_FILENAME'
                    }), 400
                
                # –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≤–∏–∑–Ω–∞—á–∞—î–º–æ —Ñ–æ—Ä–º–∞—Ç, —è–∫—â–æ filename –Ω–µ –º–∞—î —Ä–æ–∑—à–∏—Ä–µ–Ω–Ω—è –∞–±–æ –º–∞—î –Ω–µ–¥–æ–∑–≤–æ–ª–µ–Ω–µ
                original_filename = filename
                if '.' not in filename or not allowed_file(filename):
                    print(f"üîç Detecting audio format from base64 data (filename: {filename})...")
                    sys.stdout.flush()
                    
                    detected_format = detect_audio_format_from_base64(file_base64)
                    
                    if detected_format:
                        # –í–∏–¥–∞–ª—è—î–º–æ —Å—Ç–∞—Ä–µ —Ä–æ–∑—à–∏—Ä–µ–Ω–Ω—è, —è–∫—â–æ –≤–æ–Ω–æ —î, —ñ –¥–æ–¥–∞—î–º–æ –≤–∏–∑–Ω–∞—á–µ–Ω–µ
                        if '.' in filename:
                            filename = filename.rsplit('.', 1)[0] + '.' + detected_format
                        else:
                            filename = filename + '.' + detected_format
                        print(f"‚úÖ Detected format: {detected_format} ‚Üí filename: {filename}")
                    else:
                        # –Ø–∫—â–æ –Ω–µ –≤–¥–∞–ª–æ—Å—è –≤–∏–∑–Ω–∞—á–∏—Ç–∏, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ .m4a –¥–ª—è iOS —Ñ–∞–π–ª—ñ–≤
                        if 'Screen Recording' in filename or 'screen' in filename.lower():
                            detected_format = 'm4a'
                        else:
                            detected_format = 'm4a'  # –î–µ—Ñ–æ–ª—Ç –¥–ª—è iOS
                        
                        if '.' in filename:
                            filename = filename.rsplit('.', 1)[0] + '.' + detected_format
                        else:
                            filename = filename + '.' + detected_format
                        print(f"‚ö†Ô∏è Could not detect format, using default: {detected_format} ‚Üí filename: {filename}")
                    sys.stdout.flush()
                
                print(f"üìù Step 3: Parameters extracted: num_speakers={num_speakers}, language={language}, segment_duration={segment_duration}, overlap={overlap}, filename={filename} (original: {original_filename})")
                sys.stdout.flush()
                
                # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç—É —Ñ–∞–π–ª—É
                if not allowed_file(filename):
                    return jsonify({
                        'success': False,
                        'error': f'Invalid audio format. Allowed: {", ".join(ALLOWED_EXTENSIONS)}. Original filename: {original_filename}, processed: {filename}',
                        'code': 'INVALID_FORMAT'
                    }), 400
                
                # –°—Ç–≤–æ—Ä—é—î–º–æ –∑–∞–≤–¥–∞–Ω–Ω—è –î–û –¥–µ–∫–æ–¥—É–≤–∞–Ω–Ω—è —Ñ–∞–π–ª—É
                with jobs_lock:
                    jobs[job_id] = {
                        'status': 'pending',
                        'result': None,
                        'error': None,
                        'created_at': datetime.now(),
                        'include_transcription': include_transcription,
                        'processing_mode': processing_mode
                    }
                
                print(f"‚úÖ [Job {job_id}] Job created, returning job_id IMMEDIATELY")
                sys.stdout.flush()
                
                # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ job_id –û–î–†–ê–ó–£ (–î–û –¥–µ–∫–æ–¥—É–≤–∞–Ω–Ω—è base64!)
                response = jsonify({
                    'success': True,
                    'job_id': job_id,
                    'status': 'pending',
                    'message': 'Processing started. Use GET /api/diarize/{job_id}/status to check progress.'
                })
                response.headers.add('Access-Control-Allow-Origin', '*')
                
                # –î–µ–∫–æ–¥—É—î–º–æ base64 —Ç–∞ –∑–±–µ—Ä—ñ–≥–∞—î–º–æ —Ñ–∞–π–ª –≤ —Ñ–æ–Ω—ñ –ü–û–°–õ–Ø –≤—ñ–¥–ø—Ä–∞–≤–∫–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ
                def decode_and_process():
                    try:
                        print(f"üíæ [Job {job_id}] Background: Starting base64 decode...")
                        print(f"üìã [Job {job_id}] Background: Original base64 type: {type(file_base64)}")
                        print(f"üìã [Job {job_id}] Background: Original base64 length: {len(str(file_base64))} characters")
                        sys.stdout.flush()
                        
                        # –û—á–∏—â–∞—î–º–æ base64 —Ä—è–¥–æ–∫
                        file_base64_clean = str(file_base64).strip()
                        print(f"üìã [Job {job_id}] Background: After strip() length: {len(file_base64_clean)}")
                        
                        # –í–∏–¥–∞–ª—è—î–º–æ data URI –ø—Ä–µ—Ñ—ñ–∫—Å (—è–∫—â–æ —î), –Ω–∞–ø—Ä–∏–∫–ª–∞–¥: "data:audio/m4a;base64,"
                        if ',' in file_base64_clean:
                            before_split = file_base64_clean
                            file_base64_clean = file_base64_clean.split(',', 1)[1]
                            print(f"üìã [Job {job_id}] Background: Found comma, removed data URI prefix. Before: {len(before_split)}, After: {len(file_base64_clean)}")
                            print(f"üìã [Job {job_id}] Background: Removed prefix: {before_split[:before_split.index(',')]}")
                        
                        # –í–∏–¥–∞–ª—è—î–º–æ –≤—Å—ñ –ø—Ä–æ–±—ñ–ª–∏, –ø–µ—Ä–µ–Ω–æ—Å–∏ —Ä—è–¥–∫—ñ–≤ —Ç–∞ —ñ–Ω—à—ñ –∑–∞–π–≤—ñ —Å–∏–º–≤–æ–ª–∏
                        before_clean = file_base64_clean
                        file_base64_clean = file_base64_clean.replace('\n', '').replace('\r', '').replace(' ', '').replace('\t', '')
                        if len(before_clean) != len(file_base64_clean):
                            print(f"üìã [Job {job_id}] Background: Removed whitespace. Before: {len(before_clean)}, After: {len(file_base64_clean)}")
                        
                        # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ base64url (URL-safe) –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏–π base64
                        # base64url –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î: - –∑–∞–º—ñ—Å—Ç—å +, _ –∑–∞–º—ñ—Å—Ç—å /
                        if '-' in file_base64_clean or '_' in file_base64_clean:
                            before_url = file_base64_clean
                            file_base64_clean = file_base64_clean.replace('-', '+').replace('_', '/')
                            print(f"üìã [Job {job_id}] Background: Converted base64url to standard base64. Found - or _ characters.")
                            print(f"üìã [Job {job_id}] Background: Count of -: {before_url.count('-')}, Count of _: {before_url.count('_')}")
                        
                        # –í–∏–¥–∞–ª—è—î–º–æ –∫—Ä–∞–ø–∫–∏ —Ç–∞ —ñ–Ω—à—ñ –Ω–µ–≤–∞–ª—ñ–¥–Ω—ñ —Å–∏–º–≤–æ–ª–∏ (—Ç—ñ–ª—å–∫–∏ –∑–∞–ª–∏—à–∞—î–º–æ –≤–∞–ª—ñ–¥–Ω—ñ base64 —Å–∏–º–≤–æ–ª–∏)
                        import re
                        before_invalid_removal = file_base64_clean
                        # –ó–∞–ª–∏—à–∞—î–º–æ —Ç—ñ–ª—å–∫–∏ –≤–∞–ª—ñ–¥–Ω—ñ base64 —Å–∏–º–≤–æ–ª–∏: A-Z, a-z, 0-9, +, /, =
                        file_base64_clean = re.sub(r'[^A-Za-z0-9+/=]', '', file_base64_clean)
                        if len(before_invalid_removal) != len(file_base64_clean):
                            removed_count = len(before_invalid_removal) - len(file_base64_clean)
                            removed_chars = set(before_invalid_removal) - set(file_base64_clean)
                            print(f"‚ö†Ô∏è [Job {job_id}] Background: Removed {removed_count} invalid characters: {removed_chars}")
                        
                        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —â–æ –∑–∞–ª–∏—à–∏–ª–∏—Å—è —Ç—ñ–ª—å–∫–∏ –≤–∞–ª—ñ–¥–Ω—ñ base64 —Å–∏–º–≤–æ–ª–∏
                        if not re.match(r'^[A-Za-z0-9+/=]+$', file_base64_clean):
                            # –¶–µ –Ω–µ –ø–æ–≤–∏–Ω–Ω–æ —Å—Ç–∞—Ç–∏—Å—è –ø—ñ—Å–ª—è re.sub, –∞–ª–µ –Ω–∞ –≤—Å—è–∫ –≤–∏–ø–∞–¥–æ–∫
                            invalid_chars = set(file_base64_clean) - set('ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/=')
                            print(f"‚ùå [Job {job_id}] Background: Invalid base64 characters found: {invalid_chars}")
                            print(f"üìã [Job {job_id}] Background: First 200 chars: {file_base64_clean[:200]}")
                            raise ValueError(f"Invalid base64 characters found: {invalid_chars}. Base64 should only contain A-Z, a-z, 0-9, +, /, and =")
                        
                        print(f"üìã [Job {job_id}] Background: Base64 validation passed. Length: {len(file_base64_clean)}")
                        
                        # –î–æ–¥–∞—î–º–æ padding, —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ (base64 –º–∞—î –±—É—Ç–∏ –∫—Ä–∞—Ç–Ω–∏–π 4)
                        # –ê–ª–µ —Å–ø–æ—á–∞—Ç–∫—É –≤–∏–¥–∞–ª—è—î–º–æ —ñ—Å–Ω—É—é—á—ñ padding —Å–∏–º–≤–æ–ª–∏, —â–æ–± –ø—Ä–∞–≤–∏–ª—å–Ω–æ —Ä–æ–∑—Ä–∞—Ö—É–≤–∞—Ç–∏
                        original_length = len(file_base64_clean)
                        padding_at_end = len(file_base64_clean.rstrip('='))
                        file_base64_clean = file_base64_clean.rstrip('=')
                        
                        missing_padding = len(file_base64_clean) % 4
                        if missing_padding:
                            padding_needed = 4 - missing_padding
                            file_base64_clean += '=' * padding_needed
                            print(f"üìã [Job {job_id}] Background: Added {padding_needed} padding characters (length was {len(file_base64_clean) - padding_needed}, now {len(file_base64_clean)})")
                        else:
                            print(f"üìã [Job {job_id}] Background: No padding needed (length {len(file_base64_clean)} is multiple of 4)")
                        
                        print(f"üìã [Job {job_id}] Background: Final base64 length: {len(file_base64_clean)}")
                        print(f"üìã [Job {job_id}] Background: First 100 chars: {file_base64_clean[:100]}")
                        print(f"üìã [Job {job_id}] Background: Last 50 chars: {file_base64_clean[-50:]}")
                        print(f"üìã [Job {job_id}] Background: Base64 characters breakdown:")
                        print(f"   - Letters (A-Z, a-z): {sum(1 for c in file_base64_clean if c.isalpha())}")
                        print(f"   - Digits (0-9): {sum(1 for c in file_base64_clean if c.isdigit())}")
                        print(f"   - Plus (+): {file_base64_clean.count('+')}")
                        print(f"   - Slash (/): {file_base64_clean.count('/')}")
                        print(f"   - Equals (=): {file_base64_clean.count('=')}")
                        sys.stdout.flush()
                        
                        # –î–æ–¥–∞—Ç–∫–æ–≤–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞: —è–∫—â–æ —Ü–µ –≤–∏–≥–ª—è–¥–∞—î —è–∫ —Ç–µ–∫—Å—Ç, –∞ –Ω–µ base64
                        if len(file_base64_clean) < 100 and file_base64_clean.count('+') + file_base64_clean.count('/') < 2:
                            print(f"‚ùå [Job {job_id}] Background: ERROR - This doesn't look like base64 data!")
                            print(f"   Length: {len(file_base64_clean)}, Contains: {file_base64_clean}")
                            raise ValueError(f"This doesn't look like base64-encoded audio data. Received text: '{file_base64_clean}'. Make sure you use 'Encode Media' action with Base64 format in Shortcut, not just the filename.")
                        
                        # –î–µ–∫–æ–¥—É—î–º–æ base64
                        try:
                            print(f"üíæ [Job {job_id}] Background: Attempting base64 decode...")
                            file_data = base64.b64decode(file_base64_clean, validate=True)
                            print(f"‚úÖ [Job {job_id}] Background: Base64 decode successful! Decoded size: {len(file_data)} bytes ({len(file_data) / (1024*1024):.2f} MB)")
                        except Exception as decode_error:
                            print(f"‚ùå [Job {job_id}] Background: Base64 decode error: {decode_error}")
                            print(f"üìã [Job {job_id}] Background: Base64 length: {len(file_base64_clean)}")
                            print(f"üìã [Job {job_id}] Background: First 200 chars: {file_base64_clean[:200]}")
                            print(f"üìã [Job {job_id}] Background: Last 100 chars: {file_base64_clean[-100:]}")
                            raise ValueError(f"Invalid base64 data: {str(decode_error)}")
                        sys.stdout.flush()
                        file_size = len(file_data)
                        print(f"üìä [Job {job_id}] Background: Decoded file size: {file_size / (1024*1024):.2f} MB")
                        sys.stdout.flush()
                        
                        if file_size > MAX_FILE_SIZE:
                            with jobs_lock:
                                jobs[job_id]['status'] = 'failed'
                                jobs[job_id]['error'] = f'File too large. Maximum size: {MAX_FILE_SIZE / (1024*1024):.0f} MB'
                                jobs[job_id]['code'] = 'FILE_SIZE_EXCEEDED'
                            return
                        
                        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Ñ–∞–π–ª
                        safe_filename = secure_filename(filename)
                        filepath = os.path.join(UPLOAD_FOLDER, f"{job_id}_{safe_filename}")
                        
                        print(f"üíæ [Job {job_id}] Background: Saving file to: {filepath}")
                        sys.stdout.flush()
                        
                        with open(filepath, 'wb') as f:
                            f.write(file_data)
                        
                        print(f"‚úÖ [Job {job_id}] Background: File saved, starting processing...")
                        sys.stdout.flush()
                        
                        # –ó–∞–ø—É—Å–∫–∞—î–º–æ –æ–±—Ä–æ–±–∫—É
                        process_audio_background(job_id, filepath, num_speakers, language, segment_duration, overlap, processing_mode)
                    except Exception as e:
                        print(f"‚ùå [Job {job_id}] Background: Error in decode_and_process: {e}")
                        import traceback
                        traceback.print_exc()
                        sys.stdout.flush()
                        with jobs_lock:
                            jobs[job_id]['status'] = 'failed'
                            jobs[job_id]['error'] = str(e)
                            jobs[job_id]['code'] = 'PROCESSING_ERROR'
                
                thread = threading.Thread(target=decode_and_process, daemon=True)
                thread.start()
                
                return response, 202  # 202 Accepted
                
            except Exception as json_error:
                print(f"‚ùå [Job {job_id}] Error parsing JSON: {json_error}")
                sys.stdout.flush()
                return jsonify({
                    'success': False,
                    'error': f'Invalid JSON format: {str(json_error)}',
                    'code': 'INVALID_JSON'
                }), 400
        
        else:
            # Legacy: multipart/form-data (–¥–ª—è —Å—É–º—ñ—Å–Ω–æ—Å—Ç—ñ)
            print(f"üì¶ Step 2: Using legacy multipart/form-data mode...")
            sys.stdout.flush()
            
            print(f"üìã Step 2: Content-Length: {request.content_length}")
            sys.stdout.flush()
            
            print(f"üì¶ Step 3: Accessing request.files (this may take time for large files)...")
            sys.stdout.flush()
            try:
                has_file = 'file' in request.files
                print(f"üì¶ Step 3: 'file' in request.files: {has_file}")
            except Exception as parse_error:
                print(f"‚ùå Step 3: Error parsing request.files: {parse_error}")
                raise
            sys.stdout.flush()
            
            if not has_file:
                return jsonify({
                    'success': False,
                    'error': 'No file uploaded. Use JSON with base64 for faster processing.',
                    'code': 'NO_FILE'
                }), 400
            
            file = request.files['file']
            if file.filename == '':
                return jsonify({
                    'success': False,
                    'error': 'No file selected',
                    'code': 'NO_FILE'
                }), 400
            
            if not allowed_file(file.filename):
                return jsonify({
                    'success': False,
                    'error': f'Invalid audio format. Allowed: {", ".join(ALLOWED_EXTENSIONS)}',
                    'code': 'INVALID_FORMAT'
                }), 400
            
            # –û—Ç—Ä–∏–º—É—î–º–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ (–≤—Å—ñ –º–∞—é—Ç—å –∑–Ω–∞—á–µ–Ω–Ω—è –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º)
            num_speakers = request.form.get('num_speakers', type=int) or 2  # –ó–∞–≤–∂–¥–∏ 2
            language = request.form.get('language', type=str) or 'English'  # –ó–∞–≤–∂–¥–∏ English
            if language and language.lower() == 'auto':
                language = None
            segment_duration = float(request.form.get('segment_duration', 2.5))  # –ó–∞–≤–∂–¥–∏ 2.5
            overlap = float(request.form.get('overlap', 0.4))  # –ó–∞–≤–∂–¥–∏ 0.4
            include_transcription = request.form.get('include_transcription', 'true').lower() == 'true'  # –ó–∞–≤–∂–¥–∏ True
            processing_mode = request.form.get('mode', 'fast')  # 'smart' –∞–±–æ 'fast'
            
            # –°—Ç–≤–æ—Ä—é—î–º–æ –∑–∞–≤–¥–∞–Ω–Ω—è
            with jobs_lock:
                jobs[job_id] = {
                    'status': 'pending',
                    'result': None,
                    'error': None,
                    'created_at': datetime.now(),
                    'include_transcription': include_transcription,
                    'processing_mode': processing_mode
                }
            
            # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Ñ–∞–π–ª —Ç–∞ –æ–±—Ä–æ–±–ª—è—î–º–æ
            filename = secure_filename(file.filename)
            filepath = os.path.join(UPLOAD_FOLDER, f"{job_id}_{filename}")
            file.save(filepath)
            
            file_size = os.path.getsize(filepath)
            if file_size > MAX_FILE_SIZE:
                os.remove(filepath)
                return jsonify({
                    'success': False,
                    'error': f'File too large. Maximum size: {MAX_FILE_SIZE / (1024*1024):.0f} MB',
                    'code': 'FILE_SIZE_EXCEEDED'
                }), 413
            
            # –ó–∞–ø—É—Å–∫–∞—î–º–æ –æ–±—Ä–æ–±–∫—É –≤ —Ñ–æ–Ω—ñ
            thread = threading.Thread(
                target=process_audio_background,
                args=(job_id, filepath, num_speakers, language, segment_duration, overlap, processing_mode),
                daemon=True
            )
            thread.start()
            
            response = jsonify({
                'success': True,
                'job_id': job_id,
                'status': 'pending',
                'message': 'Processing started. Use GET /api/diarize/{job_id}/status to check progress.'
            })
            response.headers.add('Access-Control-Allow-Origin', '*')
            return response, 202
    
    except Exception as e:
        print(f"‚ùå [Job {job_id}] Error creating job: {e}")
        import traceback
        traceback.print_exc()
        sys.stdout.flush()
        sys.stderr.flush()
        
        # –í–∏–¥–∞–ª—è—î–º–æ —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª —É —Ä–∞–∑—ñ –ø–æ–º–∏–ª–∫–∏
        try:
            if filepath and os.path.exists(filepath):
                os.remove(filepath)
        except:
            pass
        
        # –í–∏–¥–∞–ª—è—î–º–æ job –∑—ñ —Å–ª–æ–≤–Ω–∏–∫–∞
        with jobs_lock:
            if job_id in jobs:
                del jobs[job_id]
        
        # –î–µ—Ç–∞–ª—å–Ω–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ –ø–æ–º–∏–ª–∫—É
        error_msg = str(e)
        if "Bad Request" in error_msg or "400" in error_msg:
            error_msg = "Invalid request format. Ensure file is sent as multipart/form-data with 'file' field."
        
        response = jsonify({
            'success': False,
            'error': error_msg,
            'code': 'PROCESSING_ERROR',
            'debug_info': {
                'request_method': request.method,
                'content_type': request.content_type,
                'files_keys': list(request.files.keys()) if hasattr(request, 'files') else [],
                'form_keys': list(request.form.keys()) if hasattr(request, 'form') else []
            }
        })
        response.headers.add('Access-Control-Allow-Origin', '*')
        return response, 500


@app.route('/api/diarize/<job_id>/status', methods=['GET', 'OPTIONS'])
def get_diarize_status(job_id):
    """–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å—É –∑–∞–≤–¥–∞–Ω–Ω—è –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó"""
    if request.method == 'OPTIONS':
        response = jsonify({})
        response.headers.add('Access-Control-Allow-Origin', '*')
        response.headers.add('Access-Control-Allow-Methods', 'GET, OPTIONS')
        return response
    
    with jobs_lock:
        if job_id not in jobs:
            return jsonify({
                'success': False,
                'error': 'Job not found',
                'code': 'JOB_NOT_FOUND'
            }), 404
        
        job = jobs[job_id]
        
        if job['status'] == 'completed':
            result = job['result']
            # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ —Ç—ñ–ª—å–∫–∏ –º–µ—Ç–∞–¥–∞–Ω—ñ combined (–±–µ–∑ segments, –±–æ segments –ø–∞—Ä—Å—è—Ç—å—Å—è –∑–∞—Å–æ–±–∞–º–∏ Shortcut)
            combined = result.get('combined', {})
            combined_metadata = {
                'num_speakers': combined.get('num_speakers', 0),
                'num_segments': combined.get('num_segments', 0)
            }
            return jsonify({
                'success': True,
                'status': 'completed',
                'combined': combined_metadata
            }), 200
        elif job['status'] == 'failed':
            return jsonify({
                'success': False,
                'status': 'failed',
                'error': job.get('error', 'Unknown error'),
                'code': job.get('code', 'PROCESSING_ERROR')
            }), 200
        else:
            return jsonify({
                'success': True,
                'status': job['status'],
                'message': 'Processing in progress...'
            }), 200


def remove_filler_words(text):
    """
    –í–∏–¥–∞–ª—è—î filler words (Uh., Um.) –∑ —Ç–µ–∫—Å—Ç—É —è–∫ –æ–∫—Ä–µ–º—ñ —Å–ª–æ–≤–∞, –Ω–µ —á–∞—Å—Ç–∏–Ω–∏ —ñ–Ω—à–∏—Ö —Å–ª—ñ–≤.
    
    Args:
        text: —Ç–µ–∫—Å—Ç –¥–ª—è –æ—á–∏—â–µ–Ω–Ω—è
    
    Returns:
        –æ—á–∏—â–µ–Ω–∏–π —Ç–µ–∫—Å—Ç –±–µ–∑ filler words
    """
    import re
    # –í–∏–¥–∞–ª—è—î–º–æ "Uh." —Ç–∞ "Um." —è–∫ –æ–∫—Ä–µ–º—ñ —Å–ª–æ–≤–∞ (–∑ word boundaries)
    # –¢–∞–∫–æ–∂ –æ–±—Ä–æ–±–ª—è—î–º–æ –≤–∞—Ä—ñ–∞–Ω—Ç–∏ –∑ –ø—Ä–æ–±—ñ–ª–∞–º–∏ —Ç–∞ –ø—É–Ω–∫—Ç—É–∞—Ü—ñ—î—é
    # \b - word boundary, —â–æ–± –Ω–µ –≤–∏–¥–∞–ª—è—Ç–∏ —á–∞—Å—Ç–∏–Ω–∏ —ñ–Ω—à–∏—Ö —Å–ª—ñ–≤
    text = re.sub(r'\bUh\.\s*', '', text, flags=re.IGNORECASE)
    text = re.sub(r'\bUm\.\s*', '', text, flags=re.IGNORECASE)
    text = re.sub(r'\bUh\s+', '', text, flags=re.IGNORECASE)
    text = re.sub(r'\bUm\s+', '', text, flags=re.IGNORECASE)
    # –í–∏–¥–∞–ª—è—î–º–æ –ø–æ–¥–≤—ñ–π–Ω—ñ –ø—Ä–æ–±—ñ–ª–∏, —è–∫—ñ –º–æ–≥–ª–∏ –∑–∞–ª–∏—à–∏—Ç–∏—Å—è
    text = re.sub(r'\s+', ' ', text)
    return text.strip()


def format_dialogue_from_segments(segments):
    """
    –§–æ—Ä–º–∞—Ç—É—î —Å–µ–≥–º–µ–Ω—Ç–∏ –¥—ñ–∞–ª–æ–≥—É —É —á–∏—Ç–∞–±–µ–ª—å–Ω–∏–π —Ç–µ–∫—Å—Ç –∑ —Ç–∞–π–º—Å—Ç–µ–º–ø–∞–º–∏ —Ç–∞ —Å–ø—ñ–∫–µ—Ä–∞–º–∏.
    –§–æ—Ä–º–∞—Ç –∫–æ–∂–Ω–æ—ó —Ä–µ–ø–ª—ñ–∫–∏ –≤ –æ–¥–Ω–æ–º—É —Ä—è–¥–∫—É:
        MM:SS Speaker X: [text]
    
    –ú—ñ–∂ —Ä–µ–ø–ª—ñ–∫–∞–º–∏ - –ø–µ—Ä–µ–Ω–æ—Å —Ä—è–¥–∫–∞ (\n) –¥–ª—è –∑—Ä—É—á–Ω–æ–≥–æ —Ä–æ–∑–±–∏—Ç—Ç—è –≤ Shortcut.
    
    Args:
        segments: —Å–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ [{'speaker': int, 'start': float, 'end': float, 'text': str}]
    
    Returns:
        formatted_text: –≤—ñ–¥—Ñ–æ—Ä–º–∞—Ç–æ–≤–∞–Ω–∏–π –¥—ñ–∞–ª–æ–≥, –¥–µ –∫–æ–∂–Ω–∞ —Ä–µ–ø–ª—ñ–∫–∞ –≤ –æ–¥–Ω–æ–º—É —Ä—è–¥–∫—É
    """
    if not segments:
        return "Error: No dialogue segments found"
    
    formatted_replicas = []
    
    for seg in segments:
        # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ —á–∞—Å –∑ —Å–µ–∫—É–Ω–¥ —É MM:SS
        start_time = seg.get('start', 0)
        minutes = int(start_time // 60)
        seconds = int(start_time % 60)
        time_str = f"{minutes:02d}:{seconds:02d}"
        
        # –û—Ç—Ä–∏–º—É—î–º–æ —Å–ø—ñ–∫–µ—Ä–∞ —Ç–∞ —Ç–µ–∫—Å—Ç
        speaker = seg.get('speaker', 0)
        text = seg.get('text', '').strip()
        
        if not text:
            continue
        
        # –í–∏–¥–∞–ª—è—î–º–æ filler words (Uh., Um.) –ø–µ—Ä–µ–¥ —Ñ–æ—Ä–º–∞—Ç—É–≤–∞–Ω–Ω—è–º
        text = remove_filler_words(text)
        
        if not text:  # –Ø–∫—â–æ –ø—ñ—Å–ª—è –≤–∏–¥–∞–ª–µ–Ω–Ω—è filler words —Ç–µ–∫—Å—Ç —Å—Ç–∞–≤ –ø–æ—Ä–æ–∂–Ω—ñ–º
            continue
        
        # –§–æ—Ä–º–∞—Ç—É—î–º–æ –æ–¥–Ω—É —Ä–µ–ø–ª—ñ–∫—É –≤ –æ–¥–Ω–æ–º—É —Ä—è–¥–∫—É: MM:SS Speaker X: [text]
        replica = f"{time_str} Speaker {speaker}: {text}"
        formatted_replicas.append(replica)
    
    # –û–±'—î–¥–Ω—É—î–º–æ –≤—Å—ñ —Ä–µ–ø–ª—ñ–∫–∏ –ø–µ—Ä–µ–Ω–æ—Å–æ–º —Ä—è–¥–∫–∞ (–¥–ª—è –∑—Ä—É—á–Ω–æ–≥–æ —Ä–æ–∑–±–∏—Ç—Ç—è –≤ Shortcut)
    return "\n".join(formatted_replicas)


def format_single_speaker_files_markdown(all_speakers_segments, original_diarization_segments=None):
    """
    –§–æ—Ä–º–∞—Ç—É—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –æ–±—Ä–æ–±–∫–∏ –æ–¥–Ω–æ–≥–æ–ª–æ—Å–∏—Ö —Ñ–∞–π–ª—ñ–≤ —É Markdown —Ñ–æ—Ä–º–∞—Ç.
    –°—Ç–≤–æ—Ä—é—î –æ–∫—Ä–µ–º—ñ –∫–ª—é—á—ñ –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ —Ñ–∞–π–ª—É —Ç–∞ —Å–ø—ñ–∫–µ—Ä–∞.
    
    Args:
        all_speakers_segments: dict {speaker_id: [segments]} - —Å–µ–≥–º–µ–Ω—Ç–∏ –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ –∑ –æ–¥–Ω–æ–≥–æ–ª–æ—Å–∏—Ö —Ñ–∞–π–ª—ñ–≤
        original_diarization_segments: list - –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω—ñ segments –∑ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó (–º—ñ—Å—Ç–∏—Ç—å —Ä–µ–ø–ª—ñ–∫–∏ –æ–±–æ—Ö —Å–ø—ñ–∫–µ—Ä—ñ–≤)
    
    Returns:
        dict: {
            'File1Speaker0': markdown_text –¥–ª—è —Å–ø—ñ–∫–µ—Ä–∞ 0 –∑ —Ñ–∞–π–ª—É 1,
            'File1Speaker1': markdown_text –¥–ª—è —Å–ø—ñ–∫–µ—Ä–∞ 1 –∑ —Ñ–∞–π–ª—É 1,
            'File2Speaker0': markdown_text –¥–ª—è —Å–ø—ñ–∫–µ—Ä–∞ 0 –∑ —Ñ–∞–π–ª—É 2,
            'File2Speaker1': markdown_text –¥–ª—è —Å–ø—ñ–∫–µ—Ä–∞ 1 –∑ —Ñ–∞–π–ª—É 2
        }
    """
    result = {}
    
    # –í–ò–ö–û–†–ò–°–¢–û–í–£–Ñ–ú–û all_speakers_segments (–≤—ñ–¥—Ñ—ñ–ª—å—Ç—Ä–æ–≤–∞–Ω—ñ —Ä–µ–ø–ª—ñ–∫–∏ –∑ –æ–¥–Ω–æ–≥–æ–ª–æ—Å–∏—Ö —Ñ–∞–π–ª—ñ–≤)
    # –∑–∞–º—ñ—Å—Ç—å original_diarization_segments, —â–æ–± –ø–æ–∫–∞–∑–∞—Ç–∏ —Ç—ñ–ª—å–∫–∏ —Ä–µ–ø–ª—ñ–∫–∏, —è–∫—ñ –∑–∞–ª–∏—à–∏–ª–∏—Å—è –ø—ñ—Å–ª—è —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó
    
    # –ù–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ –Ω—É–º–µ—Ä–∞—Ü—ñ—é —Å–ø—ñ–∫–µ—Ä—ñ–≤ –¥–æ 0, 1
    unique_speakers = sorted(set(all_speakers_segments.keys()))
    speaker_mapping = {old_id: new_id for new_id, old_id in enumerate(unique_speakers)}
    
    # –°—Ç–≤–æ—Ä—é—î–º–æ –º–∞–ø—ñ–Ω–≥: —è–∫–∏–π —Å–ø—ñ–∫–µ—Ä –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î —è–∫–æ–º—É —Ñ–∞–π–ª—É
    # File1 = –ø–µ—Ä—à–∏–π —Å–ø—ñ–∫–µ—Ä (–∑–∞ –ø–æ—Ä—è–¥–∫–æ–º), File2 = –¥—Ä—É–≥–∏–π —Å–ø—ñ–∫–µ—Ä
    file_to_speaker = {}
    for idx, speaker_id in enumerate(unique_speakers, start=1):
        file_to_speaker[idx] = speaker_id  # File1 -> speaker_id, File2 -> speaker_id
    
    # –õ–æ–≥—É—î–º–æ –¥–ª—è –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
    print(f"üîç [format_markdown] Input data:")
    print(f"   - all_speakers_segments keys: {list(all_speakers_segments.keys())}")
    for speaker_id, segments in all_speakers_segments.items():
        print(f"   - Speaker {speaker_id}: {len(segments)} segments")
        if segments:
            print(f"     First segment: speaker={segments[0].get('speaker')}, text={segments[0].get('text', '')[:50]}")
    print(f"   - speaker_mapping: {speaker_mapping}")
    print(f"   - file_to_speaker: {file_to_speaker}")
    sys.stdout.flush()
    
    # –ó–∞–≤–∂–¥–∏ —Å—Ç–≤–æ—Ä—é—î–º–æ 4 –∫–ª—é—á—ñ: File1Speaker0, File1Speaker1, File2Speaker0, File2Speaker1
    for file_idx in range(1, 3):  # File1 —ñ File2
        print(f"üîç [format_markdown] File{file_idx}: processing...")
        sys.stdout.flush()
        
        # –î–ª—è –∫–æ–∂–Ω–æ–≥–æ –Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ (0 —ñ 1) —Å—Ç–≤–æ—Ä—é—î–º–æ –∫–ª—é—á
        for normalized_speaker_id in range(2):  # –ó–∞–≤–∂–¥–∏ Speaker 0 —ñ 1
            key = f'File{file_idx}Speaker{normalized_speaker_id}'
            markdown_lines = []
            
            # –î–æ–¥–∞—î–º–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫
            markdown_lines.append(f"# –†–µ–ø–ª—ñ–∫–∏ —Å–ø—ñ–∫–µ—Ä–∞ {normalized_speaker_id}")
            markdown_lines.append("")  # –ü–æ—Ä–æ–∂–Ω—ñ–π —Ä—è–¥–æ–∫ –ø—ñ—Å–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–∞
            
            # –í–ò–ü–†–ê–í–õ–ï–ù–ù–Ø: –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —î —Å–ø—ñ–∫–µ—Ä –∑ normalized_speaker_id –≤ all_speakers_segments
            # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π ID —Å–ø—ñ–∫–µ—Ä–∞, —è–∫–∏–π –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î normalized_speaker_id
            original_speaker_id = None
            for orig_id, norm_id in speaker_mapping.items():
                if norm_id == normalized_speaker_id:
                    original_speaker_id = orig_id
                    break
            
            # –Ø–∫—â–æ –∑–Ω–∞–π—à–ª–∏ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π ID —Å–ø—ñ–∫–µ—Ä–∞, –ø–æ–∫–∞–∑—É—î–º–æ –π–æ–≥–æ —Ä–µ–ø–ª—ñ–∫–∏
            if original_speaker_id is not None and original_speaker_id in all_speakers_segments:
                file_segments = all_speakers_segments[original_speaker_id]
                if file_segments:
                    for seg in file_segments:
                        # –§–æ—Ä–º–∞—Ç—É—î–º–æ —á–∞—Å
                        start_time = seg.get('start', 0)
                        minutes = int(start_time // 60)
                        seconds = int(start_time % 60)
                        time_str = f"{minutes:02d}:{seconds:02d}"
                        
                        # –î–æ–¥–∞—î–º–æ —Ä–µ–ø–ª—ñ–∫—É
                        text = seg.get('text', '').strip()
                        if text:
                            # –í–∏–¥–∞–ª—è—î–º–æ filler words (Uh., Um.) –ø–µ—Ä–µ–¥ –¥–æ–¥–∞–≤–∞–Ω–Ω—è–º
                            text = remove_filler_words(text)
                            if text:  # –î–æ–¥–∞—î–º–æ —Ç—ñ–ª—å–∫–∏ —è–∫—â–æ –ø—ñ—Å–ª—è –æ—á–∏—â–µ–Ω–Ω—è —Ç–µ–∫—Å—Ç –Ω–µ –ø–æ—Ä–æ–∂–Ω—ñ–π
                                markdown_lines.append(f"{time_str} Speaker {normalized_speaker_id}: {text}")
                else:
                    markdown_lines.append("(–Ω–µ–º–∞—î —Ä–µ–ø–ª—ñ–∫)")
            else:
                # –°–ø—ñ–∫–µ—Ä –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ - –ø–æ–∫–∞–∑—É—î–º–æ "(–Ω–µ–º–∞—î —Ä–µ–ø–ª—ñ–∫)"
                markdown_lines.append("(–Ω–µ–º–∞—î —Ä–µ–ø–ª—ñ–∫)")
            
            result[key] = "\n".join(markdown_lines)
            print(f"üîç [format_markdown] {key}: {len(result[key])} chars, original_speaker_id={original_speaker_id}")
            sys.stdout.flush()
    
    return result


@app.route('/api/diarize/<job_id>/formatted', methods=['GET', 'OPTIONS'])
def get_diarize_formatted(job_id):
    """–û—Ç—Ä–∏–º—É—î –≤—ñ–¥—Ñ–æ—Ä–º–∞—Ç–æ–≤–∞–Ω–∏–π –¥—ñ–∞–ª–æ–≥ —É —á–∏—Ç–∞–±–µ–ª—å–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—ñ"""
    if request.method == 'OPTIONS':
        response = jsonify({})
        response.headers.add('Access-Control-Allow-Origin', '*')
        response.headers.add('Access-Control-Allow-Methods', 'GET, OPTIONS')
        return response
    
    with jobs_lock:
        if job_id not in jobs:
            return jsonify({
                'success': False,
                'error': 'Job not found',
                'code': 'JOB_NOT_FOUND'
            }), 404
        
        job = jobs[job_id]
        
        if job['status'] == 'completed':
            result = job.get('result', {})
            combined = result.get('combined', {})
            segments = combined.get('segments', [])
            
            if not segments:
                return jsonify({
                    'success': False,
                    'error': 'No dialogue segments found in result',
                    'code': 'NO_SEGMENTS'
                }), 200
            
            # –§–æ—Ä–º–∞—Ç—É—î–º–æ –¥—ñ–∞–ª–æ–≥
            formatted_dialogue = format_dialogue_from_segments(segments)
            
            response = jsonify({
                'success': True,
                'status': 'completed',
                'formatted_dialogue': formatted_dialogue
            })
            response.headers.add('Access-Control-Allow-Origin', '*')
            return response, 200
        elif job['status'] == 'failed':
            response = jsonify({
                'success': False,
                'status': 'failed',
                'error': job.get('error', 'Unknown error'),
                'code': job.get('code', 'PROCESSING_ERROR')
            })
            response.headers.add('Access-Control-Allow-Origin', '*')
            return response, 200
        else:
            response = jsonify({
                'success': True,
                'status': job['status'],
                'message': 'Processing in progress...'
            })
            response.headers.add('Access-Control-Allow-Origin', '*')
            return response, 200


@app.route('/process', methods=['POST', 'OPTIONS'])
def process_audio():
    """
    –û—Å–Ω–æ–≤–Ω–∏–π –µ–Ω–¥–ø–æ—ñ–Ω—Ç –¥–ª—è iOS Shortcuts - –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞ –æ–±—Ä–æ–±–∫–∞.
    –ü–æ–≤–µ—Ä—Ç–∞—î job_id –æ–¥—Ä–∞–∑—É, –æ–±—Ä–æ–±–∫–∞ –≤–∏–∫–æ–Ω—É—î—Ç—å—Å—è –≤ —Ñ–æ–Ω—ñ.
    """
    # –û–±—Ä–æ–±–∫–∞ OPTIONS –¥–ª—è preflight –∑–∞–ø–∏—Ç—ñ–≤ (CORS)
    if request.method == 'OPTIONS':
        print("‚úÖ OPTIONS preflight request received from", request.remote_addr)
        response = jsonify({})
        response.headers.add('Access-Control-Allow-Origin', '*')
        response.headers.add('Access-Control-Allow-Headers', 'Content-Type')
        response.headers.add('Access-Control-Allow-Methods', 'POST, OPTIONS')
        return response
    
    print(f"üì• POST /process request received from {request.remote_addr}")
    print(f"üìã Headers: {dict(request.headers)}")
    print(f"üì¶ Files in request: {list(request.files.keys())}")
    print(f"üìù Form data keys: {list(request.form.keys())}")
    
    # –ü—Ä–∏–º—É—Å–æ–≤–æ —Å–∫–∏–¥–∞—î–º–æ –±—É—Ñ–µ—Ä –≤–∏–≤–æ–¥—É
    import sys
    sys.stdout.flush()
    
    filepath = None
    job_id = str(uuid.uuid4())
    
    try:
        print(f"üîµ [Job {job_id}] Starting request processing...")
        sys.stdout.flush()
        
        # –í–∞–ª—ñ–¥–∞—Ü—ñ—è —Ñ–∞–π–ª—É
        if 'file' not in request.files:
            return jsonify({
                'success': False,
                'error': 'No file uploaded',
                'code': 'NO_FILE'
            }), 400
        
        file = request.files['file']
        if file.filename == '':
            return jsonify({
                'success': False,
                'error': 'No file selected',
                'code': 'NO_FILE'
            }), 400
        
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç—É —Ñ–∞–π–ª—É
        if not allowed_file(file.filename):
            return jsonify({
                'success': False,
                'error': f'Invalid audio format. Allowed: {", ".join(ALLOWED_EXTENSIONS)}',
                'code': 'INVALID_FORMAT'
            }), 400
        
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Ñ–∞–π–ª —Ç–∏–º—á–∞—Å–æ–≤–æ –∑ —É–Ω—ñ–∫–∞–ª—å–Ω–∏–º —ñ–º'—è–º
        filename = secure_filename(file.filename)
        filepath = os.path.join(UPLOAD_FOLDER, f"{job_id}_{filename}")
        file.save(filepath)
        
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ä–æ–∑–º—ñ—Ä—É —Ñ–∞–π–ª—É
        file_size = os.path.getsize(filepath)
        if file_size > MAX_FILE_SIZE:
            os.remove(filepath)
            return jsonify({
                'success': False,
                'error': f'File too large. Maximum size: {MAX_FILE_SIZE / (1024*1024):.0f} MB',
                'code': 'FILE_SIZE_EXCEEDED'
            }), 413
        
        # –û—Ç—Ä–∏–º—É—î–º–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
        num_speakers = request.form.get('num_speakers', type=int)
        language = request.form.get('language', type=str) or None
        if language and language.lower() == 'auto':
            language = None
        
        # –ó–±—ñ–ª—å—à—É—î–º–æ –º—ñ–Ω—ñ–º–∞–ª—å–Ω—É –¥–æ–≤–∂–∏–Ω—É —Å–µ–≥–º–µ–Ω—Ç–∞ –¥–ª—è —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ
        segment_duration = float(request.form.get('segment_duration', 1.5))  # –Ø–∫ –≤ demo2 –¥–ª—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ
        overlap = float(request.form.get('overlap', 0.5))
        
        print(f"üìÅ [Job {job_id}] File saved: {filename} ({file_size / (1024*1024):.2f} MB)")
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ –∑–∞–≤–¥–∞–Ω–Ω—è
        with jobs_lock:
            jobs[job_id] = {
                'status': 'pending',
                'result': None,
                'error': None,
                'created_at': datetime.now()
            }
        
        # –ó–∞–ø—É—Å–∫–∞—î–º–æ –æ–±—Ä–æ–±–∫—É –≤ —Ñ–æ–Ω—ñ
        thread = threading.Thread(
            target=process_audio_background,
            args=(job_id, filepath, num_speakers, language, segment_duration, overlap),
            daemon=True
        )
        thread.start()
        
        print(f"‚úÖ [Job {job_id}] Job created, processing started in background")
        sys.stdout.flush()
        
        # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ job_id –æ–¥—Ä–∞–∑—É
        return jsonify({
            'success': True,
            'job_id': job_id,
            'status': 'pending',
            'message': 'Processing started. Use GET /process/{job_id}/status to check progress.'
        }), 202  # 202 Accepted
    
    except Exception as e:
        print(f"‚ùå [Job {job_id}] Error creating job: {e}")
        import traceback
        import sys
        traceback.print_exc()
        sys.stdout.flush()
        sys.stderr.flush()
        
        # –í–∏–¥–∞–ª—è—î–º–æ —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª —É —Ä–∞–∑—ñ –ø–æ–º–∏–ª–∫–∏
        try:
            if filepath and os.path.exists(filepath):
                os.remove(filepath)
        except:
            pass
        
        # –í–∏–¥–∞–ª—è—î–º–æ job –∑—ñ —Å–ª–æ–≤–Ω–∏–∫–∞
        with jobs_lock:
            if job_id in jobs:
                del jobs[job_id]
        
        return jsonify({
            'success': False,
            'error': str(e),
            'code': 'PROCESSING_ERROR'
        }), 500


@app.route('/process/<job_id>/status', methods=['GET', 'OPTIONS'])
def get_job_status(job_id):
    """–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å—É –∑–∞–≤–¥–∞–Ω–Ω—è"""
    if request.method == 'OPTIONS':
        response = jsonify({})
        response.headers.add('Access-Control-Allow-Origin', '*')
        response.headers.add('Access-Control-Allow-Methods', 'GET, OPTIONS')
        return response
    
    with jobs_lock:
        if job_id not in jobs:
            return jsonify({
                'success': False,
                'error': 'Job not found',
                'code': 'JOB_NOT_FOUND'
            }), 404
        
        job = jobs[job_id]
        response_data = {
            'success': True,
            'job_id': job_id,
            'status': job['status']
        }
        
        if job['status'] == 'completed':
            response_data['result'] = job['result']
        elif job['status'] == 'failed':
            response_data['error'] = job['error']
            response_data['code'] = 'PROCESSING_ERROR'
        
        return jsonify(response_data)


@app.route('/process/<job_id>/result', methods=['GET', 'OPTIONS'])
def get_job_result(job_id):
    """–û—Ç—Ä–∏–º–∞–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É –∑–∞–≤–¥–∞–Ω–Ω—è (alias –¥–ª—è status)"""
    return get_job_status(job_id)


def clean_base64_string(base64_data):
    """
    –û—á–∏—â–∞—î base64 —Ä—è–¥–æ–∫ –≤—ñ–¥ data URI –ø—Ä–µ—Ñ—ñ–∫—Å—ñ–≤, –ø—Ä–æ–±—ñ–ª—ñ–≤, –∫–æ–Ω–≤–µ—Ä—Ç—É—î base64url –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏–π base64,
    –≤–∏–¥–∞–ª—è—î –Ω–µ–≤–∞–ª—ñ–¥–Ω—ñ —Å–∏–º–≤–æ–ª–∏ —Ç–∞ –¥–æ–¥–∞—î –ø—Ä–∞–≤–∏–ª—å–Ω–µ padding.
    """
    import re
    import base64
    
    # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ –≤ —Ä—è–¥–æ–∫ —Ç–∞ –æ—á–∏—â–∞—î–º–æ
    base64_clean = str(base64_data).strip()
    
    # –í–∏–¥–∞–ª—è—î–º–æ data URI –ø—Ä–µ—Ñ—ñ–∫—Å (—è–∫—â–æ —î), –Ω–∞–ø—Ä–∏–∫–ª–∞–¥: "data:audio/m4a;base64,"
    if ',' in base64_clean:
        base64_clean = base64_clean.split(',', 1)[1]
    
    # –í–∏–¥–∞–ª—è—î–º–æ –≤—Å—ñ –ø—Ä–æ–±—ñ–ª–∏, –ø–µ—Ä–µ–Ω–æ—Å–∏ —Ä—è–¥–∫—ñ–≤ —Ç–∞ —ñ–Ω—à—ñ –∑–∞–π–≤—ñ —Å–∏–º–≤–æ–ª–∏
    base64_clean = base64_clean.replace('\n', '').replace('\r', '').replace(' ', '').replace('\t', '')
    
    # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ base64url (URL-safe) –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏–π base64
    if '-' in base64_clean or '_' in base64_clean:
        base64_clean = base64_clean.replace('-', '+').replace('_', '/')
    
    # –í–∏–¥–∞–ª—è—î–º–æ –∫—Ä–∞–ø–∫–∏ —Ç–∞ —ñ–Ω—à—ñ –Ω–µ–≤–∞–ª—ñ–¥–Ω—ñ —Å–∏–º–≤–æ–ª–∏ (—Ç—ñ–ª—å–∫–∏ –∑–∞–ª–∏—à–∞—î–º–æ –≤–∞–ª—ñ–¥–Ω—ñ base64 —Å–∏–º–≤–æ–ª–∏)
    base64_clean = re.sub(r'[^A-Za-z0-9+/=]', '', base64_clean)
    
    # –î–æ–¥–∞—î–º–æ padding, —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ (base64 –º–∞—î –±—É—Ç–∏ –∫—Ä–∞—Ç–Ω–∏–π 4)
    base64_clean = base64_clean.rstrip('=')
    missing_padding = len(base64_clean) % 4
    if missing_padding:
        base64_clean += '=' * (4 - missing_padding)
    
    return base64_clean


def determine_main_speaker_from_segments(combined_segments, duration=None):
    """
    –í–∏–∑–Ω–∞—á–∞—î –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ –Ω–∞ –æ—Å–Ω–æ–≤—ñ —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó.
    –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î —Ç—É —Å–∞–º—É –ª–æ–≥—ñ–∫—É, —â–æ —ñ –≤ enhance_main_speaker_audio:
    - –ö—Ä–∏—Ç–µ—Ä—ñ–π 1: –ö—ñ–ª—å–∫—ñ—Å—Ç—å —Å–ª—ñ–≤ (–Ω–∞–π—Ç–æ—á–Ω—ñ—à–∏–π –ø–æ–∫–∞–∑–Ω–∏–∫)
    - –ö—Ä–∏—Ç–µ—Ä—ñ–π 2: –¢—Ä–∏–≤–∞–ª—ñ—Å—Ç—å (—è–∫—â–æ —Ä—ñ–∑–Ω–∏—Ü—è –≤ —Å–ª–æ–≤–∞—Ö <10%)
    
    Args:
        combined_segments: —Å–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ –∑ –ø–æ–ª—è–º–∏ 'speaker', 'start', 'end', 'text'
        duration: –∑–∞–≥–∞–ª—å–Ω–∞ —Ç—Ä–∏–≤–∞–ª—ñ—Å—Ç—å –∞—É–¥—ñ–æ (–æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ, –¥–ª—è –ª–æ–≥—É–≤–∞–Ω–Ω—è)
    
    Returns:
        main_speaker: –Ω–æ–º–µ—Ä –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
        speaker_stats: —Å–ª–æ–≤–Ω–∏–∫ –∑—ñ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ—é –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
    """
    import sys
    
    # –ü—ñ–¥—Ä–∞—Ö–æ–≤—É—î–º–æ —Ç—Ä–∏–≤–∞–ª—ñ—Å—Ç—å –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
    speaker_durations = {}
    speaker_word_counts = {}
    speaker_first_segment = {}  # –ß–∞—Å –ø–µ—Ä—à–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞ –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
    
    for seg in combined_segments:
        speaker = seg['speaker']
        duration_seg = seg['end'] - seg['start']
        word_count = len(seg.get('text', '').split())
        
        if speaker not in speaker_durations:
            speaker_durations[speaker] = 0
            speaker_word_counts[speaker] = 0
            speaker_first_segment[speaker] = seg['start']
        
        speaker_durations[speaker] += duration_seg
        speaker_word_counts[speaker] += word_count
        
        # –û–Ω–æ–≤–ª—é—î–º–æ —á–∞—Å –ø–µ—Ä—à–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞, —è–∫—â–æ –∑–Ω–∞–π—à–ª–∏ —Ä–∞–Ω—ñ—à–∏–π
        if seg['start'] < speaker_first_segment[speaker]:
            speaker_first_segment[speaker] = seg['start']
    
    # –ö–†–ò–¢–ò–ß–ù–û: –í–∏–∑–Ω–∞—á–∞—î–º–æ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
    # –ö—Ä–∏—Ç–µ—Ä—ñ–π 1: –ö—ñ–ª—å–∫—ñ—Å—Ç—å —Å–ª—ñ–≤ (–Ω–∞–π—Ç–æ—á–Ω—ñ—à–∏–π –ø–æ–∫–∞–∑–Ω–∏–∫)
    # –ö—Ä–∏—Ç–µ—Ä—ñ–π 2: –¢—Ä–∏–≤–∞–ª—ñ—Å—Ç—å (—è–∫—â–æ —Ä—ñ–∑–Ω–∏—Ü—è –≤ —Å–ª–æ–≤–∞—Ö <10%)
    main_speaker_by_words = max(speaker_word_counts.items(), key=lambda x: x[1])[0]
    main_word_count = speaker_word_counts[main_speaker_by_words]
    
    main_speaker_by_duration = max(speaker_durations.items(), key=lambda x: x[1])[0]
    main_duration = speaker_durations[main_speaker_by_duration]
    
    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —Ä—ñ–∑–Ω–∏—Ü—è –≤ —Å–ª–æ–≤–∞—Ö –Ω–µ–≤–µ–ª–∏–∫–∞ (<10%)
    if len(speaker_word_counts) > 1:
        sorted_word_counts = sorted(speaker_word_counts.items(), key=lambda x: x[1], reverse=True)
        first_word_count = sorted_word_counts[0][1]
        second_word_count = sorted_word_counts[1][1] if len(sorted_word_counts) > 1 else 0
        total_words = sum(speaker_word_counts.values())
        word_diff_ratio = (first_word_count - second_word_count) / total_words if total_words > 0 else 1.0
        
        # –Ø–∫—â–æ —Ä—ñ–∑–Ω–∏—Ü—è –≤ —Å–ª–æ–≤–∞—Ö <10%, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ç—Ä–∏–≤–∞–ª—ñ—Å—Ç—å
        if word_diff_ratio < 0.10:
            print(f"‚ö†Ô∏è  Word count difference is small ({word_diff_ratio*100:.1f}%), using duration-based selection")
            main_speaker = main_speaker_by_duration
        else:
            # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Å–ª—ñ–≤ (–Ω–∞–π—Ç–æ—á–Ω—ñ—à–∏–π –ø–æ–∫–∞–∑–Ω–∏–∫)
            main_speaker = main_speaker_by_words
    else:
        main_speaker = main_speaker_by_words
    
    # –§–æ—Ä–º—É—î–º–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    speaker_stats = {}
    for speaker in sorted(speaker_durations.keys()):
        dur = speaker_durations[speaker]
        words = speaker_word_counts[speaker]
        speaker_stats[speaker] = {
            'duration': dur,
            'word_count': words,
            'first_segment_time': speaker_first_segment[speaker]
        }
    if duration:
        print(f"üìä Speaker statistics from combined transcription:")
        for speaker in sorted(speaker_durations.keys()):
            dur = speaker_durations[speaker]
            words = speaker_word_counts[speaker]
            print(f"   Speaker {speaker}: {dur:.2f}s ({dur/duration*100:.1f}%), {words} words{' üëë' if speaker == main_speaker else ''}")
        print(f"‚úÖ Main speaker determined: {main_speaker} ({main_word_count} words, {main_duration:.2f}s, {main_duration/duration*100:.1f}%)")
        sys.stdout.flush()
    
    return main_speaker, speaker_stats


def format_speaker_dialogue(segments, main_speaker):
    """
    –§–æ—Ä–º–∞—Ç—É—î —Å–µ–≥–º–µ–Ω—Ç–∏ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ —è–∫ –¥—ñ–∞–ª–æ–≥ –∑ –æ–¥–Ω–∏–º —Å–ø—ñ–∫–µ—Ä–æ–º.
    –§–æ—Ä–º–∞—Ç: –¢–∞–π–º—Å—Ç–µ–º–ø, —Å–ø—ñ–∫–µ—Ä –Ω–æ–º–µ—Ä, —Ä–µ–ø–ª—ñ–∫–∞
    
    Args:
        segments: —Å–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ –∑ –ø–æ–ª—è–º–∏ 'speaker', 'start', 'end', 'text'
        main_speaker: –Ω–æ–º–µ—Ä –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
    
    Returns:
        formatted_lines: —Å–ø–∏—Å–æ–∫ –≤—ñ–¥—Ñ–æ—Ä–º–∞—Ç–æ–≤–∞–Ω–∏—Ö —Ä—è–¥–∫—ñ–≤
    """
    formatted_lines = []
    
    for seg in segments:
        if seg['speaker'] == main_speaker:
            # –§–æ—Ä–º–∞—Ç—É—î–º–æ —Ç–∞–π–º—Å—Ç–µ–º–ø —è–∫ MM:SS - MM:SS
            start_min = int(seg['start'] // 60)
            start_sec = int(seg['start'] % 60)
            end_min = int(seg['end'] // 60)
            end_sec = int(seg['end'] % 60)
            
            timestamp = f"{start_min:02d}:{start_sec:02d} - {end_min:02d}:{end_sec:02d}"
            speaker_num = seg['speaker']
            text = seg.get('text', '').strip()
            
            formatted_lines.append(f"{timestamp}, —Å–ø—ñ–∫–µ—Ä {speaker_num}, {text}")
    
    return formatted_lines


def separate_speakers_with_speechbrain(audio_path, output_dir):
    """
    –†–æ–∑–¥—ñ–ª—è—î —Å–ø—ñ–∫–µ—Ä—ñ–≤ –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é SpeechBrain SepformerSeparation.
    –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î —Ç–æ–π —Å–∞–º–∏–π –ø—ñ–¥—Ö—ñ–¥, —â–æ —ñ –≤ speechbrain_separation.py –¥–ª—è —è–∫—ñ—Å–Ω–æ—ó –Ω–∞—Ä—ñ–∑–∫–∏.
    
    Args:
        audio_path: —à–ª—è—Ö –¥–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–æ–≥–æ –∞—É–¥—ñ–æ —Ñ–∞–π–ª—É
        output_dir: –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–æ–∑–¥—ñ–ª–µ–Ω–∏—Ö —Ñ–∞–π–ª—ñ–≤
    
    Returns:
        dict: {
            'success': bool,
            'speaker_files': {speaker_id: {'path': str, 'speaker_label': str}},
            'error': str (—è–∫—â–æ –ø–æ–º–∏–ª–∫–∞)
        }
    """
    import sys
    
    try:
        # –Ü–º–ø–æ—Ä—Ç—É—î–º–æ –Ω–µ–æ–±—Ö—ñ–¥–Ω—ñ –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∏
        try:
            import pyannote_patch  # noqa: F401
            from speechbrain.inference.separation import SepformerSeparation as Separator
            import torch
            import torchaudio
        except ImportError as e:
            print(f"‚ö†Ô∏è SpeechBrain separation not available: {e}, falling back to simple extraction")
            sys.stdout.flush()
            return {'success': False, 'error': f'SpeechBrain separation not available: {e}'}
        
        # –í–∏–∑–Ω–∞—á–∞—î–º–æ device
        if torch.backends.mps.is_available():
            device = "mps"
        elif torch.cuda.is_available():
            device = "cuda"
        else:
            device = "cpu"
        
        print(f"üîÄ [SpeechBrain] Using device: {device}")
        sys.stdout.flush()
        
        # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –º–æ–¥–µ–ª—å
        cache_dir = os.path.expanduser(
            os.getenv("SPEECHBRAIN_CACHE_DIR", "~/.cache/speechbrain/sepformer-wsj02mix")
        )
        
        print(f"üì¶ [SpeechBrain] Loading sepformer-wsj02mix model...")
        sys.stdout.flush()
        
        try:
            model = Separator.from_hparams(
                source="speechbrain/sepformer-wsj02mix",
                savedir=cache_dir,
                run_opts={"device": device},
            )
            print(f"‚úÖ [SpeechBrain] Model loaded successfully")
            sys.stdout.flush()
        except Exception as e:
            print(f"‚ö†Ô∏è [SpeechBrain] Failed to load model: {e}, falling back to simple extraction")
            sys.stdout.flush()
            return {'success': False, 'error': f'Failed to load model: {e}'}
        
        # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –∞—É–¥—ñ–æ —á–µ—Ä–µ–∑ librosa (–ø—ñ–¥—Ç—Ä–∏–º—É—î –±—ñ–ª—å—à–µ —Ñ–æ—Ä–º–∞—Ç—ñ–≤, –≤–∫–ª—é—á–∞—é—á–∏ m4a)
        try:
            # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ librosa –¥–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è (–ø—ñ–¥—Ç—Ä–∏–º—É—î m4a, mp3, —Ç–æ—â–æ)
            audio_data, sample_rate = librosa.load(audio_path, sr=None, mono=False)
            
            # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ –≤ torch tensor
            if len(audio_data.shape) == 1:
                # Mono audio - –¥–æ–¥–∞—î–º–æ –≤–∏–º—ñ—Ä –∫–∞–Ω–∞–ª—É
                waveform = torch.from_numpy(audio_data).unsqueeze(0).float()
            else:
                # Multi-channel audio - shape [channels, samples]
                waveform = torch.from_numpy(audio_data).float()
            
            print(f"‚úÖ [SpeechBrain] Loaded via librosa: shape={waveform.shape}, sr={sample_rate}")
            sys.stdout.flush()
        except Exception as load_error:
            print(f"‚ùå [SpeechBrain] Audio loading failed with librosa: {load_error}")
            import traceback
            traceback.print_exc()
            sys.stdout.flush()
            return {'success': False, 'error': f'Audio loading failed: {load_error}'}
        
        # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ –≤ mono —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ
        if waveform.shape[0] > 1:
            waveform = waveform.mean(dim=0, keepdim=True)
        
        # Resample –¥–æ 8kHz (SpeechBrain –≤–∏–º–∞–≥–∞—î 8kHz)
        if sample_rate != 8000:
            print(f"üîÑ [SpeechBrain] Resampling from {sample_rate}Hz to 8000Hz")
            sys.stdout.flush()
            resampler = torchaudio.transforms.Resample(sample_rate, 8000)
            waveform = resampler(waveform)
            sample_rate = 8000
        
        total_samples = waveform.shape[1]
        
        # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è chunking (—è–∫ –≤ speechbrain_separation.py)
        max_chunk_seconds = float(os.getenv("SPEECHBRAIN_CHUNK_SECONDS", "30"))
        max_chunk_samples = int(max_chunk_seconds * sample_rate)
        max_chunk_samples = max(max_chunk_samples, sample_rate * 5)  # –º—ñ–Ω—ñ–º—É–º 5 —Å–µ–∫—É–Ω–¥
        
        print(f"üîç [SpeechBrain] Waveform shape: {waveform.shape}, chunk size: {max_chunk_samples} samples")
        sys.stdout.flush()
        
        # –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –æ–±—Ä–æ–±–∫–∏ chunk
        def separate_chunk(chunk_tensor: torch.Tensor):
            chunk_tensor = chunk_tensor.to(device)
            with torch.no_grad():
                result = model.separate_batch(chunk_tensor)
            return result.cpu()
        
        # –ó–∞–ø—É—Å–∫–∞—î–º–æ separation –∑ chunking –¥–ª—è –¥–æ–≤–≥–∏—Ö —Ñ–∞–π–ª—ñ–≤
        print(f"üîÑ [SpeechBrain] Running speaker separation...")
        sys.stdout.flush()
        
        if total_samples > max_chunk_samples:
            print(f"üì¶ [SpeechBrain] Processing in chunks (total: {total_samples} samples)")
            sys.stdout.flush()
            chunk_outputs = []
            for start in range(0, total_samples, max_chunk_samples):
                end = min(start + max_chunk_samples, total_samples)
                print(f"   üîÑ [SpeechBrain] Separating chunk {start}:{end} ({start/sample_rate:.1f}s - {end/sample_rate:.1f}s)")
                sys.stdout.flush()
                chunk = waveform[:, start:end]
                chunk_outputs.append(separate_chunk(chunk))
            est_sources = torch.cat(chunk_outputs, dim=1)
        else:
            waveform = waveform.to(device)
            est_sources = separate_chunk(waveform)
        
        # –û–±—Ä–æ–±–ª—è—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç (—è–∫ –≤ speechbrain_separation.py)
        if est_sources.dim() == 3:
            est_sources = est_sources[0]  # [time, num_speakers]
        
        if est_sources.dim() == 2:
            if est_sources.shape[0] == model.hparams.num_spks:
                # shape [num_speakers, time]
                sources_tensor = est_sources
            elif est_sources.shape[1] == model.hparams.num_spks:
                sources_tensor = est_sources.transpose(0, 1)
            else:
                raise ValueError(f"Unexpected est_sources shape: {est_sources.shape}")
        else:
            raise ValueError(f"Unsupported est_sources dimension: {est_sources.dim()}")
        
        sources_tensor = sources_tensor.cpu()
        
        num_speakers = sources_tensor.shape[0]
        print(f"‚úÖ [SpeechBrain] Found {num_speakers} speakers")
        sys.stdout.flush()
        
        # –ó–∞—Å—Ç–æ—Å–æ–≤—É—î–º–æ —Å–∏–ª—å–Ω–µ –ø—Ä–∏–≥–ª—É—à–µ–Ω–Ω—è —Å–ª–∞–±–∫–∏—Ö —Å–∏–≥–Ω–∞–ª—ñ–≤
        print(f"üîá [SpeechBrain] Applying noise gate to suppress weak signals...")
        sys.stdout.flush()
        
        def apply_noise_gate(audio_tensor, threshold=0.05, ratio=10.0, attack=0.01, release=0.1):
            """
            –ó–∞—Å—Ç–æ—Å–æ–≤—É—î noise gate –¥–ª—è –ø—Ä–∏–≥–ª—É—à–µ–Ω–Ω—è —Å–ª–∞–±–∫–∏—Ö —Å–∏–≥–Ω–∞–ª—ñ–≤.
            
            Args:
                audio_tensor: torch.Tensor [samples] –∞–±–æ [channels, samples]
                threshold: –ü–æ—Ä—ñ–≥ –µ–Ω–µ—Ä–≥—ñ—ó (0.0-1.0), –Ω–∏–∂—á–µ —è–∫–æ–≥–æ —Å–∏–≥–Ω–∞–ª –ø—Ä–∏–≥–ª—É—à—É—î—Ç—å—Å—è
                ratio: –ö–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç –ø—Ä–∏–≥–ª—É—à–µ–Ω–Ω—è (1.0 = –±–µ–∑ –∑–º—ñ–Ω, 10.0 = —Å–∏–ª—å–Ω–µ –ø—Ä–∏–≥–ª—É—à–µ–Ω–Ω—è)
                attack: –ß–∞—Å –∞—Ç–∞–∫–∏ (–≤ —Å–µ–∫—É–Ω–¥–∞—Ö)
                release: –ß–∞—Å –≤—ñ–¥–ø—É—Å–∫–∞–Ω–Ω—è (–≤ —Å–µ–∫—É–Ω–¥–∞—Ö)
            """
            # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ –≤ numpy —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ
            if isinstance(audio_tensor, torch.Tensor):
                audio_np = audio_tensor.numpy()
            else:
                audio_np = audio_tensor
            
            # –û–±—á–∏—Å–ª—é—î–º–æ –µ–Ω–µ—Ä–≥—ñ—é —Å–∏–≥–Ω–∞–ª—É (RMS)
            if len(audio_np.shape) == 1:
                # Mono
                energy = np.abs(audio_np)
            else:
                # Multi-channel - –±–µ—Ä–µ–º–æ —Å–µ—Ä–µ–¥–Ω—î
                energy = np.abs(audio_np).mean(axis=0)
            
            # –û–±—á–∏—Å–ª—é—î–º–æ RMS –≤ —Å–∫–ª—è–Ω–Ω–æ–º—É –≤—ñ–∫–Ω—ñ (–¥–ª—è –ø–ª–∞–≤–Ω–æ—Å—Ç—ñ)
            window_size = int(sample_rate * 0.05)  # 50ms –≤—ñ–∫–Ω–æ
            if window_size < 1:
                window_size = 1
            
            # –û–±—á–∏—Å–ª—é—î–º–æ RMS
            rms = np.sqrt(np.convolve(energy ** 2, np.ones(window_size) / window_size, mode='same'))
            rms_normalized = rms / (np.max(rms) + 1e-8)  # –ù–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ –¥–æ 0-1
            
            # –°—Ç–≤–æ—Ä—é—î–º–æ gate mask
            gate_mask = np.ones_like(rms_normalized)
            
            # –ó–∞—Å—Ç–æ—Å–æ–≤—É—î–º–æ threshold
            below_threshold = rms_normalized < threshold
            gate_mask[below_threshold] = 1.0 / ratio  # –°–∏–ª—å–Ω–µ –ø—Ä–∏–≥–ª—É—à–µ–Ω–Ω—è —Å–ª–∞–±–∫–∏—Ö —Å–∏–≥–Ω–∞–ª—ñ–≤
            
            # –ü–ª–∞–≤–Ω—ñ –ø–µ—Ä–µ—Ö–æ–¥–∏ (attack/release)
            attack_samples = int(sample_rate * attack)
            release_samples = int(sample_rate * release)
            
            # –ó–∞—Å—Ç–æ—Å–æ–≤—É—î–º–æ –ø–ª–∞–≤–Ω—ñ –ø–µ—Ä–µ—Ö–æ–¥–∏
            smoothed_mask = np.copy(gate_mask)
            for i in range(1, len(gate_mask)):
                if gate_mask[i] > gate_mask[i-1]:
                    # Attack - —à–≤–∏–¥–∫–æ –≤—ñ–¥–∫—Ä–∏–≤–∞—î–º–æ
                    start = max(0, i - attack_samples)
                    smoothed_mask[start:i] = np.linspace(gate_mask[i-1], gate_mask[i], i - start)
                elif gate_mask[i] < gate_mask[i-1]:
                    # Release - –ø–æ–≤—ñ–ª—å–Ω–æ –∑–∞–∫—Ä–∏–≤–∞—î–º–æ
                    end = min(len(gate_mask), i + release_samples)
                    smoothed_mask[i:end] = np.linspace(gate_mask[i], gate_mask[i-1], end - i)
            
            gate_mask = smoothed_mask
            
            # –ó–∞—Å—Ç–æ—Å–æ–≤—É—î–º–æ mask –¥–æ –∞—É–¥—ñ–æ
            if len(audio_np.shape) == 1:
                gated_audio = audio_np * gate_mask
            else:
                gated_audio = audio_np * gate_mask[np.newaxis, :]
            
            return gated_audio
        
        # –ó–∞—Å—Ç–æ—Å–æ–≤—É—î–º–æ noise gate –¥–æ –∫–æ–∂–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
        gated_sources = []
        for idx in range(num_speakers):
            source_tensor = sources_tensor[idx]
            source_np = source_tensor.squeeze().numpy()
            
            # –ó–∞—Å—Ç–æ—Å–æ–≤—É—î–º–æ noise gate –∑ —Å–∏–ª—å–Ω–∏–º –ø—Ä–∏–≥–ª—É—à–µ–Ω–Ω—è–º
            gated_audio = apply_noise_gate(
                source_np, 
                threshold=0.15,  # –ü–æ—Ä—ñ–≥ 15% –≤—ñ–¥ –º–∞–∫—Å–∏–º—É–º—É (–º–µ–Ω—à–µ –ø–µ—Ä–µ—Ä–∏–≤–∞–Ω—å –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞)
                ratio=20.0,  # –°–∏–ª—å–Ω–µ –ø—Ä–∏–≥–ª—É—à–µ–Ω–Ω—è (20:1)
                attack=0.01,  # –®–≤–∏–¥–∫–∞ –∞—Ç–∞–∫–∞
                release=0.1  # –ü–æ–≤—ñ–ª—å–Ω–µ –≤—ñ–¥–ø—É—Å–∫–∞–Ω–Ω—è
            )
            
            gated_sources.append(gated_audio)
        
        print(f"‚úÖ [SpeechBrain] Noise gate applied (threshold=0.15, ratio=20:1)")
        sys.stdout.flush()
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ output directory
        os.makedirs(output_dir, exist_ok=True)
        
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Ñ–∞–π–ª–∏ –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ (–ü–û–í–ù–Ü–°–¢–Æ —Ä–æ–∑–¥—ñ–ª–µ–Ω—ñ —Ñ–∞–π–ª–∏ –∑ –ø—Ä–∏–≥–ª—É—à–µ–Ω–Ω—è–º —Å–ª–∞–±–∫–∏—Ö —Å–∏–≥–Ω–∞–ª—ñ–≤)
        speaker_files = {}
        for idx in range(num_speakers):
            speaker_id = idx
            speaker_name = f"SPEAKER_{idx:02d}"
            output_path = os.path.join(output_dir, f"speaker_{speaker_id}.wav")
            
            gated_audio = gated_sources[idx]
            sf.write(output_path, gated_audio, sample_rate)
            
            speaker_files[speaker_id] = {
                'path': output_path,
                'speaker_label': speaker_name
            }
            
            duration = len(gated_audio) / sample_rate
            print(f"‚úÖ [SpeechBrain] Saved speaker {speaker_id} ({speaker_name}): {duration:.2f}s (FULL SEPARATED TRACK with noise gate)")
            sys.stdout.flush()
        
        return {
            'success': True,
            'speaker_files': speaker_files,
            'speaker_map': {f"SPEAKER_{i:02d}": i for i in range(num_speakers)}  # –ú–∞–ø—ñ–Ω–≥ –º—ñ–∂ labels —Ç–∞ —á–∏—Å–ª–æ–≤–∏–º–∏ ID
        }
        
    except Exception as e:
        print(f"‚ùå [SpeechBrain] Error in separation: {e}")
        import traceback
        traceback.print_exc()
        sys.stdout.flush()
        return {'success': False, 'error': str(e)}


def extract_single_speaker_audio(audio_path, speaker_segments, output_dir):
    """
    –í–∏—Ç—è–≥—É—î —Å–µ–≥–º–µ–Ω—Ç–∏ –æ–¥–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ –∑ –∞—É–¥—ñ–æ —Ñ–∞–π–ª—É —Ç–∞ –∑–±–µ—Ä—ñ–≥–∞—î —è–∫ –æ–∫—Ä–µ–º–∏–π —Ñ–∞–π–ª.
    FALLBACK –º–µ—Ç–æ–¥ - –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è, —è–∫—â–æ PyAnnote separation –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–∏–π.
    
    Args:
        audio_path: —à–ª—è—Ö –¥–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–æ–≥–æ –∞—É–¥—ñ–æ —Ñ–∞–π–ª—É
        speaker_segments: —Å–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ –¥–ª—è —Ü—å–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ [{'start': float, 'end': float}]
        output_dir: –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≤–∏—Ç—è–≥–Ω—É—Ç–æ–≥–æ —Ñ–∞–π–ª—É
    
    Returns:
        output_path: —à–ª—è—Ö –¥–æ –∑–±–µ—Ä–µ–∂–µ–Ω–æ–≥–æ –æ–¥–Ω–æ–≥–æ–ª–æ—Å–æ–≥–æ —Ñ–∞–π–ª—É
    """
    try:
        # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –∞—É–¥—ñ–æ
        audio, sr = librosa.load(audio_path, sr=None)
        duration = len(audio) / sr
        
        # –ó–±–∏—Ä–∞—î–º–æ –≤—Å—ñ —Å–µ–≥–º–µ–Ω—Ç–∏ —Å–ø—ñ–∫–µ—Ä–∞ –≤ –æ–¥–∏–Ω –º–∞—Å–∏–≤
        speaker_audio_segments = []
        for seg in speaker_segments:
            start_time = max(0, seg['start'])
            end_time = min(duration, seg['end'])
            start_sample = int(start_time * sr)
            end_sample = int(end_time * sr)
            
            if start_sample < len(audio) and end_sample <= len(audio) and start_sample < end_sample:
                segment_audio = audio[start_sample:end_sample]
                speaker_audio_segments.append(segment_audio)
        
        if not speaker_audio_segments:
            return None
        
        # –û–±'—î–¥–Ω—É—î–º–æ –≤—Å—ñ —Å–µ–≥–º–µ–Ω—Ç–∏ –≤ –æ–¥–∏–Ω –∞—É–¥—ñ–æ —Ñ–∞–π–ª
        combined_audio = np.concatenate(speaker_audio_segments)
        
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —è–∫ WAV —Ñ–∞–π–ª
        speaker_id = speaker_segments[0].get('speaker', 0)
        output_path = os.path.join(output_dir, f"speaker_{speaker_id}.wav")
        sf.write(output_path, combined_audio, sr)
        
        print(f"‚úÖ Extracted speaker {speaker_id} audio: {len(combined_audio)/sr:.2f}s ‚Üí {output_path}")
        return output_path
    
    except Exception as e:
        print(f"‚ùå Error extracting speaker audio: {e}")
        import traceback
        traceback.print_exc()
        return None


def process_single_speaker_files_sync(audio_path, diarization_segments):
    """
    –°–ò–ù–•–†–û–ù–ù–ê –æ–±—Ä–æ–±–∫–∞ –æ–¥–Ω–æ–≥–æ–ª–æ—Å–∏—Ö —Ñ–∞–π–ª—ñ–≤ (–ø–æ–≤–µ—Ä—Ç–∞—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ–¥—Ä–∞–∑—É):
    1. –†–æ–∑—Ä—ñ–∑–∞—î –∞—É–¥—ñ–æ –Ω–∞ –æ–¥–Ω–æ–≥–æ–ª–æ—Å—ñ —Ñ–∞–π–ª–∏ –ø–æ —Å–ø—ñ–∫–µ—Ä–∞—Ö
    2. –¢—Ä–∞–Ω—Å–∫—Ä–∏–±—É—î –∫–æ–∂–µ–Ω –æ–¥–Ω–æ–≥–æ–ª–æ—Å–∏–π —Ñ–∞–π–ª
    3. –í–∏–∑–Ω–∞—á–∞—î –≥–æ–ª–æ–≤–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ (—Ç–æ–π, —Ö—Ç–æ –±—ñ–ª—å—à–µ –≥–æ–≤–æ—Ä–∏–≤, –Ω–µ –æ–±—Ä–∏–≤–∫–∞–º–∏)
    4. –í–∏–¥–∞–ª—è—î –¥—Ä—É–≥–æ—Ä—è–¥–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
    """
    import sys
    
    print(f"üîÄ Step 1: Splitting audio into single-speaker files...")
    sys.stdout.flush()
    
    # –ì—Ä—É–ø—É—î–º–æ —Å–µ–≥–º–µ–Ω—Ç–∏ –ø–æ —Å–ø—ñ–∫–µ—Ä–∞—Ö
    speakers_segments = {}
    for seg in diarization_segments:
        speaker = seg.get('speaker', 0)
        if speaker not in speakers_segments:
            speakers_segments[speaker] = []
        speakers_segments[speaker].append(seg)
    
    print(f"üìä Found {len(speakers_segments)} speakers")
    sys.stdout.flush()
    
    # –°—Ç–≤–æ—Ä—é—î–º–æ —Ç–∏–º—á–∞—Å–æ–≤—É –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—é –¥–ª—è –æ–¥–Ω–æ–≥–æ–ª–æ—Å–∏—Ö —Ñ–∞–π–ª—ñ–≤
    temp_job_id = str(uuid.uuid4())
    temp_dir = os.path.join(UPLOAD_FOLDER, f"single_speakers_{temp_job_id}")
    os.makedirs(temp_dir, exist_ok=True)
    
    # –í–∏—Ç—è–≥—É—î–º–æ –æ–¥–Ω–æ–≥–æ–ª–æ—Å—ñ —Ñ–∞–π–ª–∏
    speaker_files = {}
    for speaker, segments in speakers_segments.items():
        # –°–æ—Ä—Ç—É—î–º–æ —Å–µ–≥–º–µ–Ω—Ç–∏ –∑–∞ —á–∞—Å–æ–º
        segments_sorted = sorted(segments, key=lambda x: x['start'])
        
        output_path = extract_single_speaker_audio(audio_path, segments_sorted, temp_dir)
        if output_path:
            speaker_files[speaker] = {
                'path': output_path,
                'segments': segments_sorted
            }
    
    print(f"‚úÖ Step 1 completed: {len(speaker_files)} single-speaker files created")
    sys.stdout.flush()
    
    if not speaker_files:
        raise Exception('No single-speaker files could be extracted')
    
    # –ö—Ä–æ–∫ 2: –¢—Ä–∞–Ω—Å–∫—Ä–∏–±—É—î–º–æ –∫–æ–∂–µ–Ω –æ–¥–Ω–æ–≥–æ–ª–æ—Å–∏–π —Ñ–∞–π–ª
    print(f"üìù Step 2: Transcribing single-speaker files...")
    sys.stdout.flush()
    
    speaker_transcriptions = {}
    for speaker, file_info in speaker_files.items():
        print(f"üé§ Transcribing speaker {speaker}...")
        sys.stdout.flush()
        
        transcription, transcription_segments, words = transcribe_audio(file_info['path'], transcription_provider='whisper')
        
        if transcription:
            # –û–±—á–∏—Å–ª—é—î–º–æ –∑–∞–≥–∞–ª—å–Ω—É —Ç—Ä–∏–≤–∞–ª—ñ—Å—Ç—å —Ç–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ –¥–ª—è –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –≥–æ–ª–æ–≤–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
            total_duration = sum(seg['end'] - seg['start'] for seg in file_info['segments'])
            num_segments = len(file_info['segments'])
            
            speaker_transcriptions[speaker] = {
                'transcription': transcription,
                'segments': transcription_segments,
                'words': words,
                'total_duration': total_duration,
                'num_segments': num_segments,
                'file_path': file_info['path']
            }
            print(f"‚úÖ Speaker {speaker} transcribed: {len(transcription)} chars, {total_duration:.2f}s duration")
        else:
            print(f"‚ö†Ô∏è Speaker {speaker} transcription failed or empty")
    
    if not speaker_transcriptions:
        raise Exception('No transcriptions could be generated')
    
    # –ö—Ä–æ–∫ 3: –í–∏–∑–Ω–∞—á–∞—î–º–æ –≥–æ–ª–æ–≤–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ –Ω–∞ –æ—Å–Ω–æ–≤—ñ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó
    print(f"üë§ Step 3: Determining main speaker from transcriptions...")
    sys.stdout.flush()
    
    # –§–æ—Ä–º—É—î–º–æ combined_segments –¥–ª—è –≤—Å—ñ—Ö —Å–ø—ñ–∫–µ—Ä—ñ–≤ –∑ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ–π
    all_combined_segments = []
    for speaker, info in speaker_transcriptions.items():
        transcription_segments = info['segments']
        diarization_segments = info.get('diarization_segments', [])
        
        # –û–±'—î–¥–Ω—É—î–º–æ —Ç–∞–π–º—Å—Ç–µ–º–ø–∏ –∑ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó —Ç–∞ —Ç–µ–∫—Å—Ç –∑ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó
        if diarization_segments:
            for i, diar_seg in enumerate(diarization_segments):
                transcript_text = ""
                if i < len(transcription_segments):
                    transcript_text = transcription_segments[i].get('text', '')
                elif transcription_segments:
                    transcript_text = info['transcription']
                
                all_combined_segments.append({
                    'speaker': speaker,
                    'start': diar_seg['start'],
                    'end': diar_seg['end'],
                    'text': transcript_text
                })
        else:
            for seg in transcription_segments:
                all_combined_segments.append({
                    'speaker': speaker,
                    'start': seg.get('start', 0),
                    'end': seg.get('end', 0),
                    'text': seg.get('text', '')
                })
    
    # –û–±—á–∏—Å–ª—é—î–º–æ –∑–∞–≥–∞–ª—å–Ω—É —Ç—Ä–∏–≤–∞–ª—ñ—Å—Ç—å –¥–ª—è –ª–æ–≥—É–≤–∞–Ω–Ω—è
    total_duration = max(seg['end'] for seg in all_combined_segments) if all_combined_segments else 0
    
    # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ç—É —Å–∞–º—É –ª–æ–≥—ñ–∫—É, —â–æ —ñ –≤ enhance_main_speaker_audio
    main_speaker, speaker_stats = determine_main_speaker_from_segments(all_combined_segments, duration=total_duration)
    
    # –ö—Ä–æ–∫ 4: –§–æ—Ä–º—É—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç (—Ç—ñ–ª—å–∫–∏ –≥–æ–ª–æ–≤–Ω–∏–π —Å–ø—ñ–∫–µ—Ä)
    main_speaker_info = speaker_transcriptions[main_speaker]
    
    # –§–æ—Ä–º—É—î–º–æ segments –∑ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó + —Ç–∞–π–º—Å—Ç–µ–º–ø–∏ –∑ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–æ—ó –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó
    combined_segments = []
    diarization_segments = main_speaker_info.get('diarization_segments', [])
    transcription_segments = main_speaker_info['segments']
    
    if diarization_segments:
        # –û–±'—î–¥–Ω—É—î–º–æ: –±–µ—Ä–µ–º–æ —Ç–µ–∫—Å—Ç –∑ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó, —Ç–∞–π–º—Å—Ç–µ–º–ø–∏ –∑ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó
        for i, diar_seg in enumerate(diarization_segments):
            transcript_text = ""
            if i < len(transcription_segments):
                transcript_text = transcription_segments[i].get('text', '')
            elif transcription_segments:
                transcript_text = main_speaker_info['transcription']
            
            combined_segments.append({
                'speaker': main_speaker,
                'start': diar_seg['start'],
                'end': diar_seg['end'],
                'text': transcript_text
            })
    else:
        for seg in transcription_segments:
            combined_segments.append({
                'speaker': main_speaker,
                'start': seg.get('start', 0),
                'end': seg.get('end', 0),
                'text': seg.get('text', '')
            })
    
    # –§–æ—Ä–º–∞—Ç—É—î–º–æ –¥—ñ–∞–ª–æ–≥ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
    dialogue_lines = format_speaker_dialogue(combined_segments, main_speaker)
    
    result = {
        'success': True,
        'files': [{
            'speaker': main_speaker,
            'transcript': main_speaker_info['transcription'],
            'segments': combined_segments,  # –°–µ–≥–º–µ–Ω—Ç–∏ –∑ —Ç–∞–π–º—Å—Ç–µ–º–ø–∞–º–∏ –≤—ñ–¥–Ω–æ—Å–Ω–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–æ–≥–æ —Ñ–∞–π–ª—É
            'timestamps': [{'start': seg['start'], 'end': seg['end']} for seg in combined_segments],
            'total_duration': main_speaker_info['total_duration'],
            'num_segments': main_speaker_info['num_segments'],
            'dialogue': dialogue_lines  # –í—ñ–¥—Ñ–æ—Ä–º–∞—Ç–æ–≤–∞–Ω–∏–π –¥—ñ–∞–ª–æ–≥: –¢–∞–π–º—Å—Ç–µ–º–ø, —Å–ø—ñ–∫–µ—Ä –Ω–æ–º–µ—Ä, —Ä–µ–ø–ª—ñ–∫–∞
        }],
        'main_speaker': main_speaker,
        'secondary_speakers_removed': [s for s in speaker_transcriptions.keys() if s != main_speaker]
    }
    
    print(f"‚úÖ Processing completed successfully!")
    sys.stdout.flush()
    
    # –û—á–∏—â–∞—î–º–æ —Ç–∏–º—á–∞—Å–æ–≤—ñ —Ñ–∞–π–ª–∏
    try:
        import shutil
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir)
    except Exception as e:
        print(f"‚ö†Ô∏è Could not clean up temp files: {e}")
    
    return result


def process_single_speaker_files_background(job_id, audio_path, diarization_segments):
    """
    –§–æ–Ω–æ–≤–∞ –æ–±—Ä–æ–±–∫–∞ –æ–¥–Ω–æ–≥–æ–ª–æ—Å–∏—Ö —Ñ–∞–π–ª—ñ–≤:
    1. –†–æ–∑—Ä—ñ–∑–∞—î –∞—É–¥—ñ–æ –Ω–∞ –æ–¥–Ω–æ–≥–æ–ª–æ—Å—ñ —Ñ–∞–π–ª–∏ –ø–æ —Å–ø—ñ–∫–µ—Ä–∞—Ö
    2. –¢—Ä–∞–Ω—Å–∫—Ä–∏–±—É—î –∫–æ–∂–µ–Ω –æ–¥–Ω–æ–≥–æ–ª–æ—Å–∏–π —Ñ–∞–π–ª
    3. –í–∏–∑–Ω–∞—á–∞—î –≥–æ–ª–æ–≤–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ (—Ç–æ–π, —Ö—Ç–æ –±—ñ–ª—å—à–µ –≥–æ–≤–æ—Ä–∏–≤, –Ω–µ –æ–±—Ä–∏–≤–∫–∞–º–∏)
    4. –ü–æ–≤–µ—Ä—Ç–∞—î –≤—Å—ñ —Å–µ–≥–º–µ–Ω—Ç–∏ –∑ –ø—Ä–∞–≤–∏–ª—å–Ω–∏–º–∏ —Ç–∞–π–º—Å—Ç–µ–º–ø–∞–º–∏ –¥–ª—è –ø–æ–¥–∞–ª—å—à–æ—ó –æ–±—Ä–æ–±–∫–∏ –≤ —à–æ—Ä—Ç–∫–∞—Ç–∞—Ö
    
    Args:
        job_id: ID –∑–∞–≤–¥–∞–Ω–Ω—è
        audio_path: —à–ª—è—Ö –¥–æ –∞—É–¥—ñ–æ —Ñ–∞–π–ª—É
        diarization_segments: —Å–µ–≥–º–µ–Ω—Ç–∏ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó
    """
    import sys
    
    # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω—ñ segments –î–û –±—É–¥—å-—è–∫–æ—ó –æ–±—Ä–æ–±–∫–∏ (–≥–ª–∏–±–æ–∫–∞ –∫–æ–ø—ñ—è)
    import copy
    original_diarization_segments = copy.deepcopy(diarization_segments) if diarization_segments else []
    
    try:
        with jobs_lock:
            jobs[job_id]['status'] = 'processing'
        
        print(f"üîÄ [Job {job_id}] Step 1: Splitting audio into single-speaker files using SpeechBrain separation...")
        sys.stdout.flush()
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ —Ç–∏–º—á–∞—Å–æ–≤—É –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—é –¥–ª—è –æ–¥–Ω–æ–≥–æ–ª–æ—Å–∏—Ö —Ñ–∞–π–ª—ñ–≤
        temp_dir = os.path.join(UPLOAD_FOLDER, f"single_speakers_{job_id}")
        os.makedirs(temp_dir, exist_ok=True)
        
        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ SpeechBrain separation –¥–ª—è —è–∫—ñ—Å–Ω–æ—ó –Ω–∞—Ä—ñ–∑–∫–∏ (—è–∫ –≤ speechbrain_separation.py)
        separation_result = separate_speakers_with_speechbrain(audio_path, temp_dir)
        
        speaker_files = {}
        
        if separation_result.get('success'):
            # SpeechBrain separation —É—Å–ø—ñ—à–Ω–∞ - —Ñ–∞–π–ª–∏ –ü–û–í–ù–Ü–°–¢–Æ —Ä–æ–∑–¥—ñ–ª–µ–Ω—ñ, –Ω–µ –Ω–∞—Ä—ñ–∑–∞–Ω—ñ!
            speechbrain_speaker_files = separation_result['speaker_files']
            
            # SpeechBrain separation —Å—Ç–≤–æ—Ä—é—î –ø–æ–≤–Ω—ñ —Ç—Ä–µ–∫–∏ –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
            # –ù–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω—ñ segments - —Ñ–∞–π–ª–∏ –≤–∂–µ –º—ñ—Å—Ç—è—Ç—å —Ç—ñ–ª—å–∫–∏ –æ–¥–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
            for speaker_id, file_info in speechbrain_speaker_files.items():
                # –î–ª—è —Ä–æ–∑–¥—ñ–ª–µ–Ω–∏—Ö —Ñ–∞–π–ª—ñ–≤ segments –Ω–µ –ø–æ—Ç—Ä—ñ–±–Ω—ñ - —Ñ–∞–π–ª –≤–∂–µ –º—ñ—Å—Ç–∏—Ç—å —Ç—ñ–ª—å–∫–∏ –æ–¥–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
                # –ê–ª–µ –∑–±–µ—Ä—ñ–≥–∞—î–º–æ –ø–æ—Ä–æ–∂–Ω—ñ–π —Å–ø–∏—Å–æ–∫ segments –¥–ª—è —Å—É–º—ñ—Å–Ω–æ—Å—Ç—ñ –∑ –∫–æ–¥–æ–º –Ω–∏–∂—á–µ
                speaker_files[speaker_id] = {
                    'path': file_info['path'],
                    'segments': [],  # –†–æ–∑–¥—ñ–ª–µ–Ω—ñ —Ñ–∞–π–ª–∏ –Ω–µ –ø–æ—Ç—Ä–µ–±—É—é—Ç—å segments - –≤–æ–Ω–∏ –≤–∂–µ –ø–æ–≤–Ω—ñ —Ç—Ä–µ–∫–∏
                    'speaker_label': file_info.get('speaker_label', f'SPEAKER_{speaker_id:02d}'),
                    'is_separated': True  # –ü–æ–∑–Ω–∞—á–∫–∞, —â–æ —Ü–µ —Ä–æ–∑–¥—ñ–ª–µ–Ω–∏–π —Ñ–∞–π–ª, –∞ –Ω–µ –Ω–∞—Ä—ñ–∑–∞–Ω–∏–π
                }
            
            print(f"‚úÖ [Job {job_id}] Step 1 completed: {len(speaker_files)} FULLY SEPARATED single-speaker files created using SpeechBrain")
            sys.stdout.flush()
        else:
            # Fallback –¥–æ –ø—Ä–æ—Å—Ç–æ–≥–æ –≤–∏—Ä—ñ–∑–∞–Ω–Ω—è —Å–µ–≥–º–µ–Ω—Ç—ñ–≤
            print(f"‚ö†Ô∏è [Job {job_id}] SpeechBrain separation failed: {separation_result.get('error', 'Unknown error')}")
            print(f"üîÑ [Job {job_id}] Falling back to simple segment extraction...")
            sys.stdout.flush()
            
            # –ì—Ä—É–ø—É—î–º–æ —Å–µ–≥–º–µ–Ω—Ç–∏ –ø–æ —Å–ø—ñ–∫–µ—Ä–∞—Ö
            speakers_segments = {}
            for seg in diarization_segments:
                speaker = seg.get('speaker', 0)
                if speaker not in speakers_segments:
                    speakers_segments[speaker] = []
                speakers_segments[speaker].append(seg)
            
            print(f"üìä [Job {job_id}] Found {len(speakers_segments)} speakers")
            sys.stdout.flush()
            
            # –í–∏—Ç—è–≥—É—î–º–æ –æ–¥–Ω–æ–≥–æ–ª–æ—Å—ñ —Ñ–∞–π–ª–∏ –ø—Ä–æ—Å—Ç–∏–º –º–µ—Ç–æ–¥–æ–º
            for speaker, segments in speakers_segments.items():
                # –°–æ—Ä—Ç—É—î–º–æ —Å–µ–≥–º–µ–Ω—Ç–∏ –∑–∞ —á–∞—Å–æ–º
                segments_sorted = sorted(segments, key=lambda x: x['start'])
                
                output_path = extract_single_speaker_audio(audio_path, segments_sorted, temp_dir)
                if output_path:
                    speaker_files[speaker] = {
                        'path': output_path,
                        'segments': segments_sorted
                    }
            
            print(f"‚úÖ [Job {job_id}] Step 1 completed: {len(speaker_files)} single-speaker files created using simple extraction")
            sys.stdout.flush()
        
        if not speaker_files:
            with jobs_lock:
                jobs[job_id]['status'] = 'failed'
                jobs[job_id]['error'] = 'No single-speaker files could be extracted'
                jobs[job_id]['code'] = 'NO_FILES_EXTRACTED'
            return
        
        # –ö—Ä–æ–∫ 2: –¢—Ä–∞–Ω—Å–∫—Ä–∏–±—É—î–º–æ –∫–æ–∂–µ–Ω –æ–¥–Ω–æ–≥–æ–ª–æ—Å–∏–π —Ñ–∞–π–ª
        print(f"üìù [Job {job_id}] Step 2: Transcribing single-speaker files...")
        sys.stdout.flush()
        
        speaker_transcriptions = {}
        for speaker, file_info in speaker_files.items():
            print(f"üé§ [Job {job_id}] Transcribing speaker {speaker}...")
            sys.stdout.flush()
            
            transcription, transcription_segments, words = transcribe_audio(file_info['path'], transcription_provider='whisper')
            
            if transcription:
                # –î–ª—è —Ä–æ–∑–¥—ñ–ª–µ–Ω–∏—Ö —Ñ–∞–π–ª—ñ–≤ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ç—Ä–∏–≤–∞–ª—ñ—Å—Ç—å –∑ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó
                # (–±–æ segments –ø–æ—Ä–æ–∂–Ω—ñ –¥–ª—è —Ä–æ–∑–¥—ñ–ª–µ–Ω–∏—Ö —Ñ–∞–π–ª—ñ–≤)
                if file_info.get('is_separated'):
                    # –†–æ–∑–¥—ñ–ª–µ–Ω–∏–π —Ñ–∞–π–ª - –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ç—Ä–∏–≤–∞–ª—ñ—Å—Ç—å –∑ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó
                    total_duration = max(seg.get('end', 0) for seg in transcription_segments) if transcription_segments else 0
                    num_segments = len(transcription_segments)
                else:
                    # –ù–∞—Ä—ñ–∑–∞–Ω–∏–π —Ñ–∞–π–ª - –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ segments –∑ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó
                    total_duration = sum(seg['end'] - seg['start'] for seg in file_info['segments'])
                    num_segments = len(file_info['segments'])
                
                speaker_transcriptions[speaker] = {
                    'transcription': transcription,
                    'segments': transcription_segments,  # –°–µ–≥–º–µ–Ω—Ç–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó –æ–¥–Ω–æ–≥–æ–ª–æ—Å–æ–≥–æ —Ñ–∞–π–ª—É
                    'words': words,
                    'total_duration': total_duration,
                    'num_segments': num_segments,
                    'file_path': file_info['path'],
                    'diarization_segments': file_info.get('segments', []),  # –ú–æ–∂–µ –±—É—Ç–∏ –ø–æ—Ä–æ–∂–Ω—ñ–º –¥–ª—è —Ä–æ–∑–¥—ñ–ª–µ–Ω–∏—Ö —Ñ–∞–π–ª—ñ–≤
                    'is_separated': file_info.get('is_separated', False)  # –ü–æ–∑–Ω–∞—á–∫–∞, —â–æ —Ü–µ —Ä–æ–∑–¥—ñ–ª–µ–Ω–∏–π —Ñ–∞–π–ª
                }
                print(f"‚úÖ [Job {job_id}] Speaker {speaker} transcribed: {len(transcription)} chars, {total_duration:.2f}s duration")
            else:
                print(f"‚ö†Ô∏è [Job {job_id}] Speaker {speaker} transcription failed or empty")
        
        if not speaker_transcriptions:
            with jobs_lock:
                jobs[job_id]['status'] = 'failed'
                jobs[job_id]['error'] = 'No transcriptions could be generated'
                jobs[job_id]['code'] = 'NO_TRANSCRIPTIONS'
            return
        
        # –ö—Ä–æ–∫ 4: –í–∏–∑–Ω–∞—á–∞—î–º–æ –≥–æ–ª–æ–≤–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ –Ω–∞ –æ—Å–Ω–æ–≤—ñ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó
        print(f"üë§ [Job {job_id}] Step 4: Determining main speaker from transcriptions...")
        sys.stdout.flush()
        
        # –§–æ—Ä–º—É—î–º–æ combined_segments –¥–ª—è –≤—Å—ñ—Ö —Å–ø—ñ–∫–µ—Ä—ñ–≤ –∑ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ–π
        all_combined_segments = []
        for speaker, info in speaker_transcriptions.items():
            transcription_segments = info['segments']
            diarization_segments = info.get('diarization_segments', [])
            
            # –û–±'—î–¥–Ω—É—î–º–æ —Ç–∞–π–º—Å—Ç–µ–º–ø–∏ –∑ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó —Ç–∞ —Ç–µ–∫—Å—Ç –∑ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó
            if diarization_segments:
                for i, diar_seg in enumerate(diarization_segments):
                    transcript_text = ""
                    if i < len(transcription_segments):
                        transcript_text = transcription_segments[i].get('text', '')
                    elif transcription_segments:
                        transcript_text = info['transcription']
                    
                    all_combined_segments.append({
                        'speaker': speaker,
                        'start': diar_seg['start'],
                        'end': diar_seg['end'],
                        'text': transcript_text
                    })
            else:
                for seg in transcription_segments:
                    all_combined_segments.append({
                        'speaker': speaker,
                        'start': seg.get('start', 0),
                        'end': seg.get('end', 0),
                        'text': seg.get('text', '')
                    })
        
        # –û–±—á–∏—Å–ª—é—î–º–æ –∑–∞–≥–∞–ª—å–Ω—É —Ç—Ä–∏–≤–∞–ª—ñ—Å—Ç—å –¥–ª—è –ª–æ–≥—É–≤–∞–Ω–Ω—è
        total_duration = max(seg['end'] for seg in all_combined_segments) if all_combined_segments else 0
        
        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ç—É —Å–∞–º—É –ª–æ–≥—ñ–∫—É, —â–æ —ñ –≤ enhance_main_speaker_audio
        main_speaker, speaker_stats = determine_main_speaker_from_segments(all_combined_segments, duration=total_duration)
        print(f"‚úÖ [Job {job_id}] Main speaker determined: {main_speaker}")
        sys.stdout.flush()
        
        # –ö—Ä–æ–∫ 4: –§–æ—Ä–º—É—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç (—Ç—ñ–ª—å–∫–∏ –≥–æ–ª–æ–≤–Ω–∏–π —Å–ø—ñ–∫–µ—Ä)
        main_speaker_info = speaker_transcriptions[main_speaker]
        
        # –§–æ—Ä–º—É—î–º–æ segments –∑ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó + —Ç–∞–π–º—Å—Ç–µ–º–ø–∏ –∑ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–æ—ó –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó
        # –ö–æ–∂–µ–Ω —Å–µ–≥–º–µ–Ω—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó –º–∞—î —Ç–∞–π–º—Å—Ç–µ–º–ø–∏ –≤—ñ–¥–Ω–æ—Å–Ω–æ –æ–¥–Ω–æ–≥–æ–ª–æ—Å–æ–≥–æ —Ñ–∞–π–ª—É
        # –ü–æ—Ç—Ä—ñ–±–Ω–æ –ø–µ—Ä–µ—Ç–≤–æ—Ä–∏—Ç–∏ —ó—Ö –Ω–∞ —Ç–∞–π–º—Å—Ç–µ–º–ø–∏ –≤—ñ–¥–Ω–æ—Å–Ω–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–æ–≥–æ —Ñ–∞–π–ª—É
        combined_segments = []
        diarization_segments = main_speaker_info.get('diarization_segments', [])
        transcription_segments = main_speaker_info['segments']
        
        # –Ø–∫—â–æ —î —Å–µ–≥–º–µ–Ω—Ç–∏ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —ó—Ö —Ç–∞–π–º—Å—Ç–µ–º–ø–∏
        # –Ø–∫—â–æ –Ω–µ–º–∞—î, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ç–∞–π–º—Å—Ç–µ–º–ø–∏ –∑ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó
        if diarization_segments:
            # –û–±'—î–¥–Ω—É—î–º–æ: –±–µ—Ä–µ–º–æ —Ç–µ–∫—Å—Ç –∑ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó, —Ç–∞–π–º—Å—Ç–µ–º–ø–∏ –∑ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó
            for i, diar_seg in enumerate(diarization_segments):
                # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–∏–π —Ç–µ–∫—Å—Ç –∑ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó (—è–∫—â–æ —î)
                transcript_text = ""
                if i < len(transcription_segments):
                    transcript_text = transcription_segments[i].get('text', '')
                elif transcription_segments:
                    # –Ø–∫—â–æ —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó –º–µ–Ω—à–µ, –±–µ—Ä–µ–º–æ –≤–µ—Å—å —Ç–µ–∫—Å—Ç
                    transcript_text = main_speaker_info['transcription']
                
                combined_segments.append({
                    'speaker': main_speaker,
                    'start': diar_seg['start'],
                    'end': diar_seg['end'],
                    'text': transcript_text
                })
        else:
            # –Ø–∫—â–æ –Ω–µ–º–∞—î —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Å–µ–≥–º–µ–Ω—Ç–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó
            for seg in transcription_segments:
                combined_segments.append({
                    'speaker': main_speaker,
                    'start': seg.get('start', 0),
                    'end': seg.get('end', 0),
                    'text': seg.get('text', '')
                })
        
        # –§–æ—Ä–º—É—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –¥–ª—è –í–°–Ü–• —Å–ø—ñ–∫–µ—Ä—ñ–≤ (–Ω–µ —Ç—ñ–ª—å–∫–∏ –≥–æ–ª–æ–≤–Ω–æ–≥–æ)
        files_result = []
        all_speakers_segments = {}  # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Å–µ–≥–º–µ–Ω—Ç–∏ –¥–ª—è –≤—Å—ñ—Ö —Å–ø—ñ–∫–µ—Ä—ñ–≤
        
        for speaker, info in speaker_transcriptions.items():
            # –§–æ—Ä–º—É—î–º–æ segments –¥–ª—è —Ü—å–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
            # –¢–†–Ü–ó –†–Ü–®–ï–ù–ù–Ø: –ó—ñ—Å—Ç–∞–≤–ª–µ–Ω–Ω—è –Ω–∞ –æ—Å–Ω–æ–≤—ñ –Ω–∞–∫–æ–ø–∏—á–µ–Ω–æ—ó —Ç—Ä–∏–≤–∞–ª–æ—Å—Ç—ñ
            # –í –æ–¥–Ω–æ–≥–æ–ª–æ—Å–æ–º—É —Ñ–∞–π–ª—ñ —Å–µ–≥–º–µ–Ω—Ç–∏ –π–¥—É—Ç—å –±–µ–∑ –ø–∞—É–∑, –∞ –≤ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–æ–º—É - –∑ –ø–∞—É–∑–∞–º–∏
            # –¢–æ–º—É –ø–æ—Ç—Ä—ñ–±–Ω–æ –æ–±—á–∏—Å–ª–∏—Ç–∏ –ø–æ–∑–∏—Ü—ñ—ó —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó –≤ –æ–¥–Ω–æ–≥–æ–ª–æ—Å–æ–º—É —Ñ–∞–π–ª—ñ
            combined_segments = []
            diarization_segments = info.get('diarization_segments', [])
            transcription_segments = info['segments']
            
            if diarization_segments and transcription_segments:
                # –ö–†–ò–¢–ò–ß–ù–û: –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Å–µ–≥–º–µ–Ω—Ç–∏ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó —è–∫ –æ—Å–Ω–æ–≤—É, –∞ —Ç–µ–∫—Å—Ç –±–µ—Ä–µ–º–æ –∑ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó
                # –¶–µ –∑–∞–±–µ–∑–ø–µ—á—É—î –ø—Ä–∞–≤–∏–ª—å–Ω—ñ —Ç–∞–π–º—Å—Ç–µ–º–ø–∏ —Ç–∞ –ø—Ä–∞–≤–∏–ª—å–Ω—É –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Å–µ–≥–º–µ–Ω—Ç—ñ–≤
                
                # –ö—Ä–æ–∫ 1: –û–±—á–∏—Å–ª—é—î–º–æ –ø–æ–∑–∏—Ü—ñ—ó —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó –≤ –æ–¥–Ω–æ–≥–æ–ª–æ—Å–æ–º—É —Ñ–∞–π–ª—ñ
                diar_positions = []
                accumulated_duration = 0
                
                # –°–æ—Ä—Ç—É—î–º–æ —Å–µ–≥–º–µ–Ω—Ç–∏ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó –∑–∞ —á–∞—Å–æ–º
                sorted_diar_segments = sorted(diarization_segments, key=lambda x: x['start'])
                
                for diar_seg in sorted_diar_segments:
                    diar_duration = diar_seg['end'] - diar_seg['start']
                    diar_positions.append({
                        'position_in_single_file': accumulated_duration,  # –ü–æ–∑–∏—Ü—ñ—è –≤ –æ–¥–Ω–æ–≥–æ–ª–æ—Å–æ–º—É —Ñ–∞–π–ª—ñ
                        'original_start': diar_seg['start'],  # –û—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π —Ç–∞–π–º—Å—Ç–µ–º–ø
                        'original_end': diar_seg['end'],  # –û—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π —Ç–∞–π–º—Å—Ç–µ–º–ø
                        'duration': diar_duration,
                        'index': len(diar_positions)  # –Ü–Ω–¥–µ–∫—Å –¥–ª—è –∑—ñ—Å—Ç–∞–≤–ª–µ–Ω–Ω—è
                    })
                    accumulated_duration += diar_duration
                
                print(f"üîç [Job {job_id}] Speaker {speaker}: Matching diarization segments with transcription")
                print(f"   - Diarization segments: {len(sorted_diar_segments)}")
                print(f"   - Transcription segments: {len(transcription_segments)}")
                print(f"   - Total duration in single file: {accumulated_duration:.2f}s")
                sys.stdout.flush()
                
                # –ö—Ä–æ–∫ 2: –î–ª—è –∫–æ–∂–Ω–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó –∑–Ω–∞—Ö–æ–¥–∏–º–æ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–∏–π —Ç–µ–∫—Å—Ç –∑ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó
                for diar_pos in diar_positions:
                    # –ó–Ω–∞—Ö–æ–¥–∏–º–æ —Å–µ–≥–º–µ–Ω—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó, —è–∫–∏–π –ø–µ—Ä–µ—Ç–∏–Ω–∞—î—Ç—å—Å—è –∑ —Ü–∏–º —Å–µ–≥–º–µ–Ω—Ç–æ–º –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó
                    best_transcript = None
                    best_overlap = 0
                    
                    diar_start_in_single = diar_pos['position_in_single_file']
                    diar_end_in_single = diar_pos['position_in_single_file'] + diar_pos['duration']
                    
                    for trans_seg in transcription_segments:
                        trans_start = trans_seg.get('start', 0)  # –í—ñ–¥–Ω–æ—Å–Ω–æ –æ–¥–Ω–æ–≥–æ–ª–æ—Å–æ–≥–æ —Ñ–∞–π–ª—É
                        trans_end = trans_seg.get('end', 0)
                        text = trans_seg.get('text', '').strip()
                        
                        if not text:
                            continue
                        
                        # –û–±—á–∏—Å–ª—é—î–º–æ –ø–µ—Ä–µ–∫—Ä–∏—Ç—Ç—è
                        overlap_start = max(trans_start, diar_start_in_single)
                        overlap_end = min(trans_end, diar_end_in_single)
                        overlap = max(0, overlap_end - overlap_start)
                        
                        if overlap > best_overlap:
                            best_overlap = overlap
                            best_transcript = text
                    
                    # –Ø–∫—â–æ –∑–Ω–∞–π—à–ª–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–∏–π —Ç–µ–∫—Å—Ç, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –π–æ–≥–æ
                    # –Ø–∫—â–æ –Ω—ñ, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –≤–µ—Å—å —Ç–µ–∫—Å—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó (fallback)
                    text_to_use = best_transcript if best_transcript else info.get('transcription', '')
                    
                    if text_to_use:
                        combined_segments.append({
                            'speaker': speaker,  # –í –æ–¥–Ω–æ–≥–æ–ª–æ—Å–æ–º—É —Ñ–∞–π–ª—ñ —î —Ç—ñ–ª—å–∫–∏ —Ü–µ–π —Å–ø—ñ–∫–µ—Ä
                            'start': round(diar_pos['original_start'], 2),
                            'end': round(diar_pos['original_end'], 2),
                            'text': text_to_use
                        })
                
                print(f"‚úÖ [Job {job_id}] Speaker {speaker}: Matched {len(combined_segments)} segments")
                if len(combined_segments) > 0:
                    print(f"   - First segment: {combined_segments[0].get('start', 0):.2f}s - {combined_segments[0].get('end', 0):.2f}s, text: {combined_segments[0].get('text', '')[:50]}")
                    if len(combined_segments) > 1:
                        print(f"   - Last segment: {combined_segments[-1].get('start', 0):.2f}s - {combined_segments[-1].get('end', 0):.2f}s, text: {combined_segments[-1].get('text', '')[:50]}")
                sys.stdout.flush()
                
            elif transcription_segments:
                # –Ø–∫—â–æ –Ω–µ–º–∞—î –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—é —è–∫ —î
                for seg in transcription_segments:
                    combined_segments.append({
                        'speaker': speaker,
                        'start': seg.get('start', 0),
                        'end': seg.get('end', 0),
                        'text': seg.get('text', '').strip()
                    })
            else:
                # –Ø–∫—â–æ –Ω–µ–º–∞—î —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—é (fallback)
                for diar_seg in diarization_segments:
                    combined_segments.append({
                        'speaker': speaker,
                        'start': diar_seg['start'],
                        'end': diar_seg['end'],
                        'text': info.get('transcription', '')
                    })
            
            all_speakers_segments[speaker] = combined_segments
            
            # –§–æ—Ä–º–∞—Ç—É—î–º–æ –¥—ñ–∞–ª–æ–≥ –¥–ª—è –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
            dialogue_lines = format_speaker_dialogue(combined_segments, main_speaker) if speaker == main_speaker else []
            
            files_result.append({
                'speaker': speaker,
                'transcript': info['transcription'],
                'segments': combined_segments,
                'timestamps': [{'start': seg['start'], 'end': seg['end']} for seg in combined_segments],
                'total_duration': info['total_duration'],
                'num_segments': info['num_segments'],
                'dialogue': dialogue_lines if speaker == main_speaker else []  # –í—ñ–¥—Ñ–æ—Ä–º–∞—Ç–æ–≤–∞–Ω–∏–π –¥—ñ–∞–ª–æ–≥ —Ç—ñ–ª—å–∫–∏ –¥–ª—è –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
            })
        
        # –ö—Ä–æ–∫ 6: –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –æ–¥–Ω–æ–≥–æ–ª–æ—Å—ñ —Ñ–∞–π–ª–∏ —Ç–∞ —Å—Ç–≤–æ—Ä—é—î–º–æ –ø–æ—Å–∏–ª–∞–Ω–Ω—è –¥–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è
        print(f"üéµ [Job {job_id}] Step 6: Preparing single-speaker audio files for download...")
        sys.stdout.flush()
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—é –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –æ–¥–Ω–æ–≥–æ–ª–æ—Å–∏—Ö —Ñ–∞–π–ª—ñ–≤ (–Ω–µ –≤–∏–¥–∞–ª—è—î–º–æ temp_dir –æ–¥—Ä–∞–∑—É)
        audio_files_urls = {}
        for speaker, file_info in speaker_files.items():
            file_path = file_info.get('path')
            if file_path and os.path.exists(file_path):
                try:
                    # –ö–æ–ø—ñ—é—î–º–æ —Ñ–∞–π–ª –≤ –ø–æ—Å—Ç—ñ–π–Ω—É –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—é –¥–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è
                    # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ job_id —Ç–∞ speaker_id –¥–ª—è —É–Ω—ñ–∫–∞–ª—å–Ω–æ—Å—Ç—ñ
                    download_dir = os.path.join(UPLOAD_FOLDER, 'single_speaker_audio')
                    os.makedirs(download_dir, exist_ok=True)
                    
                    # –°—Ç–≤–æ—Ä—é—î–º–æ —É–Ω—ñ–∫–∞–ª—å–Ω–µ —ñ–º'—è —Ñ–∞–π–ª—É
                    file_extension = os.path.splitext(file_path)[1] or '.wav'
                    download_filename = f"{job_id}_speaker_{speaker}{file_extension}"
                    download_path = os.path.join(download_dir, download_filename)
                    
                    # –ö–æ–ø—ñ—é—î–º–æ —Ñ–∞–π–ª
                    import shutil
                    shutil.copy2(file_path, download_path)
                    
                    file_size = os.path.getsize(download_path)
                    
                    # –°—Ç–≤–æ—Ä—é—î–º–æ URL –¥–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è
                    audio_files_urls[speaker] = {
                        'url': f'/api/single-speaker-audio/{job_id}/{speaker}',
                        'filename': f"speaker_{speaker}{file_extension}",
                        'size_bytes': file_size
                    }
                    
                    print(f"‚úÖ [Job {job_id}] Prepared speaker {speaker} audio: {file_size} bytes ‚Üí {download_path}")
                except Exception as e:
                    print(f"‚ö†Ô∏è [Job {job_id}] Failed to prepare speaker {speaker} audio: {e}")
                    import traceback
                    traceback.print_exc()
                    sys.stdout.flush()
        
        print(f"üéµ [Job {job_id}] Prepared {len(audio_files_urls)} audio files for download")
        sys.stdout.flush()
        
        result = {
            'success': True,
            'files': files_result,  # –í—Å—ñ —Å–ø—ñ–∫–µ—Ä–∏, –Ω–µ —Ç—ñ–ª—å–∫–∏ –≥–æ–ª–æ–≤–Ω–∏–π
            'main_speaker': main_speaker,  # –ì–æ–ª–æ–≤–Ω–∏–π —Å–ø—ñ–∫–µ—Ä –º—ñ–∂ –æ–¥–Ω–æ–≥–æ–ª–æ—Å–∏–º–∏ —Ñ–∞–π–ª–∞–º–∏
            'all_speakers_segments': all_speakers_segments,  # –°–µ–≥–º–µ–Ω—Ç–∏ –¥–ª—è –≤—Å—ñ—Ö —Å–ø—ñ–∫–µ—Ä—ñ–≤ (–¥–ª—è Markdown —Ñ–æ—Ä–º–∞—Ç—É–≤–∞–Ω–Ω—è)
            'original_diarization_segments': original_diarization_segments,  # –û—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω—ñ segments –∑ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó (–¥–ª—è –ø–æ–∫–∞–∑—É –≤—Å—ñ—Ö —Ä–µ–ø–ª—ñ–∫) - –∑–±–µ—Ä–µ–∂–µ–Ω—ñ –î–û –æ–±—Ä–æ–±–∫–∏
            'audio_files': audio_files_urls  # –ü–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ –æ–¥–Ω–æ–≥–æ–ª–æ—Å—ñ –∞—É–¥—ñ–æ —Ñ–∞–π–ª–∏ –¥–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è
        }
        
        # –û–Ω–æ–≤–ª—é—î–º–æ —Å—Ç–∞—Ç—É—Å
        with jobs_lock:
            jobs[job_id]['status'] = 'completed'
            jobs[job_id]['result'] = result
        
        print(f"‚úÖ [Job {job_id}] Processing completed successfully!")
        sys.stdout.flush()
        
        # –û—á–∏—â–∞—î–º–æ —Ç–∏–º—á–∞—Å–æ–≤—ñ —Ñ–∞–π–ª–∏
        try:
            import shutil
            if os.path.exists(temp_dir):
                shutil.rmtree(temp_dir)
            if os.path.exists(audio_path):
                os.remove(audio_path)
        except Exception as e:
            print(f"‚ö†Ô∏è [Job {job_id}] Could not clean up temp files: {e}")
    
    except Exception as e:
        print(f"‚ùå [Job {job_id}] Error in background processing: {e}")
        import traceback
        traceback.print_exc()
        sys.stdout.flush()
        
        with jobs_lock:
            jobs[job_id]['status'] = 'failed'
            jobs[job_id]['error'] = str(e)
            jobs[job_id]['code'] = 'PROCESSING_ERROR'


@app.route('/api/process-single-speaker-files', methods=['POST', 'OPTIONS'])
def api_process_single_speaker_files():
    """
    –ê–°–ò–ù–•–†–û–ù–ù–ò–ô API –µ–Ω–¥–ø–æ—ñ–Ω—Ç –¥–ª—è —Ä–æ–∑—Ä—ñ–∑–∞–Ω–Ω—è –∞—É–¥—ñ–æ –Ω–∞ –æ–¥–Ω–æ–≥–æ–ª–æ—Å—ñ —Ñ–∞–π–ª–∏ —Ç–∞ —ó—Ö –æ–±—Ä–æ–±–∫–∏.
    –ü—Ä–∏–π–º–∞—î JSON –∑ base64-encoded —Ñ–∞–π–ª–æ–º —Ç–∞ job_id –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó.
    –ü–æ–≤–µ—Ä—Ç–∞—î job_id –æ–¥—Ä–∞–∑—É, –æ–±—Ä–æ–±–∫–∞ –≤–∏–∫–æ–Ω—É—î—Ç—å—Å—è –≤ —Ñ–æ–Ω—ñ.
    """
    import sys
    import base64
    
    print(f"üîµ [API] /api/process-single-speaker-files called - Method: {request.method}, Remote: {request.remote_addr}")
    sys.stdout.flush()
    
    # –û–±—Ä–æ–±–∫–∞ OPTIONS –¥–ª—è preflight –∑–∞–ø–∏—Ç—ñ–≤ (CORS)
    if request.method == 'OPTIONS':
        print("‚úÖ OPTIONS preflight request received from", request.remote_addr)
        response = jsonify({})
        response.headers.add('Access-Control-Allow-Origin', '*')
        response.headers.add('Access-Control-Allow-Headers', 'Content-Type')
        response.headers.add('Access-Control-Allow-Methods', 'POST, OPTIONS')
        sys.stdout.flush()
        return response
    
    print(f"üì• POST /api/process-single-speaker-files request received from {request.remote_addr}")
    print(f"üìã Request headers: {dict(request.headers)}")
    print(f"üìã Request content type: {request.content_type}")
    sys.stdout.flush()
    
    # –ì–µ–Ω–µ—Ä—É—î–º–æ job_id –î–û try –±–ª–æ–∫—É
    job_id = str(uuid.uuid4())
    filepath = None
    
    try:
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —Ü–µ JSON
        is_json = request.is_json or (request.content_type and 'application/json' in request.content_type)
        
        if not is_json:
            return jsonify({
                'success': False,
                'error': 'Content-Type must be application/json',
                'code': 'INVALID_CONTENT_TYPE'
            }), 400
        
        # –ü–∞—Ä—Å–∏–º–æ JSON
        data = request.get_json()
        if not data:
            return jsonify({
                'success': False,
                'error': 'No JSON data received',
                'code': 'NO_DATA'
            }), 400
        
        # –î–µ—Ç–∞–ª—å–Ω–µ –ª–æ–≥—É–≤–∞–Ω–Ω—è JSON
        print(f"üì¶ [Job {job_id}] JSON parsed successfully")
        print(f"üìã [Job {job_id}] JSON keys: {list(data.keys())}")
        print(f"üìã [Job {job_id}] Full JSON structure:")
        print(f"   - file: {'present' if 'file' in data else 'MISSING'}, type: {type(data.get('file'))}, length: {len(str(data.get('file', ''))) if data.get('file') else 0}")
        print(f"   - filename: {data.get('filename', 'MISSING')}")
        print(f"   - diarization_job_id: {data.get('diarization_job_id', 'MISSING')}")
        
        # –õ–æ–≥—É—î–º–æ –ø–µ—Ä—à—ñ 100 —Å–∏–º–≤–æ–ª—ñ–≤ base64 (—è–∫—â–æ —î)
        if 'file' in data and data['file']:
            file_preview = str(data['file'])[:100]
            print(f"   - file preview (first 100 chars): {file_preview}...")
        
        # –õ–æ–≥—É—î–º–æ –≤–µ—Å—å JSON (–æ–±–º–µ–∂–µ–Ω–æ, —â–æ–± –Ω–µ –∑–∞—Å–º—ñ—á—É–≤–∞—Ç–∏ –ª–æ–≥–∏)
        import json
        json_str = json.dumps(data, indent=2, default=str)
        if len(json_str) > 1000:
            print(f"   - JSON (first 1000 chars): {json_str[:1000]}...")
        else:
            print(f"   - Full JSON: {json_str}")
        sys.stdout.flush()
        
        # –û—Ç—Ä–∏–º—É—î–º–æ base64 —Ñ–∞–π–ª
        file_base64 = data.get('file')
        filename = data.get('filename', 'audio.wav')
        diarization_job_id = data.get('diarization_job_id')
        
        if not file_base64:
            return jsonify({
                'success': False,
                'error': 'No file data provided. Send file as base64 string in "file" field.',
                'code': 'NO_FILE'
            }), 400
        
        if not diarization_job_id:
            return jsonify({
                'success': False,
                'error': 'No diarization_job_id provided. Send job_id from diarization in "diarization_job_id" field.',
                'code': 'NO_DIARIZATION_JOB_ID'
            }), 400
        
        # –í–∏—Ç—è–≥—É—î–º–æ segments –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó
        with jobs_lock:
            if diarization_job_id not in jobs:
                return jsonify({
                    'success': False,
                    'error': f'Diarization job {diarization_job_id} not found. Make sure diarization is completed first.',
                    'code': 'DIARIZATION_JOB_NOT_FOUND'
                }), 404
            
            diarization_job = jobs[diarization_job_id]
            if diarization_job['status'] != 'completed':
                return jsonify({
                    'success': False,
                    'error': f'Diarization job {diarization_job_id} is not completed yet. Status: {diarization_job["status"]}',
                    'code': 'DIARIZATION_NOT_COMPLETED'
                }), 400
            
            diarization_result = diarization_job.get('result', {})
            
            # –ë–µ—Ä–µ–º–æ segments –∑ combined, –±–æ –≤–æ–Ω–∏ –º—ñ—Å—Ç—è—Ç—å —Ç–µ–∫—Å—Ç
            # combined.segments –º—ñ—Å—Ç—è—Ç—å –æ–±'—î–¥–Ω–∞–Ω—ñ –¥–∞–Ω—ñ –∑ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó (speaker) —Ç–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó (text)
            combined = diarization_result.get('combined', {})
            segments = combined.get('segments', [])
            
            # –õ–æ–≥—É—î–º–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Å–ø—ñ–∫–µ—Ä—ñ–≤ –≤ combined
            if segments:
                unique_speakers = set(seg.get('speaker', 0) for seg in segments)
                print(f"üìä [Job {job_id}] Combined segments: {len(segments)} segments, {len(unique_speakers)} speakers: {sorted(unique_speakers)}")
                sys.stdout.flush()
            
            # –Ø–∫—â–æ –Ω–µ–º–∞—î –≤ combined, —Å–ø—Ä–æ–±—É—î–º–æ –≤–∑—è—Ç–∏ –∑ diarization (fallback, –∞–ª–µ –±–µ–∑ —Ç–µ–∫—Å—Ç—É)
            if not segments:
                diarization = diarization_result.get('diarization', {})
                segments = diarization.get('segments', [])
                if segments:
                    unique_speakers = set(seg.get('speaker', 0) for seg in segments)
                    print(f"‚ö†Ô∏è [Job {job_id}] Using diarization segments (fallback, no text): {len(segments)} segments, {len(unique_speakers)} speakers: {sorted(unique_speakers)}")
                    sys.stdout.flush()
        
        if not segments:
            return jsonify({
                'success': False,
                'error': 'No segments found in diarization result. Make sure diarization completed successfully.',
                'code': 'NO_SEGMENTS'
            }), 400
        
        print(f"üìä Diarization result: {len(segments)} segments from job {diarization_job_id}")
        sys.stdout.flush()
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≤–∏–∑–Ω–∞—á–∞—î–º–æ —Ñ–æ—Ä–º–∞—Ç, —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ
        if '.' not in filename or not allowed_file(filename):
            detected_format = detect_audio_format_from_base64(file_base64)
            if detected_format:
                if '.' in filename:
                    filename = filename.rsplit('.', 1)[0] + '.' + detected_format
                else:
                    filename = filename + '.' + detected_format
            else:
                filename = filename + '.m4a' if '.' not in filename else filename
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ –∑–∞–≤–¥–∞–Ω–Ω—è –î–û –¥–µ–∫–æ–¥—É–≤–∞–Ω–Ω—è —Ñ–∞–π–ª—É
        with jobs_lock:
            jobs[job_id] = {
                'status': 'pending',
                'result': None,
                'error': None,
                'created_at': datetime.now()
            }
            print(f"‚úÖ [Job {job_id}] Job created and stored in jobs dictionary")
            print(f"üìä Total jobs after creation: {len(jobs)}")
            print(f"üìã Job {job_id} exists in jobs: {job_id in jobs}")
            sys.stdout.flush()
        
        print(f"‚úÖ [Job {job_id}] Job created, returning job_id IMMEDIATELY")
        sys.stdout.flush()
        
        # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ job_id –û–î–†–ê–ó–£
        response = jsonify({
            'success': True,
            'job_id': job_id,
            'status': 'pending',
            'message': 'Processing started. Use GET /api/process-single-speaker-files/{job_id}/status to check progress.'
        })
        response.headers.add('Access-Control-Allow-Origin', '*')
        
        # –î–µ–∫–æ–¥—É—î–º–æ base64 —Ç–∞ –æ–±—Ä–æ–±–ª—è—î–º–æ –≤ —Ñ–æ–Ω—ñ
        def decode_and_process():
            try:
                print(f"üíæ [Job {job_id}] Background: Starting base64 decode...")
                sys.stdout.flush()
                
                # –û—á–∏—â–∞—î–º–æ base64
                file_base64_clean = clean_base64_string(file_base64)
                
                # –î–µ–∫–æ–¥—É—î–º–æ base64
                file_data = base64.b64decode(file_base64_clean, validate=True)
                file_size = len(file_data)
                print(f"‚úÖ [Job {job_id}] Background: Base64 decode successful! Decoded size: {file_size} bytes ({file_size / (1024*1024):.2f} MB)")
                sys.stdout.flush()
                
                if file_size > MAX_FILE_SIZE:
                    with jobs_lock:
                        jobs[job_id]['status'] = 'failed'
                        jobs[job_id]['error'] = f'File too large. Maximum size: {MAX_FILE_SIZE / (1024*1024):.0f} MB'
                        jobs[job_id]['code'] = 'FILE_SIZE_EXCEEDED'
                    return
                
                # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Ñ–∞–π–ª —Ç–∏–º—á–∞—Å–æ–≤–æ
                filepath = os.path.join(UPLOAD_FOLDER, f"{job_id}_{filename}")
                with open(filepath, 'wb') as f:
                    f.write(file_data)
                print(f"üíæ [Job {job_id}] Background: File saved: {filepath}")
                sys.stdout.flush()
                
                # –õ–æ–≥—É—î–º–æ segments –ø–µ—Ä–µ–¥ –ø–µ—Ä–µ–¥–∞—á–µ—é –≤ process_single_speaker_files_background
                print(f"üìä [Job {job_id}] Passing segments to process_single_speaker_files_background:")
                print(f"   - Total segments: {len(segments)}")
                if segments:
                    unique_speakers = set(seg.get('speaker', 0) for seg in segments)
                    print(f"   - Unique speakers: {sorted(unique_speakers)}")
                    # –ü–æ–∫–∞–∑—É—î–º–æ –ø—Ä–∏–∫–ª–∞–¥ segments –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
                    for speaker_id in sorted(unique_speakers):
                        speaker_segments = [seg for seg in segments if seg.get('speaker', 0) == speaker_id]
                        print(f"   - Speaker {speaker_id}: {len(speaker_segments)} segments")
                        if speaker_segments:
                            first_seg = speaker_segments[0]
                            print(f"     Example: start={first_seg.get('start')}, text={first_seg.get('text', '')[:50]}")
                sys.stdout.flush()
                
                # –û–±—Ä–æ–±–ª—è—î–º–æ –æ–¥–Ω–æ–≥–æ–ª–æ—Å—ñ —Ñ–∞–π–ª–∏ (LLM –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—è –±—É–¥–µ –≤ —à–æ—Ä—Ç–∫–∞—Ç–∞—Ö)
                process_single_speaker_files_background(job_id, filepath, segments)
                
            except Exception as e:
                print(f"‚ùå [Job {job_id}] Background: Error: {e}")
                import traceback
                traceback.print_exc()
                with jobs_lock:
                    jobs[job_id]['status'] = 'failed'
                    jobs[job_id]['error'] = str(e)
                    jobs[job_id]['code'] = 'PROCESSING_ERROR'
        
        # –ó–∞–ø—É—Å–∫–∞—î–º–æ –æ–±—Ä–æ–±–∫—É –≤ —Ñ–æ–Ω—ñ
        thread = threading.Thread(target=decode_and_process, daemon=True)
        thread.start()
        
        return response, 202  # 202 Accepted
    
    except Exception as e:
        print(f"‚ùå [Job {job_id}] Error creating job: {e}")
        import traceback
        traceback.print_exc()
        sys.stdout.flush()
        
        # –í–∏–¥–∞–ª—è—î–º–æ job –∑—ñ —Å–ª–æ–≤–Ω–∏–∫–∞
        with jobs_lock:
            if job_id in jobs:
                del jobs[job_id]
        
        return jsonify({
            'success': False,
            'error': str(e),
            'code': 'PROCESSING_ERROR'
        }), 500


@app.route('/api/process-single-speaker-files/<job_id>/status', methods=['GET', 'OPTIONS'])
def get_process_single_speaker_files_status(job_id):
    """–û—Ç—Ä–∏–º—É—î —Å—Ç–∞—Ç—É—Å –æ–±—Ä–æ–±–∫–∏ –æ–¥–Ω–æ–≥–æ–ª–æ—Å–∏—Ö —Ñ–∞–π–ª—ñ–≤"""
    if request.method == 'OPTIONS':
        response = jsonify({})
        response.headers.add('Access-Control-Allow-Origin', '*')
        response.headers.add('Access-Control-Allow-Methods', 'GET, OPTIONS')
        return response
    
    print(f"üîµ [API] GET /api/process-single-speaker-files/{job_id}/status called from {request.remote_addr}")
    print(f"üìã Requested job_id: {job_id}")
    sys.stdout.flush()
    
    with jobs_lock:
        print(f"üìä Total jobs in memory: {len(jobs)}")
        print(f"üìã Available job_ids: {list(jobs.keys())[:5]}...")  # –ü–æ–∫–∞–∑—É—î–º–æ –ø–µ—Ä—à—ñ 5
        sys.stdout.flush()
        
        if job_id not in jobs:
            print(f"‚ùå Job {job_id} not found in jobs dictionary")
            sys.stdout.flush()
            return jsonify({
                'success': False,
                'error': f'Job not found: {job_id}',
                'code': 'JOB_NOT_FOUND',
                'available_jobs_count': len(jobs)
            }), 404
        
        job = jobs[job_id]
        
        if job['status'] == 'completed':
            result = job['result']
            
            # –§–æ—Ä–º—É—î–º–æ Markdown —Ñ–æ—Ä–º–∞—Ç—É–≤–∞–Ω–Ω—è
            markdown_data = {}
            if 'all_speakers_segments' in result:
                original_segments = result.get('original_diarization_segments', [])
                
                # –õ–æ–≥—É—î–º–æ –¥–ª—è –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
                print(f"üìä [Status {job_id}] Formatting markdown:")
                print(f"   - original_diarization_segments: {len(original_segments)} segments")
                if original_segments:
                    unique_speakers = set(seg.get('speaker', 0) for seg in original_segments)
                    print(f"   - Unique speakers in original: {sorted(unique_speakers)}")
                    # –ü–æ–∫–∞–∑—É—î–º–æ –ø–µ—Ä—à—ñ 3 segments –¥–ª—è –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
                    for i, seg in enumerate(original_segments[:3]):
                        print(f"   - Segment {i}: speaker={seg.get('speaker')}, start={seg.get('start')}, text={seg.get('text', '')[:50]}")
                print(f"   - all_speakers_segments keys: {list(result['all_speakers_segments'].keys())}")
                sys.stdout.flush()
                
                markdown_data = format_single_speaker_files_markdown(
                    result['all_speakers_segments'],
                    original_segments
                )
                
                # –î–æ–¥–∞—î–º–æ –≤—ñ–¥—Ñ–æ—Ä–º–∞—Ç–æ–≤–∞–Ω–∏–π –¥—ñ–∞–ª–æ–≥ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
                main_speaker = result.get('main_speaker')
                if main_speaker is not None and 'files' in result:
                    # –ó–Ω–∞—Ö–æ–¥–∏–º–æ —Ñ–∞–π–ª –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
                    for file_info in result['files']:
                        if file_info.get('speaker') == main_speaker and 'dialogue' in file_info:
                            dialogue_lines = file_info['dialogue']
                            if dialogue_lines:
                                # –î–æ–¥–∞—î–º–æ –∫–ª—é—á –∑ –¥—ñ–∞–ª–æ–≥–æ–º –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
                                markdown_data['MainSpeakerDialogue'] = "\n".join(dialogue_lines)
                                print(f"üìä [Status {job_id}] Added MainSpeakerDialogue: {len(dialogue_lines)} lines")
                                sys.stdout.flush()
                
                # –õ–æ–≥—É—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ñ–æ—Ä–º–∞—Ç—É–≤–∞–Ω–Ω—è
                print(f"üìä [Status {job_id}] Markdown formatting result:")
                print(f"   - Markdown keys: {list(markdown_data.keys())}")
                for key in ['File1Speaker0', 'File1Speaker1', 'File2Speaker0', 'File2Speaker1', 'MainSpeakerDialogue']:
                    if key in markdown_data:
                        content = markdown_data[key]
                        content_preview = content[:100] if content else "(empty)"
                        print(f"   - {key}: {len(content)} chars, preview: {content_preview}")
                    else:
                        print(f"   - {key}: MISSING")
                sys.stdout.flush()
            
            # –§–æ—Ä–º—É—î–º–æ —Å–ø–∏—Å–∫–∏ —Ä–µ–ø–ª—ñ–∫ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ —Ñ–∞–π–ª—É
            mainspeakerfile1 = []
            mainspeakerfile2 = []
            
            if 'files' in result:
                # –í–∏–∑–Ω–∞—á–∞—î–º–æ, —è–∫–∏–π —Å–ø—ñ–∫–µ—Ä –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î —è–∫–æ–º—É —Ñ–∞–π–ª—É
                unique_speakers = sorted(set(f['speaker'] for f in result['files']))
                file_to_speaker = {}
                for idx, speaker_id in enumerate(unique_speakers, start=1):
                    file_to_speaker[idx] = speaker_id  # File1 -> –ø–µ—Ä—à–∏–π —Å–ø—ñ–∫–µ—Ä, File2 -> –¥—Ä—É–≥–∏–π —Å–ø—ñ–∫–µ—Ä
                
                print(f"üìä [Status {job_id}] Processing main speaker files:")
                print(f"   - file_to_speaker: {file_to_speaker}")
                sys.stdout.flush()
                
                # –î–ª—è File1: –≤ –æ–¥–Ω–æ–≥–æ–ª–æ—Å–æ–º—É —Ñ–∞–π–ª—ñ —î —Ç—ñ–ª—å–∫–∏ –æ–¥–∏–Ω —Å–ø—ñ–∫–µ—Ä, —Ç–æ–º—É –≤—Å—ñ —Å–µ–≥–º–µ–Ω—Ç–∏ –Ω–∞–ª–µ–∂–∞—Ç—å —Ü—å–æ–º—É —Å–ø—ñ–∫–µ—Ä—É
                file1_speaker_id = file_to_speaker.get(1)
                if file1_speaker_id is not None:
                    # –ó–Ω–∞—Ö–æ–¥–∏–º–æ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ File1
                    file1_info = next((f for f in result['files'] if f.get('speaker') == file1_speaker_id), None)
                    if file1_info and 'segments' in file1_info:
                        file1_segments = file1_info['segments']
                        if file1_segments:
                            # –í –æ–¥–Ω–æ–≥–æ–ª–æ—Å–æ–º—É —Ñ–∞–π–ª—ñ –≤—Å—ñ —Å–µ–≥–º–µ–Ω—Ç–∏ –Ω–∞–ª–µ–∂–∞—Ç—å –æ–¥–Ω–æ–º—É —Å–ø—ñ–∫–µ—Ä—É (file1_speaker_id)
                            # –í–∏–∑–Ω–∞—á–∞—î–º–æ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ –Ω–∞ –æ—Å–Ω–æ–≤—ñ —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ (–¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—ñ –∑ enhance-main-speaker)
                            file1_duration = max(seg.get('end', 0) for seg in file1_segments) if file1_segments else 0
                            file1_main_speaker, _ = determine_main_speaker_from_segments(file1_segments, duration=file1_duration)
                            
                            print(f"   - File1: speaker_id={file1_speaker_id}, main_speaker={file1_main_speaker}, segments={len(file1_segments)}")
                            
                            # –í –æ–¥–Ω–æ–≥–æ–ª–æ—Å–æ–º—É —Ñ–∞–π–ª—ñ –≤—Å—ñ —Å–µ–≥–º–µ–Ω—Ç–∏ –Ω–∞–ª–µ–∂–∞—Ç—å –æ–¥–Ω–æ–º—É —Å–ø—ñ–∫–µ—Ä—É, —Ç–æ–º—É –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –≤—Å—ñ —Å–µ–≥–º–µ–Ω—Ç–∏
                            # –ê–ª–µ —Ñ—ñ–ª—å—Ç—Ä—É—î–º–æ —Ç—ñ–ª—å–∫–∏ —Ç—ñ, —â–æ –Ω–∞–ª–µ–∂–∞—Ç—å –æ—Å–Ω–æ–≤–Ω–æ–º—É —Å–ø—ñ–∫–µ—Ä—É (–¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—ñ)
                            for seg in file1_segments:
                                # –í –æ–¥–Ω–æ–≥–æ–ª–æ—Å–æ–º—É —Ñ–∞–π–ª—ñ –≤—Å—ñ —Å–µ–≥–º–µ–Ω—Ç–∏ –º–∞—é—Ç—å speaker == file1_speaker_id
                                # –ê–ª–µ –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—ñ –ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —Ü–µ –æ—Å–Ω–æ–≤–Ω–∏–π —Å–ø—ñ–∫–µ—Ä
                                if seg.get('speaker') == file1_main_speaker:
                                    start_time = seg.get('start', 0)
                                    minutes = int(start_time // 60)
                                    seconds = int(start_time % 60)
                                    time_str = f"{minutes:02d}:{seconds:02d}"
                                    text = seg.get('text', '').strip()
                                    if text:
                                        mainspeakerfile1.append(f"{time_str} Speaker {file1_main_speaker}: {text}")
                
                # –î–ª—è File2: –≤ –æ–¥–Ω–æ–≥–æ–ª–æ—Å–æ–º—É —Ñ–∞–π–ª—ñ —î —Ç—ñ–ª—å–∫–∏ –æ–¥–∏–Ω —Å–ø—ñ–∫–µ—Ä, —Ç–æ–º—É –≤—Å—ñ —Å–µ–≥–º–µ–Ω—Ç–∏ –Ω–∞–ª–µ–∂–∞—Ç—å —Ü—å–æ–º—É —Å–ø—ñ–∫–µ—Ä—É
                file2_speaker_id = file_to_speaker.get(2)
                if file2_speaker_id is not None:
                    # –ó–Ω–∞—Ö–æ–¥–∏–º–æ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ File2
                    file2_info = next((f for f in result['files'] if f.get('speaker') == file2_speaker_id), None)
                    if file2_info and 'segments' in file2_info:
                        file2_segments = file2_info['segments']
                        if file2_segments:
                            # –í –æ–¥–Ω–æ–≥–æ–ª–æ—Å–æ–º—É —Ñ–∞–π–ª—ñ –≤—Å—ñ —Å–µ–≥–º–µ–Ω—Ç–∏ –Ω–∞–ª–µ–∂–∞—Ç—å –æ–¥–Ω–æ–º—É —Å–ø—ñ–∫–µ—Ä—É (file2_speaker_id)
                            # –í–∏–∑–Ω–∞—á–∞—î–º–æ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ –Ω–∞ –æ—Å–Ω–æ–≤—ñ —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ (–¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—ñ –∑ enhance-main-speaker)
                            file2_duration = max(seg.get('end', 0) for seg in file2_segments) if file2_segments else 0
                            file2_main_speaker, _ = determine_main_speaker_from_segments(file2_segments, duration=file2_duration)
                            
                            print(f"   - File2: speaker_id={file2_speaker_id}, main_speaker={file2_main_speaker}, segments={len(file2_segments)}")
                            
                            # –í –æ–¥–Ω–æ–≥–æ–ª–æ—Å–æ–º—É —Ñ–∞–π–ª—ñ –≤—Å—ñ —Å–µ–≥–º–µ–Ω—Ç–∏ –Ω–∞–ª–µ–∂–∞—Ç—å –æ–¥–Ω–æ–º—É —Å–ø—ñ–∫–µ—Ä—É, —Ç–æ–º—É –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –≤—Å—ñ —Å–µ–≥–º–µ–Ω—Ç–∏
                            # –ê–ª–µ —Ñ—ñ–ª—å—Ç—Ä—É—î–º–æ —Ç—ñ–ª—å–∫–∏ —Ç—ñ, —â–æ –Ω–∞–ª–µ–∂–∞—Ç—å –æ—Å–Ω–æ–≤–Ω–æ–º—É —Å–ø—ñ–∫–µ—Ä—É (–¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—ñ)
                            for seg in file2_segments:
                                # –í –æ–¥–Ω–æ–≥–æ–ª–æ—Å–æ–º—É —Ñ–∞–π–ª—ñ –≤—Å—ñ —Å–µ–≥–º–µ–Ω—Ç–∏ –º–∞—é—Ç—å speaker == file2_speaker_id
                                # –ê–ª–µ –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—ñ –ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —Ü–µ –æ—Å–Ω–æ–≤–Ω–∏–π —Å–ø—ñ–∫–µ—Ä
                                if seg.get('speaker') == file2_main_speaker:
                                    start_time = seg.get('start', 0)
                                    minutes = int(start_time // 60)
                                    seconds = int(start_time % 60)
                                    time_str = f"{minutes:02d}:{seconds:02d}"
                                    text = seg.get('text', '').strip()
                                    if text:
                                        mainspeakerfile2.append(f"{time_str} Speaker {file2_main_speaker}: {text}")
                
                print(f"üìä [Status {job_id}] Main speaker files result:")
                print(f"   - mainspeakerfile1: {len(mainspeakerfile1)} replicas")
                print(f"   - mainspeakerfile2: {len(mainspeakerfile2)} replicas")
                sys.stdout.flush()
            
            # –û—Ç—Ä–∏–º—É—î–º–æ audio_files –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É (—è–∫—â–æ —î)
            audio_files = result.get('audio_files', {})
            
            # –î–æ–¥–∞—î–º–æ –ø–æ–≤–Ω—ñ URL –¥–ª—è audio_files
            # –í–∏–∑–Ω–∞—á–∞—î–º–æ –±–∞–∑–æ–≤–∏–π URL –∑ –∑–∞–ø–∏—Ç—É
            base_url = request.host_url.rstrip('/')
            audio_files_with_urls = {}
            for speaker, file_info in audio_files.items():
                if isinstance(file_info, dict) and 'url' in file_info:
                    # –°—Ç–≤–æ—Ä—é—î–º–æ –ø–æ–≤–Ω–∏–π URL (–≤—ñ–¥–Ω–æ—Å–Ω–∏–π —à–ª—è—Ö –≤–∂–µ —î –≤ file_info['url'])
                    relative_url = file_info['url']
                    # –Ø–∫—â–æ URL –≤–∂–µ –ø–æ–≤–Ω–∏–π (–ø–æ—á–∏–Ω–∞—î—Ç—å—Å—è –∑ http), –Ω–µ –¥–æ–¥–∞—î–º–æ base_url
                    if relative_url.startswith('http://') or relative_url.startswith('https://'):
                        full_url = relative_url
                    else:
                        # –î–æ–¥–∞—î–º–æ –±–∞–∑–æ–≤–∏–π URL –¥–æ –≤—ñ–¥–Ω–æ—Å–Ω–æ–≥–æ —à–ª—è—Ö—É
                        full_url = f"{base_url}{relative_url}"
                    
                    audio_files_with_urls[speaker] = {
                        **file_info,
                        'url': full_url
                    }
                else:
                    audio_files_with_urls[speaker] = file_info
            
            # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ JSON –∑ –ø–æ–ª—è–º–∏
            response_data = {
                'mainspeakerfile1': mainspeakerfile1,
                'mainspeakerfile2': mainspeakerfile2,
                'audio_files': audio_files_with_urls  # –ü–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ –æ–¥–Ω–æ–≥–æ–ª–æ—Å—ñ –∞—É–¥—ñ–æ —Ñ–∞–π–ª–∏ (–ø–æ–≤–Ω—ñ URL)
            }
            
            # –õ–æ–≥—É—î–º–æ –ø–æ–≤–Ω—É –≤—ñ–¥–ø–æ–≤—ñ–¥—å –¥–ª—è –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
            import json
            response_json = json.dumps(response_data, indent=2, ensure_ascii=False)
            print(f"üì§ [Status {job_id}] Full response JSON (first 500 chars):")
            print(response_json[:500])
            print(f"üì§ [Status {job_id}] Full response JSON length: {len(response_json)} chars")
            sys.stdout.flush()
            
            response = jsonify(response_data)
            response.headers.add('Access-Control-Allow-Origin', '*')
            return response, 200
        elif job['status'] == 'failed':
            response = jsonify({
                'success': False,
                'status': 'failed',
                'error': job.get('error', 'Unknown error'),
                'code': job.get('code', 'PROCESSING_ERROR')
            })
            response.headers.add('Access-Control-Allow-Origin', '*')
            return response, 200
        else:
            response = jsonify({
                'success': True,
                'status': job['status'],
                'message': 'Processing in progress...'
            })
            response.headers.add('Access-Control-Allow-Origin', '*')
            return response, 200


@app.route('/api/separate-audio', methods=['POST', 'OPTIONS'])
def api_separate_audio():
    """
    –ù–æ–≤–∏–π –µ–Ω–¥–ø–æ—ó–Ω—Ç –¥–ª—è —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –∞—É–¥—ñ–æ –Ω–∞ –æ–∫—Ä–µ–º—ñ –≥–æ–ª–æ—Å–∏.
    –ü—Ä–∏–π–º–∞—î –∞—É–¥—ñ–æ —Ñ–∞–π–ª, —Ä–æ–∑–±–∏–≤–∞—î –π–æ–≥–æ –Ω–∞ –¥–≤–∞ –≥–æ–ª–æ—Å–∏ –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é SpeechBrain separation,
    —ñ –ø–æ–≤–µ—Ä—Ç–∞—î –¥–≤–∞ –∞—É–¥—ñ–æ —Ç—Ä–µ–∫–∏ –≤ JSON –∑ –ø–æ–ª—è–º–∏ file1, file2.
    
    Returns:
        JSON –∑ –ø–æ–ª—è–º–∏:
        - file1: base64 encoded –∞—É–¥—ñ–æ –∞–±–æ URL –¥–æ —Ñ–∞–π–ª—É
        - file2: base64 encoded –∞—É–¥—ñ–æ –∞–±–æ URL –¥–æ —Ñ–∞–π–ª—É
        - success: bool
    """
    import sys
    import base64
    import uuid
    
    # –û–±—Ä–æ–±–∫–∞ OPTIONS –¥–ª—è preflight –∑–∞–ø–∏—Ç—ñ–≤ (CORS)
    if request.method == 'OPTIONS':
        response = jsonify({})
        response.headers.add('Access-Control-Allow-Origin', '*')
        response.headers.add('Access-Control-Allow-Headers', 'Content-Type')
        response.headers.add('Access-Control-Allow-Methods', 'POST, OPTIONS')
        return response
    
    try:
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —î —Ñ–∞–π–ª
        if 'audio' not in request.files:
            return jsonify({
                'success': False,
                'error': 'No audio file provided',
                'code': 'NO_FILE'
            }), 400
        
        audio_file = request.files['audio']
        if audio_file.filename == '':
            return jsonify({
                'success': False,
                'error': 'Empty filename',
                'code': 'EMPTY_FILENAME'
            }), 400
        
        print(f"üéµ [Separate Audio] Received file: {audio_file.filename}")
        sys.stdout.flush()
        
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª
        job_id = str(uuid.uuid4())
        file_extension = os.path.splitext(audio_file.filename)[1] or '.wav'
        temp_filename = f"separate_{job_id}{file_extension}"
        temp_path = os.path.join(UPLOAD_FOLDER, temp_filename)
        os.makedirs(UPLOAD_FOLDER, exist_ok=True)
        audio_file.save(temp_path)
        
        print(f"üíæ [Separate Audio] Saved to: {temp_path}")
        sys.stdout.flush()
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ —Ç–∏–º—á–∞—Å–æ–≤—É –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—é –¥–ª—è —Ä–æ–∑–¥—ñ–ª–µ–Ω–∏—Ö —Ñ–∞–π–ª—ñ–≤
        output_dir = os.path.join(UPLOAD_FOLDER, f"separated_{job_id}")
        os.makedirs(output_dir, exist_ok=True)
        
        # –í–∏–∫–æ–Ω—É—î–º–æ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é SpeechBrain
        print(f"üîÄ [Separate Audio] Starting SpeechBrain separation...")
        sys.stdout.flush()
        
        separation_result = separate_speakers_with_speechbrain(temp_path, output_dir)
        
        if not separation_result.get('success'):
            # –í–∏–¥–∞–ª—è—î–º–æ —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª
            try:
                os.remove(temp_path)
            except:
                pass
            return jsonify({
                'success': False,
                'error': separation_result.get('error', 'Separation failed'),
                'code': 'SEPARATION_FAILED'
            }), 500
        
        speaker_files = separation_result['speaker_files']
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —î –ø—Ä–∏–Ω–∞–π–º–Ω—ñ –¥–≤–∞ —Å–ø—ñ–∫–µ—Ä–∏
        if len(speaker_files) < 2:
            # –í–∏–¥–∞–ª—è—î–º–æ —Ç–∏–º—á–∞—Å–æ–≤—ñ —Ñ–∞–π–ª–∏
            try:
                os.remove(temp_path)
                import shutil
                shutil.rmtree(output_dir)
            except:
                pass
            return jsonify({
                'success': False,
                'error': f'Found only {len(speaker_files)} speaker(s), need at least 2',
                'code': 'INSUFFICIENT_SPEAKERS'
            }), 400
        
        # –ë–µ—Ä–µ–º–æ –ø–µ—Ä—à—ñ –¥–≤–∞ —Å–ø—ñ–∫–µ—Ä–∏
        speaker_ids = sorted(speaker_files.keys())[:2]
        speaker_0_file = speaker_files[speaker_ids[0]]['path']
        speaker_1_file = speaker_files[speaker_ids[1]]['path']
        
        print(f"‚úÖ [Separate Audio] Separation completed: speaker {speaker_ids[0]} and {speaker_ids[1]}")
        sys.stdout.flush()
        
        # –í–∏–¥–∞–ª—è—î–º–æ —Ç–∏–º—á–∞—Å–æ–≤–∏–π –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π —Ñ–∞–π–ª
        try:
            os.remove(temp_path)
        except:
            pass
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ URL-–∏ –¥–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ñ–∞–π–ª—ñ–≤
        base_url = request.host_url.rstrip('/')
        file1_url = f"{base_url}/api/separate-audio-file/{job_id}/0"
        file2_url = f"{base_url}/api/separate-audio-file/{job_id}/1"
        
        # –ü–µ—Ä–µ–º—ñ—â—É—î–º–æ —Ñ–∞–π–ª–∏ –≤ –ø–æ—Å—Ç—ñ–π–Ω—É –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—é –¥–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è
        download_dir = os.path.join(UPLOAD_FOLDER, 'separated_audio')
        os.makedirs(download_dir, exist_ok=True)
        
        file1_download_path = os.path.join(download_dir, f"{job_id}_speaker_0.wav")
        file2_download_path = os.path.join(download_dir, f"{job_id}_speaker_1.wav")
        
        import shutil
        shutil.copy2(speaker_0_file, file1_download_path)
        shutil.copy2(speaker_1_file, file2_download_path)
        
        # –í–∏–¥–∞–ª—è—î–º–æ —Ç–∏–º—á–∞—Å–æ–≤—É –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—é –∑ —Ä–æ–∑–¥—ñ–ª–µ–Ω–∏–º–∏ —Ñ–∞–π–ª–∞–º–∏
        try:
            shutil.rmtree(output_dir)
        except:
            pass
        
        # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∑ URL-–∞–º–∏
        response_data = {
            'success': True,
            'file1': file1_url,
            'file2': file2_url
        }
        
        print(f"üì§ [Separate Audio] Returning separated audio files")
        sys.stdout.flush()
        
        response = jsonify(response_data)
        response.headers.add('Access-Control-Allow-Origin', '*')
        return response, 200
        
    except Exception as e:
        print(f"‚ùå [Separate Audio] Error: {e}")
        import traceback
        traceback.print_exc()
        sys.stdout.flush()
        return jsonify({
            'success': False,
            'error': str(e),
            'code': 'PROCESSING_ERROR'
        }), 500


@app.route('/api/separate-audio-file/<job_id>/<int:speaker_id>', methods=['GET', 'OPTIONS'])
def get_separate_audio_file(job_id, speaker_id):
    """
    –ï–Ω–¥–ø–æ—ó–Ω—Ç –¥–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ä–æ–∑–¥—ñ–ª–µ–Ω–æ–≥–æ –∞—É–¥—ñ–æ —Ñ–∞–π–ª—É.
    –ü—ñ—Å–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ñ–∞–π–ª –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≤–∏–¥–∞–ª—è—î—Ç—å—Å—è.
    
    Args:
        job_id: ID –∑–∞–≤–¥–∞–Ω–Ω—è —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è
        speaker_id: ID —Å–ø—ñ–∫–µ—Ä–∞ (0 –∞–±–æ 1)
    """
    import sys
    
    # –û–±—Ä–æ–±–∫–∞ OPTIONS –¥–ª—è preflight –∑–∞–ø–∏—Ç—ñ–≤ (CORS)
    if request.method == 'OPTIONS':
        response = jsonify({})
        response.headers.add('Access-Control-Allow-Origin', '*')
        response.headers.add('Access-Control-Allow-Headers', 'Content-Type')
        response.headers.add('Access-Control-Allow-Methods', 'GET, OPTIONS')
        return response
    
    try:
        # –ó–Ω–∞—Ö–æ–¥–∏–º–æ —Ñ–∞–π–ª
        download_dir = os.path.join(UPLOAD_FOLDER, 'separated_audio')
        download_filename = f"{job_id}_speaker_{speaker_id}.wav"
        download_path = os.path.join(download_dir, download_filename)
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —Ñ–∞–π–ª —ñ—Å–Ω—É—î
        if not os.path.exists(download_path):
            print(f"‚ùå [Separate Audio Download] File not found: {download_path}")
            sys.stdout.flush()
            return jsonify({
                'success': False,
                'error': f'Audio file for speaker {speaker_id} not found',
                'code': 'FILE_NOT_FOUND'
            }), 404
        
        print(f"üì• [Separate Audio Download] Serving file: {download_path} for job {job_id}, speaker {speaker_id}")
        sys.stdout.flush()
        
        # –í—ñ–¥–ø—Ä–∞–≤–ª—è—î–º–æ —Ñ–∞–π–ª
        response = send_file(
            download_path,
            mimetype='audio/wav',
            as_attachment=True,
            download_name=f"speaker_{speaker_id}.wav"
        )
        response.headers.add('Access-Control-Allow-Origin', '*')
        
        # –í–∏–¥–∞–ª—è—î–º–æ —Ñ–∞–π–ª –ø—ñ—Å–ª—è –≤—ñ–¥–ø—Ä–∞–≤–∫–∏ (–≤ —Ñ–æ–Ω—ñ, —â–æ–± –Ω–µ –±–ª–æ–∫—É–≤–∞—Ç–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥—å)
        def delete_file_after_delay():
            import time
            time.sleep(2)  # –ó–∞—Ç—Ä–∏–º–∫–∞, —â–æ–± —Ñ–∞–π–ª —Ç–æ—á–Ω–æ –≤—ñ–¥–ø—Ä–∞–≤–∏–≤—Å—è
            try:
                if os.path.exists(download_path):
                    os.remove(download_path)
                    print(f"üóëÔ∏è [Separate Audio Download] Deleted file: {download_path}")
                    sys.stdout.flush()
            except Exception as e:
                print(f"‚ö†Ô∏è [Separate Audio Download] Failed to delete file {download_path}: {e}")
                sys.stdout.flush()
        
        # –ó–∞–ø—É—Å–∫–∞—î–º–æ –≤–∏–¥–∞–ª–µ–Ω–Ω—è –≤ –æ–∫—Ä–µ–º–æ–º—É –ø–æ—Ç–æ—Ü—ñ
        import threading
        delete_thread = threading.Thread(target=delete_file_after_delay)
        delete_thread.daemon = True
        delete_thread.start()
        
        return response
        
    except Exception as e:
        print(f"‚ùå [Separate Audio Download] Error: {e}")
        import traceback
        traceback.print_exc()
        sys.stdout.flush()
        return jsonify({
            'success': False,
            'error': str(e),
            'code': 'DOWNLOAD_ERROR'
        }), 500


@app.route('/api/diarize-and-transcribe', methods=['POST', 'OPTIONS'])
def api_diarize_and_transcribe():
    """
    –ï–Ω–¥–ø–æ—ó–Ω—Ç –¥–ª—è –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó —Ç–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó –∞—É–¥—ñ–æ —Ñ–∞–π–ª—É.
    –ü—Ä–∏–π–º–∞—î –∞—É–¥—ñ–æ —Ñ–∞–π–ª, –≤–∏–∫–æ–Ω—É—î –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—é —Ç–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—é,
    –ø–æ–≤–µ—Ä—Ç–∞—î —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç –∑ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è–º –ø–æ —Å–ø—ñ–∫–µ—Ä–∞–º.
    
    Returns:
        JSON –∑ –ø–æ–ª—è–º–∏:
        - success: bool
        - transcript: —Å–ø–∏—Å–æ–∫ —Ä—è–¥–∫—ñ–≤ —É —Ñ–æ—Ä–º–∞—Ç—ñ "–¢–∞–π–º—Å—Ç–µ–º–ø - –°–ø—ñ–∫–µ—Ä –Ω–æ–º–µ—Ä - –†–µ–ø–ª—ñ–∫–∞"
    """
    import sys
    
    # –û–±—Ä–æ–±–∫–∞ OPTIONS –¥–ª—è preflight –∑–∞–ø–∏—Ç—ñ–≤ (CORS)
    if request.method == 'OPTIONS':
        response = jsonify({})
        response.headers.add('Access-Control-Allow-Origin', '*')
        response.headers.add('Access-Control-Allow-Headers', 'Content-Type')
        response.headers.add('Access-Control-Allow-Methods', 'POST, OPTIONS')
        return response
    
    try:
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —î —Ñ–∞–π–ª
        if 'audio' not in request.files:
            return jsonify({
                'success': False,
                'error': 'No audio file provided',
                'code': 'NO_FILE'
            }), 400
        
        audio_file = request.files['audio']
        if audio_file.filename == '':
            return jsonify({
                'success': False,
                'error': 'Empty filename',
                'code': 'EMPTY_FILENAME'
            }), 400
        
        # –û—Ç—Ä–∏–º—É—î–º–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ (–æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ)
        processing_mode = request.form.get('mode', 'fast')  # 'smart' –∞–±–æ 'fast'
        transcription_provider = request.form.get('transcription_provider', 'whisper')
        num_speakers = request.form.get('num_speakers', None)
        if num_speakers:
            try:
                num_speakers = int(num_speakers)
            except:
                num_speakers = None
        
        print(f"üéµ [Diarize & Transcribe] Received file: {audio_file.filename}, mode: {processing_mode}, provider: {transcription_provider}")
        sys.stdout.flush()
        
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª
        job_id = str(uuid.uuid4())
        file_extension = os.path.splitext(audio_file.filename)[1] or '.wav'
        temp_filename = f"diarize_{job_id}{file_extension}"
        temp_path = os.path.join(UPLOAD_FOLDER, temp_filename)
        os.makedirs(UPLOAD_FOLDER, exist_ok=True)
        audio_file.save(temp_path)
        
        print(f"üíæ [Diarize & Transcribe] Saved to: {temp_path}")
        sys.stdout.flush()
        
        try:
            # –ö—Ä–æ–∫ 1: –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –∞—É–¥—ñ–æ
            print(f"üìÇ [Diarize & Transcribe] Step 1: Loading audio...")
            sys.stdout.flush()
            audio, sr = librosa.load(temp_path, sr=16000, mono=True)
            duration = librosa.get_duration(y=audio, sr=sr)
            print(f"‚è±Ô∏è  [Diarize & Transcribe] Audio duration: {duration:.2f} seconds")
            sys.stdout.flush()
            
            # –û–±—Ä–æ–±–∫–∞ –∑–∞–ª–µ–∂–Ω–æ –≤—ñ–¥ —Ä–µ–∂–∏–º—É
            if processing_mode == 'smart':
                # Smart mode: Speechmatics (—Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—è + –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—è)
                print(f"üéØ [Diarize & Transcribe] Using Smart mode: Speechmatics")
                sys.stdout.flush()
                
                transcription_text, transcription_segments, words = transcribe_with_speechmatics(temp_path, language='en')
                
                # Speechmatics –≤–∂–µ –º—ñ—Å—Ç–∏—Ç—å –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—é –≤ words
                # –°—Ç–≤–æ—Ä—é—î–º–æ —Å–µ–≥–º–µ–Ω—Ç–∏ –∑ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—î—é –∑—ñ —Å–ª—ñ–≤
                diarization_segments = []
                current_speaker = None
                current_start = None
                current_end = None
                current_text = []
                
                for word in words:
                    word_speaker = word.get('speaker', 0)
                    word_start = word.get('start', 0)
                    word_end = word.get('end', 0)
                    word_text = word.get('word', '')
                    
                    if current_speaker is None:
                        current_speaker = word_speaker
                        current_start = word_start
                        current_text = [word_text]
                    elif word_speaker == current_speaker:
                        current_text.append(word_text)
                    else:
                        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π —Å–µ–≥–º–µ–Ω—Ç
                        if current_start is not None and current_end is not None:
                            diarization_segments.append({
                                'speaker': current_speaker,
                                'start': round(current_start, 2),
                                'end': round(current_end, 2),
                                'text': ' '.join(current_text)
                            })
                        # –ü–æ—á–∏–Ω–∞—î–º–æ –Ω–æ–≤–∏–π —Å–µ–≥–º–µ–Ω—Ç
                        current_speaker = word_speaker
                        current_start = word_start
                        current_text = [word_text]
                    
                    current_end = word_end
                
                # –î–æ–¥–∞—î–º–æ –æ—Å—Ç–∞–Ω–Ω—ñ–π —Å–µ–≥–º–µ–Ω—Ç
                if current_speaker is not None and current_start is not None and current_end is not None:
                    diarization_segments.append({
                        'speaker': current_speaker,
                        'start': round(current_start, 2),
                        'end': round(current_end, 2),
                        'text': ' '.join(current_text)
                    })
                
                print(f"‚úÖ [Diarize & Transcribe] Speechmatics: Found {len(diarization_segments)} segments")
                sys.stdout.flush()
                
                # –î–ª—è Smart —Ä–µ–∂–∏–º—É combined_segments –≤–∂–µ –º—ñ—Å—Ç–∏—Ç—å —Ç–µ–∫—Å—Ç
                combined_segments = diarization_segments
            else:
                # Fast mode: Whisper + PyAnnote
                print(f"‚ö° [Diarize & Transcribe] Using Fast mode: Whisper + PyAnnote")
                sys.stdout.flush()
                
                # –ö—Ä–æ–∫ 2: –í–∏–∫–æ–Ω—É—î–º–æ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—é
                print(f"üîç [Diarize & Transcribe] Step 2: Performing speaker diarization...")
                sys.stdout.flush()
                
                # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ SpeechBrain –¥–ª—è –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó
                embeddings, timestamps = extract_speaker_embeddings(
                    temp_path,
                    segment_duration=1.5,
                    overlap=0.5
                )
                
                if embeddings is None or len(embeddings) == 0:
                    raise ValueError("Failed to extract speaker embeddings")
                
                # –í–∏–∫–æ–Ω—É—î–º–æ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—é
                diarization_segments = diarize_audio(embeddings, timestamps, num_speakers=num_speakers)
                
                if not diarization_segments:
                    raise ValueError("Diarization failed - no segments found")
                
                print(f"‚úÖ [Diarize & Transcribe] Found {len(diarization_segments)} diarization segments")
                sys.stdout.flush()
                
                # –ö—Ä–æ–∫ 3: –¢—Ä–∞–Ω—Å–∫—Ä–∏–±—É—î–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–µ –∞—É–¥—ñ–æ (–∂–æ—Ä—Å—Ç–∫–æ –∑–∞–¥–∞—î–º–æ –∞–Ω–≥–ª—ñ–π—Å—å–∫—É –º–æ–≤—É)
                print(f"üìù [Diarize & Transcribe] Step 3: Transcribing audio (language: en)...")
                sys.stdout.flush()
                
                transcription_text, transcription_segments, words = transcribe_audio(
                    temp_path,  # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–µ –∞—É–¥—ñ–æ –±–µ–∑ noise gate
                    language='en',  # –ñ–æ—Ä—Å—Ç–∫–æ –∑–∞–¥–∞—î–º–æ –∞–Ω–≥–ª—ñ–π—Å—å–∫—É –º–æ–≤—É
                    transcription_provider=transcription_provider
                )
                
                if not words:
                    raise ValueError("Transcription failed - no words found")
                
                print(f"‚úÖ [Diarize & Transcribe] Transcribed {len(words)} words")
                sys.stdout.flush()
                
                # –ö—Ä–æ–∫ 4: –û–±'—î–¥–Ω—É—î–º–æ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—é –∑ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—î—é (—Ç—ñ–ª—å–∫–∏ –¥–ª—è Fast —Ä–µ–∂–∏–º—É)
                print(f"üîó [Diarize & Transcribe] Step 4: Combining diarization with transcription...")
                sys.stdout.flush()
                
                # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –ø—Ä–æ—Å—Ç–∏–π —Å–ø–æ—Å—ñ–± –æ–±'—î–¥–Ω–∞–Ω–Ω—è (–±–µ–∑ LLM –¥–ª—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ)
                # –í–ê–ñ–õ–ò–í–û: –í—ñ–¥—Å—Ç–µ–∂—É—î–º–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω—ñ —Å–ª–æ–≤–∞, —â–æ–± —É–Ω–∏–∫–Ω—É—Ç–∏ –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤
                used_word_indices = set()
                combined_segments = []
                
                # –°–æ—Ä—Ç—É—î–º–æ —Å–µ–≥–º–µ–Ω—Ç–∏ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó –∑–∞ —á–∞—Å–æ–º –ø–æ—á–∞—Ç–∫—É
                sorted_diar_segments = sorted(diarization_segments, key=lambda x: x['start'])
                
                for diar_seg in sorted_diar_segments:
                    # –ó–Ω–∞—Ö–æ–¥–∏–º–æ —Å–ª–æ–≤–∞, —è–∫—ñ –ø–æ—Ç—Ä–∞–ø–ª—è—é—Ç—å –≤ —Ü–µ–π —Å–µ–≥–º–µ–Ω—Ç —ñ —â–µ –Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω—ñ
                    segment_words = []
                for word_idx, word in enumerate(words):
                    # –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –≤–∂–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω—ñ —Å–ª–æ–≤–∞
                    if word_idx in used_word_indices:
                        continue
                    
                    word_start = word.get('start', 0)
                    word_end = word.get('end', 0)
                    word_center = (word_start + word_end) / 2.0
                    
                    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —Å–ª–æ–≤–æ –ø–æ—Ç—Ä–∞–ø–ª—è—î –≤ —Å–µ–≥–º–µ–Ω—Ç (–ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ü–µ–Ω—Ç—Ä —Å–ª–æ–≤–∞)
                    # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –º'—è–∫—É —É–º–æ–≤—É: —è–∫—â–æ —Ü–µ–Ω—Ç—Ä —Å–ª–æ–≤–∞ –≤ –º–µ–∂–∞—Ö —Å–µ–≥–º–µ–Ω—Ç—É –∞–±–æ —Å–ª–æ–≤–æ —á–∞—Å—Ç–∫–æ–≤–æ –ø–µ—Ä–µ—Ç–∏–Ω–∞—î—Ç—å—Å—è
                    if (word_center >= diar_seg['start'] and word_center <= diar_seg['end']) or \
                       (word_start < diar_seg['end'] and word_end > diar_seg['start']):
                        segment_words.append((word_idx, word.get('word', '')))
                
                    # –Ø–∫—â–æ –∑–Ω–∞–π—à–ª–∏ —Å–ª–æ–≤–∞ –¥–ª—è —Ü—å–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç—É, –¥–æ–¥–∞—î–º–æ —ó—Ö
                    if segment_words:
                        text = ' '.join([w[1] for w in segment_words]).strip()
                        if text:
                            # –ü–æ–∑–Ω–∞—á–∞—î–º–æ —Å–ª–æ–≤–∞ —è–∫ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω—ñ
                            for word_idx, _ in segment_words:
                                used_word_indices.add(word_idx)
                            
                            combined_segments.append({
                                'speaker': diar_seg['speaker'],
                                'start': diar_seg['start'],
                                'end': diar_seg['end'],
                                'text': text
                            })
                
                # –î–æ–¥–∞—î–º–æ —Å–ª–æ–≤–∞, —è–∫—ñ –Ω–µ –ø–æ—Ç—Ä–∞–ø–∏–ª–∏ –≤ –∂–æ–¥–µ–Ω —Å–µ–≥–º–µ–Ω—Ç –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó
                # (–º–æ–∂–µ —Å—Ç–∞—Ç–∏—Å—è, —è–∫—â–æ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—è –Ω–µ –ø–æ–∫—Ä–∏–≤–∞—î –≤–µ—Å—å —á–∞—Å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó)
                unused_words = []
                for word_idx, word in enumerate(words):
                    if word_idx not in used_word_indices:
                        word_text = word.get('word', '').strip()
                        if word_text:
                            unused_words.append((word_idx, word))
                
                if unused_words:
                    print(f"‚ö†Ô∏è  [Diarize & Transcribe] Found {len(unused_words)} words not assigned to any segment, adding them...")
                    sys.stdout.flush()
                    
                    # –ì—Ä—É–ø—É—î–º–æ –Ω–µ–≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω—ñ —Å–ª–æ–≤–∞ –∑–∞ —á–∞—Å–æ–º (—Å–µ–≥–º–µ–Ω—Ç–∏ –ø–æ 1 —Å–µ–∫—É–Ω–¥—ñ)
                    unused_words_sorted = sorted(unused_words, key=lambda x: x[1].get('start', 0))
                    current_group = []
                    current_start = None
                    
                    for word_idx, word in unused_words_sorted:
                        word_start = word.get('start', 0)
                        
                        if current_start is None:
                            current_start = word_start
                            current_group = [(word_idx, word)]
                        elif word_start - current_start < 1.0:  # –ì—Ä—É–ø—É—î–º–æ —Å–ª–æ–≤–∞ –≤ –º–µ–∂–∞—Ö 1 —Å–µ–∫—É–Ω–¥–∏
                            current_group.append((word_idx, word))
                        else:
                            # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –ø–æ—Ç–æ—á–Ω—É –≥—Ä—É–ø—É
                            if current_group:
                                text = ' '.join([w[1].get('word', '') for w in current_group]).strip()
                                if text:
                                    # –í–∏–∑–Ω–∞—á–∞—î–º–æ —Å–ø—ñ–∫–µ—Ä–∞ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –Ω–∞–π–±–ª–∏–∂—á–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç—É –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó
                                    speaker = 0
                                    if sorted_diar_segments:
                                        # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –Ω–∞–π–±–ª–∏–∂—á–∏–π —Å–µ–≥–º–µ–Ω—Ç
                                        min_dist = float('inf')
                                        for seg in sorted_diar_segments:
                                            seg_center = (seg['start'] + seg['end']) / 2.0
                                            dist = abs(current_start - seg_center)
                                            if dist < min_dist:
                                                min_dist = dist
                                                speaker = seg['speaker']
                                    
                                    combined_segments.append({
                                        'speaker': speaker,
                                        'start': round(current_start, 2),
                                        'end': round(current_group[-1][1].get('end', current_start), 2),
                                        'text': text
                                    })
                            
                            # –ü–æ—á–∏–Ω–∞—î–º–æ –Ω–æ–≤—É –≥—Ä—É–ø—É
                            current_start = word_start
                            current_group = [(word_idx, word)]
                    
                    # –î–æ–¥–∞—î–º–æ –æ—Å—Ç–∞–Ω–Ω—é –≥—Ä—É–ø—É
                    if current_group:
                        text = ' '.join([w[1].get('word', '') for w in current_group]).strip()
                        if text:
                            speaker = 0
                            if sorted_diar_segments:
                                min_dist = float('inf')
                                for seg in sorted_diar_segments:
                                    seg_center = (seg['start'] + seg['end']) / 2.0
                                    dist = abs(current_start - seg_center)
                                    if dist < min_dist:
                                        min_dist = dist
                                        speaker = seg['speaker']
                            
                            combined_segments.append({
                                'speaker': speaker,
                                'start': round(current_start, 2),
                                'end': round(current_group[-1][1].get('end', current_start), 2),
                                'text': text
                            })
                
                # –î–æ–¥–∞—Ç–∫–æ–≤–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞: –≤–∏–¥–∞–ª—è—î–º–æ –¥—É–±–ª—ñ–∫–∞—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤—ñ —Ç–µ–∫—Å—Ç—É —Ç–∞ —á–∞—Å—É
                # –ê–ª–µ —Ç—ñ–ª—å–∫–∏ –¥–ª—è —ñ–¥–µ–Ω—Ç–∏—á–Ω–∏—Ö —Ç–µ–∫—Å—Ç—ñ–≤ –∑ –¥—É–∂–µ –±–ª–∏–∑—å–∫–∏–º —á–∞—Å–æ–º (<1 —Å–µ–∫)
                unique_segments = []
                seen_exact = set()
                for seg in combined_segments:
                    text_key = seg['text'].strip().lower()
                    time_key = int(seg['start'])
                    exact_key = (text_key, time_key)
                    
                    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ç—ñ–ª—å–∫–∏ –Ω–∞ —Ç–æ—á–Ω—ñ –¥—É–±–ª—ñ–∫–∞—Ç–∏ (—ñ–¥–µ–Ω—Ç–∏—á–Ω–∏–π —Ç–µ–∫—Å—Ç + —Ç–æ–π —Å–∞–º–∏–π —á–∞—Å)
                    if exact_key not in seen_exact:
                        unique_segments.append(seg)
                        seen_exact.add(exact_key)
                
                combined_segments = unique_segments
                
                print(f"‚úÖ [Diarize & Transcribe] Combined {len(combined_segments)} segments (after deduplication)")
                sys.stdout.flush()
            
            # –ö—Ä–æ–∫ 5: –§–æ—Ä–º–∞—Ç—É—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç (–¥–ª—è –æ–±–æ—Ö —Ä–µ–∂–∏–º—ñ–≤)
            print(f"üìã [Diarize & Transcribe] Step 5: Formatting transcript...")
            sys.stdout.flush()
            
            transcript_lines = []
            for seg in combined_segments:
                start_time = seg['start']
                minutes = int(start_time // 60)
                seconds = int(start_time % 60)
                timestamp = f"{minutes:02d}:{seconds:02d}"
                speaker_num = seg['speaker']
                text = seg['text']
                
                transcript_lines.append(f"{timestamp} - –°–ø—ñ–∫–µ—Ä {speaker_num} - {text}")
            
            # –í–∏–¥–∞–ª—è—î–º–æ —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª
            try:
                os.remove(temp_path)
            except:
                pass
            
            # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            response_data = {
                'success': True,
                'transcript': transcript_lines
            }
            
            print(f"üì§ [Diarize & Transcribe] Returning transcript with {len(transcript_lines)} lines")
            sys.stdout.flush()
            
            response = jsonify(response_data)
            response.headers.add('Access-Control-Allow-Origin', '*')
            return response, 200
            
        except Exception as processing_error:
            # –í–∏–¥–∞–ª—è—î–º–æ —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª –ø—Ä–∏ –ø–æ–º–∏–ª—Ü—ñ
            try:
                os.remove(temp_path)
            except:
                pass
            raise processing_error
        
    except Exception as e:
        print(f"‚ùå [Diarize & Transcribe] Error: {e}")
        import traceback
        traceback.print_exc()
        sys.stdout.flush()
        return jsonify({
            'success': False,
            'error': str(e),
            'code': 'PROCESSING_ERROR'
        }), 500


@app.route('/api/single-speaker-audio/<job_id>/<int:speaker_id>', methods=['GET', 'OPTIONS'])
def get_single_speaker_audio(job_id, speaker_id):
    """
    –ï–Ω–¥–ø–æ—ñ–Ω—Ç –¥–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –æ–¥–Ω–æ–≥–æ–ª–æ—Å–æ–≥–æ –∞—É–¥—ñ–æ —Ñ–∞–π–ª—É.
    –ü—ñ—Å–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ñ–∞–π–ª –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≤–∏–¥–∞–ª—è—î—Ç—å—Å—è.
    
    Args:
        job_id: ID –∑–∞–≤–¥–∞–Ω–Ω—è –æ–±—Ä–æ–±–∫–∏ –æ–¥–Ω–æ–≥–æ–ª–æ—Å–∏—Ö —Ñ–∞–π–ª—ñ–≤
        speaker_id: ID —Å–ø—ñ–∫–µ—Ä–∞ (0, 1, —Ç–æ—â–æ)
    """
    import sys
    
    # –û–±—Ä–æ–±–∫–∞ OPTIONS –¥–ª—è preflight –∑–∞–ø–∏—Ç—ñ–≤ (CORS)
    if request.method == 'OPTIONS':
        response = jsonify({})
        response.headers.add('Access-Control-Allow-Origin', '*')
        response.headers.add('Access-Control-Allow-Headers', 'Content-Type')
        response.headers.add('Access-Control-Allow-Methods', 'GET, OPTIONS')
        return response
    
    try:
        # –ó–Ω–∞—Ö–æ–¥–∏–º–æ —Ñ–∞–π–ª
        download_dir = os.path.join(UPLOAD_FOLDER, 'single_speaker_audio')
        file_extension = '.wav'  # –ó–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º
        download_filename = f"{job_id}_speaker_{speaker_id}{file_extension}"
        download_path = os.path.join(download_dir, download_filename)
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —Ñ–∞–π–ª —ñ—Å–Ω—É—î
        if not os.path.exists(download_path):
            # –°–ø—Ä–æ–±—É—î–º–æ –∑–Ω–∞–π—Ç–∏ —Ñ–∞–π–ª –∑ —ñ–Ω—à–∏–º —Ä–æ–∑—à–∏—Ä–µ–Ω–Ω—è–º
            found = False
            for ext in ['.wav', '.m4a', '.mp3', '.flac']:
                alt_path = os.path.join(download_dir, f"{job_id}_speaker_{speaker_id}{ext}")
                if os.path.exists(alt_path):
                    download_path = alt_path
                    download_filename = f"{job_id}_speaker_{speaker_id}{ext}"
                    found = True
                    break
            
            if not found:
                print(f"‚ùå [Audio Download] File not found: {download_path}")
                sys.stdout.flush()
                return jsonify({
                    'success': False,
                    'error': f'Audio file for speaker {speaker_id} not found',
                    'code': 'FILE_NOT_FOUND'
                }), 404
        
        print(f"üì• [Audio Download] Serving file: {download_path} for job {job_id}, speaker {speaker_id}")
        sys.stdout.flush()
        
        # –í–∏–∑–Ω–∞—á–∞—î–º–æ MIME type –Ω–∞ –æ—Å–Ω–æ–≤—ñ —Ä–æ–∑—à–∏—Ä–µ–Ω–Ω—è
        mime_types = {
            '.wav': 'audio/wav',
            '.m4a': 'audio/m4a',
            '.mp3': 'audio/mpeg',
            '.flac': 'audio/flac'
        }
        file_ext = os.path.splitext(download_path)[1].lower()
        mime_type = mime_types.get(file_ext, 'audio/wav')
        
        # –í—ñ–¥–ø—Ä–∞–≤–ª—è—î–º–æ —Ñ–∞–π–ª
        response = send_file(
            download_path,
            mimetype=mime_type,
            as_attachment=True,
            download_name=f"speaker_{speaker_id}{file_ext}"
        )
        response.headers.add('Access-Control-Allow-Origin', '*')
        
        # –í–∏–¥–∞–ª—è—î–º–æ —Ñ–∞–π–ª –ø—ñ—Å–ª—è –≤—ñ–¥–ø—Ä–∞–≤–∫–∏ (–≤ —Ñ–æ–Ω—ñ, —â–æ–± –Ω–µ –±–ª–æ–∫—É–≤–∞—Ç–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥—å)
        def delete_file_after_delay():
            import time
            time.sleep(2)  # –ó–∞—Ç—Ä–∏–º–∫–∞, —â–æ–± —Ñ–∞–π–ª —Ç–æ—á–Ω–æ –≤—ñ–¥–ø—Ä–∞–≤–∏–≤—Å—è
            try:
                if os.path.exists(download_path):
                    os.remove(download_path)
                    print(f"üóëÔ∏è [Audio Download] Deleted file: {download_path}")
                    sys.stdout.flush()
            except Exception as e:
                print(f"‚ö†Ô∏è [Audio Download] Failed to delete file {download_path}: {e}")
                sys.stdout.flush()
        
        # –ó–∞–ø—É—Å–∫–∞—î–º–æ –≤–∏–¥–∞–ª–µ–Ω–Ω—è –≤ –æ–∫—Ä–µ–º–æ–º—É –ø–æ—Ç–æ—Ü—ñ
        import threading
        delete_thread = threading.Thread(target=delete_file_after_delay)
        delete_thread.daemon = True
        delete_thread.start()
        
        return response
        
    except Exception as e:
        print(f"‚ùå [Audio Download] Error: {e}")
        import traceback
        traceback.print_exc()
        sys.stdout.flush()
        return jsonify({
            'success': False,
            'error': str(e),
            'code': 'DOWNLOAD_ERROR'
        }), 500


def enhance_main_speaker_audio(audio_path, suppression_factor=0.1, num_speakers=None, llm_mode='local', transcription_provider='whisper'):
    """
    –í–∏–¥—ñ–ª—è—î –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ –≤ –∞—É–¥—ñ–æ, –ø—Ä–∏–≥–ª—É—à—É—é—á–∏ —ñ–Ω—à–∏—Ö —Å–ø—ñ–∫–µ—Ä—ñ–≤.
    
    Args:
        audio_path: —à–ª—è—Ö –¥–æ –≤—Ö—ñ–¥–Ω–æ–≥–æ –∞—É–¥—ñ–æ—Ñ–∞–π–ª—É
        suppression_factor: –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç –ø—Ä–∏–≥–ª—É—à–µ–Ω–Ω—è (0.0 = –ø–æ–≤–Ω–µ –≤–∏–¥–∞–ª–µ–Ω–Ω—è, 1.0 = –±–µ–∑ –∑–º—ñ–Ω)
        num_speakers: –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Å–ø—ñ–∫–µ—Ä—ñ–≤ (None = –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è)
        llm_mode: –†–µ–∂–∏–º LLM –¥–ª—è –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è –ø—Ä–∏–∑–Ω–∞—á–µ–Ω—å —Å–ø—ñ–∫–µ—Ä—ñ–≤ ('local', 'fast', 'smart', 'smart-2')
        suppression_factor: –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç –ø—Ä–∏–≥–ª—É—à–µ–Ω–Ω—è –¥–ª—è –Ω–µ–æ—Å–Ω–æ–≤–Ω–∏—Ö —Å–ø—ñ–∫–µ—Ä—ñ–≤ (0.0-1.0, –¥–µ 0.0 = –ø–æ–≤–Ω–µ –≤–∏–¥–∞–ª–µ–Ω–Ω—è, 1.0 = –±–µ–∑ –∑–º—ñ–Ω)
        num_speakers: –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Å–ø—ñ–∫–µ—Ä—ñ–≤ (None –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ–≥–æ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è)
    
    Returns:
        output_path: —à–ª—è—Ö –¥–æ –æ–±—Ä–æ–±–ª–µ–Ω–æ–≥–æ –∞—É–¥—ñ–æ—Ñ–∞–π–ª—É
        main_speaker: ID –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
        segments_info: —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ —Å–µ–≥–º–µ–Ω—Ç–∏ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó
    """
    import sys
    import shutil
    
    print(f"üéØ Starting main speaker enhancement for: {audio_path}")
    sys.stdout.flush()
    
    try:
        # –ö—Ä–æ–∫ 1: –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –∞—É–¥—ñ–æ
        print(f"üìÇ Step 1: Loading audio...")
        sys.stdout.flush()
        audio, sr = librosa.load(audio_path, sr=16000, mono=True)
        duration = librosa.get_duration(y=audio, sr=sr)
        print(f"‚è±Ô∏è  Audio duration: {duration:.2f} seconds, sample rate: {sr} Hz")
        sys.stdout.flush()
        
        # –ö—Ä–æ–∫ 2: –í–∏–∫–æ–Ω—É—î–º–æ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—é
        print(f"üîç Step 2: Performing speaker diarization...")
        sys.stdout.flush()
        
        # –°–ø—Ä–æ–±—É—î–º–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ pyannote –¥–ª—è –±—ñ–ª—å—à —Ç–æ—á–Ω–æ—ó –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó (—è–∫—â–æ –¥–æ—Å—Ç—É–ø–Ω–∞)
        diarization_segments = None
        use_pyannote = os.getenv('HUGGINGFACE_TOKEN') is not None
        
        if use_pyannote:
            try:
                print(f"üéØ Attempting to use PyAnnote for more accurate diarization...")
                sys.stdout.flush()
                
                # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ç–æ–π —Å–∞–º–∏–π –ø—ñ–¥—Ö—ñ–¥, —â–æ —ñ –≤ pyannote_separation.py
                try:
                    import pyannote_patch  # noqa: F401
                    from pyannote.audio import Pipeline
                    import torch
                    import torchaudio
                except ImportError:
                    raise ImportError("pyannote.audio not available")
                
                hf_token = os.getenv('HUGGINGFACE_TOKEN')
                device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
                
                print(f"üì¶ Loading PyAnnote speaker-diarization-3.1 pipeline...")
                sys.stdout.flush()
                
                try:
                    # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ç–æ–π —Å–∞–º–∏–π –ø—ñ–¥—Ö—ñ–¥, —â–æ —ñ –≤ pyannote_separation.py
                    pipeline = Pipeline.from_pretrained(
                        "pyannote/speaker-diarization-3.1",
                        use_auth_token=hf_token
                    )
                    
                    if pipeline is None:
                        raise ValueError("Pipeline is None after loading")
                    pipeline.to(device)
                except Exception as load_error:
                    print(f"‚ö†Ô∏è  Failed to load PyAnnote pipeline: {load_error}")
                    # –í–∏–∫–∏–¥–∞—î–º–æ –ø–æ–º–∏–ª–∫—É, —â–æ–± –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ SpeechBrain —è–∫ fallback
                    raise
                
                print(f"‚úÖ PyAnnote pipeline loaded, running diarization on: {audio_path}")
                sys.stdout.flush()
                
                # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –∞—É–¥—ñ–æ —Ç–∞–∫ —Å–∞–º–æ, —è–∫ –≤ pyannote_separation.py
                # soundfile –≤–∂–µ —ñ–º–ø–æ—Ä—Ç–æ–≤–∞–Ω–æ –Ω–∞ –ø–æ—á–∞—Ç–∫—É —Ñ–∞–π–ª—É
                try:
                    data, sample_rate = sf.read(audio_path, dtype='float32')
                    if len(data.shape) == 1:
                        waveform = torch.from_numpy(data).unsqueeze(0).float()
                    else:
                        waveform = torch.from_numpy(data).transpose(0, 1).float()
                except Exception as load_error:
                    # Fallback –¥–æ torchaudio —è–∫—â–æ soundfile –Ω–µ –ø—Ä–∞—Ü—é—î
                    print(f"‚ö†Ô∏è  soundfile failed: {load_error}, trying torchaudio...")
                    sys.stdout.flush()
                    waveform, sample_rate = torchaudio.load(audio_path)
                
                # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ –≤ mono —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ
                if waveform.shape[0] > 1:
                    waveform = torch.mean(waveform, dim=0, keepdim=True)
                
                # Resample –¥–æ 16kHz —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ
                if sample_rate != 16000:
                    resampler = torchaudio.transforms.Resample(sample_rate, 16000)
                    waveform = resampler(waveform)
                    sample_rate = 16000
                
                # –ó–∞–ø—É—Å–∫–∞—î–º–æ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—é
                diarization = pipeline({
                    "waveform": waveform,
                    "sample_rate": sample_rate
                })
                
                # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç pyannote –≤ –Ω–∞—à —Ñ–æ—Ä–º–∞—Ç
                diarization_segments = []
                speaker_map = {}  # –ú–∞–ø—ñ–Ω–≥ pyannote labels –¥–æ —á–∏—Å–ª–æ–≤–∏—Ö ID
                next_speaker_id = 0
                
                # –°–ø–æ—á–∞—Ç–∫—É –∑–±–∏—Ä–∞—î–º–æ –≤—Å—ñ —É–Ω—ñ–∫–∞–ª—å–Ω—ñ —Å–ø—ñ–∫–µ—Ä—ñ–≤
                for turn, _, speaker in diarization.itertracks(yield_label=True):
                    if speaker not in speaker_map:
                        speaker_map[speaker] = next_speaker_id
                        next_speaker_id += 1
                
                # –¢–µ–ø–µ—Ä —Å—Ç–≤–æ—Ä—é—î–º–æ —Å–µ–≥–º–µ–Ω—Ç–∏
                for turn, _, speaker in diarization.itertracks(yield_label=True):
                    speaker_id = speaker_map[speaker]
                    diarization_segments.append({
                        'speaker': speaker_id,
                        'start': round(turn.start, 2),
                        'end': round(turn.end, 2)
                    })
                
                # –°–æ—Ä—Ç—É—î–º–æ –∑–∞ —á–∞—Å–æ–º
                diarization_segments.sort(key=lambda x: x['start'])
                
                print(f"‚úÖ PyAnnote found {len(diarization_segments)} segments from {len(speaker_map)} speakers")
                print(f"   Speaker mapping: {speaker_map}")
                sys.stdout.flush()
                
            except Exception as e:
                print(f"‚ö†Ô∏è  PyAnnote diarization failed: {e}")
                import traceback
                traceback.print_exc()
                sys.stdout.flush()
                diarization_segments = None
        
        # –Ø–∫—â–æ pyannote –Ω–µ —Å–ø—Ä–∞—Ü—é–≤–∞–ª–∞, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ SpeechBrain
        if diarization_segments is None:
            print(f"üìä Using SpeechBrain diarization...")
            sys.stdout.flush()
            
            # –í–∏—Ç—è–≥—É—î–º–æ –µ–º–±–µ–¥–¥–∏–Ω–≥–∏
            embeddings, timestamps = extract_speaker_embeddings(
                audio_path,
                segment_duration=1.5,
                overlap=0.5
            )
            
            if embeddings is None or len(embeddings) == 0:
                raise ValueError("Failed to extract speaker embeddings")
            
            # –í–∏–∫–æ–Ω—É—î–º–æ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—é
            diarization_segments = diarize_audio(embeddings, timestamps, num_speakers=num_speakers)
            
            if not diarization_segments:
                raise ValueError("Diarization failed - no segments found")
        
        print(f"‚úÖ Found {len(diarization_segments)} diarization segments")
        sys.stdout.flush()
        
        # –ö—Ä–æ–∫ 3: –¢—Ä–∞–Ω—Å–∫—Ä–∏–±—É—î–º–æ –∞—É–¥—ñ–æ —Ç–∞ –æ–±'—î–¥–Ω—É—î–º–æ –∑ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—î—é
        # –ö–†–ò–¢–ò–ß–ù–û: –°–ø–æ—á–∞—Ç–∫—É –æ–±'—î–¥–Ω—É—î–º–æ, –ø–æ—Ç—ñ–º –≤–∏–∑–Ω–∞—á–∞—î–º–æ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –æ–±'—î–¥–Ω–∞–Ω–æ—ó —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó
        print(f"üìù Step 3: Transcribing and combining with diarization...")
        sys.stdout.flush()
        
        transcription_text, transcription_segments, words = transcribe_audio(audio_path, language=None, transcription_provider=transcription_provider)
        
        # –û–±'—î–¥–Ω—É—î–º–æ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—é –∑ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—î—é
        combined_segments = combine_diarization_and_transcription(diarization_segments, words, llm_mode=llm_mode)
        
        print(f"‚úÖ Combined {len(combined_segments)} segments from transcription and diarization")
        sys.stdout.flush()
        
        # –ö—Ä–æ–∫ 4: –í–∏–∑–Ω–∞—á–∞—î–º–æ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –û–ë'–Ñ–î–ù–ê–ù–û–á —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó
        print(f"üë§ Step 4: Determining main speaker from combined transcription...")
        sys.stdout.flush()
        
        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —É–Ω—ñ—Ñ—ñ–∫–æ–≤–∞–Ω—É —Ñ—É–Ω–∫—Ü—ñ—é –¥–ª—è –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
        main_speaker, speaker_stats = determine_main_speaker_from_segments(combined_segments, duration=duration)
        
        # –û—Ç—Ä–∏–º—É—î–º–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–ª—è –ø–æ–¥–∞–ª—å—à–æ–≥–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è
        speaker_durations = {spk: stats['duration'] for spk, stats in speaker_stats.items()}
        main_duration = speaker_stats[main_speaker]['duration'] if main_speaker in speaker_stats else 0
        
        # –ö—Ä–æ–∫ 5: –°—Ç–≤–æ—Ä—é—î–º–æ –º–∞—Å–∫—É –¥–ª—è –∞—É–¥—ñ–æ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –û–ë'–Ñ–î–ù–ê–ù–û–á —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó
        print(f"üéöÔ∏è  Step 5: Creating audio mask (suppression factor: {suppression_factor})...")
        sys.stdout.flush()
        
        num_samples = len(audio)
        
        # –î—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞: –ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –Ω–∞—è–≤–Ω—ñ—Å—Ç—å combined_segments
        if not combined_segments:
            print(f"‚ö†Ô∏è  WARNING: combined_segments is empty! Cannot create mask.")
            sys.stdout.flush()
            # –Ø–∫—â–æ –Ω–µ–º–∞—î —Å–µ–≥–º–µ–Ω—Ç—ñ–≤, —Å—Ç–≤–æ—Ä—é—î–º–æ –º–∞—Å–∫—É –±–µ–∑ –∑–º—ñ–Ω (1.0 –¥–ª—è –≤—Å—å–æ–≥–æ)
            mask = np.ones(num_samples, dtype=np.float32)
            enhanced_audio = audio * mask
            print(f"‚ö†Ô∏è  No mask applied - using original audio")
            sys.stdout.flush()
        else:
            print(f"üìä Using {len(combined_segments)} segments for mask creation, main_speaker={main_speaker}")
            sys.stdout.flush()
            
            # –Ø–∫—â–æ suppression_factor = 0, –ø–æ–≤–Ω—ñ—Å—Ç—é –≤–∏–¥–∞–ª—è—î–º–æ –∑–≤—É–∫ —ñ–Ω—à–∏—Ö —Å–ø—ñ–∫–µ—Ä—ñ–≤ —á–µ—Ä–µ–∑ –º–∞—Å–∫—É
            if suppression_factor == 0.0:
                print(f"üîá Suppression factor is 0.0 - completely removing other speakers using COMBINED transcription timestamps...")
                sys.stdout.flush()
                
                # –°—Ç–≤–æ—Ä—é—î–º–æ –º–∞—Å–∫—É: 1.0 –¥–ª—è –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞, 0.0 –¥–ª—è —ñ–Ω—à–∏—Ö
                mask = np.zeros(num_samples, dtype=np.float32)
                
                # –ó–∞—Å—Ç–æ—Å–æ–≤—É—î–º–æ –º–∞—Å–∫—É –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞ –∑ –û–ë'–Ñ–î–ù–ê–ù–û–á —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó
                main_speaker_segments_count = 0
                other_speaker_segments_count = 0
                
                for seg_idx, seg in enumerate(combined_segments):
                    speaker = seg['speaker']
                    start_time = seg['start']
                    end_time = seg['end']
                    text = seg.get('text', '')[:50]
                    
                    start_sample = int(start_time * sr)
                    end_sample = int(end_time * sr)
                    
                    # –û–±–º–µ–∂—É—î–º–æ –º–µ–∂—ñ –º–∞—Å–∏–≤—É
                    start_sample = max(0, min(start_sample, num_samples))
                    end_sample = max(0, min(end_sample, num_samples))
                    
                    if start_sample < end_sample:
                        if speaker == main_speaker:
                            # –û—Å–Ω–æ–≤–Ω–∏–π —Å–ø—ñ–∫–µ—Ä - –∑–∞–ª–∏—à–∞—î–º–æ –∑–≤—É–∫ (1.0)
                            mask[start_sample:end_sample] = 1.0
                            main_speaker_segments_count += 1
                        else:
                            # –Ü–Ω—à—ñ —Å–ø—ñ–∫–µ—Ä–∏ - –ø–æ–≤–Ω—ñ—Å—Ç—é –≤–∏–¥–∞–ª—è—î–º–æ –∑–≤—É–∫ (0.0)
                            mask[start_sample:end_sample] = 0.0
                            other_speaker_segments_count += 1
                            # –î–µ—Ç–∞–ª—å–Ω–µ –ª–æ–≥—É–≤–∞–Ω–Ω—è –¥–ª—è –Ω–µ–æ—Å–Ω–æ–≤–Ω–∏—Ö —Å–ø—ñ–∫–µ—Ä—ñ–≤
                            print(f"   üîá [Mask] Segment {seg_idx}: Removing speaker {speaker} "
                                  f"({start_time:.2f}-{end_time:.2f}s, {end_time-start_time:.2f}s): '{text}...'")
                            sys.stdout.flush()
                
                print(f"üìä Mask created: {main_speaker_segments_count} segments of main speaker (kept), {other_speaker_segments_count} segments of other speakers (removed)")
                sys.stdout.flush()
                
                # –ó–∞—Å—Ç–æ—Å–æ–≤—É—î–º–æ –º–∞—Å–∫—É –¥–æ –∞—É–¥—ñ–æ
                # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ä–æ–∑–º—ñ—Ä–∏: —è–∫—â–æ –∞—É–¥—ñ–æ 2D (stereo), –º–∞—Å–∫–∞ –º–∞—î –±—É—Ç–∏ 2D —Ç–µ–∂
                print(f"üîç [Mask Debug] audio shape: {audio.shape}, mask shape: {mask.shape}")
                sys.stdout.flush()
                
                if len(audio.shape) == 2:
                    # Stereo –∞—É–¥—ñ–æ - –º–∞—Å–∫–∞ –º–∞—î –±—É—Ç–∏ 2D
                    mask_2d = mask[:, np.newaxis]  # –î–æ–¥–∞—î–º–æ –≤–∏–º—ñ—Ä –¥–ª—è –∫–∞–Ω–∞–ª—ñ–≤
                    enhanced_audio = audio * mask_2d
                    print(f"üîç [Mask Debug] Applied 2D mask to stereo audio")
                else:
                    # Mono –∞—É–¥—ñ–æ - –º–∞—Å–∫–∞ 1D
                    enhanced_audio = audio * mask
                    print(f"üîç [Mask Debug] Applied 1D mask to mono audio")
                sys.stdout.flush()
                
                # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ –º–∞—Å–∫–∞ –¥—ñ–π—Å–Ω–æ –∑–∞—Å—Ç–æ—Å—É–≤–∞–ª–∞—Å—è
                max_audio_before = np.max(np.abs(audio))
                max_audio_after = np.max(np.abs(enhanced_audio))
                print(f"üîç [Mask Debug] Max audio before mask: {max_audio_before:.6f}, after mask: {max_audio_after:.6f}")
                sys.stdout.flush()
                
                # –û–±—á–∏—Å–ª—é—î–º–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                main_speaker_duration_samples = np.sum(mask > 0)
                main_speaker_duration = main_speaker_duration_samples / sr
                print(f"‚úÖ Applied mask: main speaker audio kept ({main_speaker_duration:.2f}s), other speakers completely removed")
                sys.stdout.flush()
            else:
                # –°—Ç–≤–æ—Ä—é—î–º–æ –º–∞—Å–∏–≤ –º–∞—Å–æ–∫ (1.0 –¥–ª—è –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞, suppression_factor –¥–ª—è —ñ–Ω—à–∏—Ö)
                # –í–ê–ñ–õ–ò–í–û: –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î–º–æ —è–∫ suppression_factor, —â–æ–± –ø—Ä–æ–º—ñ–∂–∫–∏ –º—ñ–∂ —Å–µ–≥–º–µ–Ω—Ç–∞–º–∏ —Ç–µ–∂ –±—É–ª–∏ –ø—Ä–∏–≥–ª—É—à–µ–Ω—ñ
                mask = np.full(num_samples, suppression_factor, dtype=np.float32)
                
                # –ó–∞—Å—Ç–æ—Å–æ–≤—É—î–º–æ –º–∞—Å–∫—É –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞ –∑ –û–ë'–Ñ–î–ù–ê–ù–û–á —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó
                main_speaker_segments_count = 0
                other_speaker_segments_count = 0
                
                for seg_idx, seg in enumerate(combined_segments):
                    speaker = seg['speaker']
                    start_time = seg['start']
                    end_time = seg['end']
                    text = seg.get('text', '')[:50]
                    
                    start_sample = int(start_time * sr)
                    end_sample = int(end_time * sr)
                    
                    # –û–±–º–µ–∂—É—î–º–æ –º–µ–∂—ñ –º–∞—Å–∏–≤—É
                    start_sample = max(0, min(start_sample, num_samples))
                    end_sample = max(0, min(end_sample, num_samples))
                    
                    if start_sample < end_sample:
                        if speaker == main_speaker:
                            # –û—Å–Ω–æ–≤–Ω–∏–π —Å–ø—ñ–∫–µ—Ä - –∑–∞–ª–∏—à–∞—î–º–æ –±–µ–∑ –∑–º—ñ–Ω (1.0)
                            mask[start_sample:end_sample] = 1.0
                            main_speaker_segments_count += 1
                        else:
                            # –ù–µ–æ—Å–Ω–æ–≤–Ω–∏–π —Å–ø—ñ–∫–µ—Ä - –ø—Ä–∏–≥–ª—É—à—É—î–º–æ
                            mask[start_sample:end_sample] = suppression_factor
                            other_speaker_segments_count += 1
                            # –î–µ—Ç–∞–ª—å–Ω–µ –ª–æ–≥—É–≤–∞–Ω–Ω—è –¥–ª—è –Ω–µ–æ—Å–Ω–æ–≤–Ω–∏—Ö —Å–ø—ñ–∫–µ—Ä—ñ–≤
                            print(f"   üîá [Mask] Segment {seg_idx}: Suppressing speaker {speaker} "
                                  f"({start_time:.2f}-{end_time:.2f}s, {end_time-start_time:.2f}s): '{text}...'")
                            sys.stdout.flush()
                
                print(f"üìä Mask created: {main_speaker_segments_count} segments of main speaker (kept at 1.0), {other_speaker_segments_count} segments of other speakers (suppressed to {suppression_factor})")
                
                # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ –º–∞—Å–∫–∞ –¥—ñ–π—Å–Ω–æ –∑–∞—Å—Ç–æ—Å—É–≤–∞–ª–∞—Å—è –¥–æ —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ –Ω–µ–æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
                for seg in combined_segments:
                    if seg['speaker'] != main_speaker:
                        start_sample = int(seg['start'] * sr)
                        end_sample = int(seg['end'] * sr)
                        start_sample = max(0, min(start_sample, num_samples))
                        end_sample = max(0, min(end_sample, num_samples))
                        if start_sample < end_sample:
                            mask_values = mask[start_sample:end_sample]
                            avg_mask_value = np.mean(mask_values)
                            if abs(avg_mask_value - suppression_factor) > 0.01:
                                print(f"   ‚ö†Ô∏è [Mask Check] Segment '{seg.get('text', '')[:50]}...' "
                                      f"({seg['start']:.2f}-{seg['end']:.2f}s): "
                                      f"expected mask={suppression_factor}, actual={avg_mask_value:.3f}")
                            else:
                                print(f"   ‚úÖ [Mask Check] Segment '{seg.get('text', '')[:50]}...' "
                                      f"({seg['start']:.2f}-{seg['end']:.2f}s): "
                                      f"mask correctly applied={avg_mask_value:.3f}")
                sys.stdout.flush()
                
                # –ó–∞—Å—Ç–æ—Å–æ–≤—É—î–º–æ –º–∞—Å–∫—É –¥–æ –∞—É–¥—ñ–æ
                # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ä–æ–∑–º—ñ—Ä–∏: —è–∫—â–æ –∞—É–¥—ñ–æ 2D (stereo), –º–∞—Å–∫–∞ –º–∞—î –±—É—Ç–∏ 2D —Ç–µ–∂
                print(f"üîç [Mask Debug] audio shape: {audio.shape}, mask shape: {mask.shape}")
                sys.stdout.flush()
                
                if len(audio.shape) == 2:
                    # Stereo –∞—É–¥—ñ–æ - –º–∞—Å–∫–∞ –º–∞—î –±—É—Ç–∏ 2D
                    mask_2d = mask[:, np.newaxis]  # –î–æ–¥–∞—î–º–æ –≤–∏–º—ñ—Ä –¥–ª—è –∫–∞–Ω–∞–ª—ñ–≤
                    enhanced_audio = audio * mask_2d
                    print(f"üîç [Mask Debug] Applied 2D mask to stereo audio")
                else:
                    # Mono –∞—É–¥—ñ–æ - –º–∞—Å–∫–∞ 1D
                    enhanced_audio = audio * mask
                    print(f"üîç [Mask Debug] Applied 1D mask to mono audio")
                sys.stdout.flush()
                
                # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ –º–∞—Å–∫–∞ –¥—ñ–π—Å–Ω–æ –∑–∞—Å—Ç–æ—Å—É–≤–∞–ª–∞—Å—è
                max_audio_before = np.max(np.abs(audio))
                max_audio_after = np.max(np.abs(enhanced_audio))
                print(f"üîç [Mask Debug] Max audio before mask: {max_audio_before:.6f}, after mask: {max_audio_after:.6f}")
                
                # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ñ —Å–µ–≥–º–µ–Ω—Ç–∏ —Å–ø—ñ–∫–µ—Ä–∞ 1 - —á–∏ –≤–æ–Ω–∏ –¥—ñ–π—Å–Ω–æ –ø—Ä–∏–≥–ª—É—à–µ–Ω—ñ
                print(f"üîç [Audio Level Check] Checking audio levels for non-main speaker segments...")
                for seg in combined_segments:
                    if seg['speaker'] != main_speaker:
                        start_sample = int(seg['start'] * sr)
                        end_sample = int(seg['end'] * sr)
                        start_sample = max(0, min(start_sample, num_samples))
                        end_sample = max(0, min(end_sample, num_samples))
                        if start_sample < end_sample:
                            # –ü–æ—Ä—ñ–≤–Ω—é—î–º–æ –∞—É–¥—ñ–æ –¥–æ —Ç–∞ –ø—ñ—Å–ª—è –º–∞—Å–∫–∏ –¥–ª—è —Ü—å–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞
                            audio_before_seg = np.max(np.abs(audio[start_sample:end_sample]))
                            audio_after_seg = np.max(np.abs(enhanced_audio[start_sample:end_sample]))
                            expected_after = audio_before_seg * suppression_factor
                            ratio = audio_after_seg / audio_before_seg if audio_before_seg > 0 else 0
                            print(f"   üîä [Audio Check] Segment '{seg.get('text', '')[:40]}...' "
                                  f"({seg['start']:.2f}-{seg['end']:.2f}s): "
                                  f"before={audio_before_seg:.6f}, after={audio_after_seg:.6f}, "
                                  f"ratio={ratio:.3f} (expected ~{suppression_factor:.3f})")
                            if abs(ratio - suppression_factor) > 0.05:
                                print(f"      ‚ö†Ô∏è WARNING: Audio not properly suppressed! Ratio should be ~{suppression_factor:.3f}")
                sys.stdout.flush()
                
                # –û–±—á–∏—Å–ª—é—î–º–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                main_speaker_duration_samples = np.sum(mask == 1.0)
                main_speaker_duration = main_speaker_duration_samples / sr
                suppressed_duration_samples = np.sum((mask > 0) & (mask < 1.0))
                suppressed_duration = suppressed_duration_samples / sr
                print(f"‚úÖ Applied mask: main speaker audio kept ({main_speaker_duration:.2f}s), other speakers suppressed ({suppressed_duration:.2f}s at {suppression_factor*100:.0f}% volume)")
                sys.stdout.flush()
        
        # –ö—Ä–æ–∫ 6: –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –æ–±—Ä–æ–±–ª–µ–Ω–∏–π —Ñ–∞–π–ª
        print(f"üíæ Step 6: Saving enhanced audio...")
        sys.stdout.flush()
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ –≤–∏—Ö—ñ–¥–Ω–∏–π —Ñ–∞–π–ª
        output_dir = os.path.join(UPLOAD_FOLDER, 'enhanced')
        os.makedirs(output_dir, exist_ok=True)
        
        base_name = os.path.splitext(os.path.basename(audio_path))[0]
        output_path = os.path.join(output_dir, f"{base_name}_main_speaker.wav")
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ enhanced_audio –≤–∏–∑–Ω–∞—á–µ–Ω–æ
        if 'enhanced_audio' not in locals():
            print(f"‚ùå ERROR: enhanced_audio is not defined! Using original audio.")
            sys.stdout.flush()
            enhanced_audio = audio
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ä–æ–∑–º—ñ—Ä–∏ –ø–µ—Ä–µ–¥ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è–º
        print(f"üîç [Save Debug] enhanced_audio shape: {enhanced_audio.shape}, dtype: {enhanced_audio.dtype}")
        print(f"üîç [Save Debug] Max value in enhanced_audio: {np.max(np.abs(enhanced_audio)):.6f}")
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ enhanced_audio –¥—ñ–π—Å–Ω–æ –≤—ñ–¥—Ä—ñ–∑–Ω—è—î—Ç—å—Å—è –≤—ñ–¥ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–æ–≥–æ –∞—É–¥—ñ–æ
        audio_diff = np.max(np.abs(audio - enhanced_audio))
        print(f"üîç [Save Debug] Max difference between original and enhanced audio: {audio_diff:.6f}")
        if audio_diff < 0.001:
            print(f"‚ö†Ô∏è [Save Debug] WARNING: Enhanced audio is almost identical to original! Mask might not be applied correctly.")
        else:
            print(f"‚úÖ [Save Debug] Enhanced audio differs from original (difference: {audio_diff:.6f})")
        sys.stdout.flush()
        
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –æ–±—Ä–æ–±–ª–µ–Ω–∏–π –∞—É–¥—ñ–æ
        sf.write(output_path, enhanced_audio, sr)
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —Ñ–∞–π–ª –¥—ñ–π—Å–Ω–æ –∑–±–µ—Ä–µ–∂–µ–Ω–æ
        if os.path.exists(output_path):
            file_size = os.path.getsize(output_path)
            print(f"‚úÖ Enhanced audio saved to: {output_path} (size: {file_size} bytes)")
            
            # –î–û–î–ê–¢–ö–û–í–ê –ü–ï–†–ï–í–Ü–†–ö–ê: –ß–∏—Ç–∞—î–º–æ –∑–±–µ—Ä–µ–∂–µ–Ω–∏–π —Ñ–∞–π–ª —ñ –ø–æ—Ä—ñ–≤–Ω—é—î–º–æ –∑ enhanced_audio
            try:
                loaded_audio, loaded_sr = sf.read(output_path)
                print(f"üîç [File Verify] Loaded file: shape={loaded_audio.shape}, sr={loaded_sr}, max={np.max(np.abs(loaded_audio)):.6f}")
                
                # –ü–æ—Ä—ñ–≤–Ω—é—î–º–æ –∑ enhanced_audio
                if loaded_audio.shape == enhanced_audio.shape:
                    diff = np.max(np.abs(loaded_audio - enhanced_audio))
                    print(f"üîç [File Verify] Difference between saved and enhanced_audio: {diff:.6f}")
                    if diff > 0.001:
                        print(f"‚ö†Ô∏è [File Verify] WARNING: Saved file differs from enhanced_audio!")
                    else:
                        print(f"‚úÖ [File Verify] Saved file matches enhanced_audio")
                else:
                    print(f"‚ö†Ô∏è [File Verify] WARNING: Shape mismatch! saved={loaded_audio.shape}, enhanced={enhanced_audio.shape}")
            except Exception as e:
                print(f"‚ö†Ô∏è [File Verify] Could not verify saved file: {e}")
        else:
            print(f"‚ùå ERROR: File was not saved! Path: {output_path}")
        sys.stdout.flush()
        
        # –ö—Ä–æ–∫ 7: –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –≤–∂–µ –æ–±'—î–¥–Ω–∞–Ω—É —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—é –¥–ª—è –≤—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è
        print(f"üìù Step 7: Using combined transcription for display...")
        sys.stdout.flush()
        
        # combined_segments –≤–∂–µ —Å—Ç–≤–æ—Ä–µ–Ω—ñ –Ω–∞ –∫—Ä–æ—Ü—ñ 3, –Ω–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ –ø–æ–≤—Ç–æ—Ä–Ω–æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±—É–≤–∞—Ç–∏
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ –¥–∞–Ω—ñ –¥–ª—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó –º–∞—Å–∫–∏
        mask_data = []
        for seg in sorted(diarization_segments, key=lambda x: x['start']):
            mask_data.append({
                'start': seg['start'],
                'end': seg['end'],
                'speaker': seg['speaker'],
                'is_main_speaker': seg['speaker'] == main_speaker,
                'suppression_applied': suppression_factor if seg['speaker'] != main_speaker else 1.0
            })
        
        segments_info = {
            'total_segments': len(diarization_segments),
            'main_speaker': main_speaker,
            'main_speaker_duration': main_duration,
            'main_speaker_percentage': main_duration / duration * 100,
            'all_speakers': {speaker: dur for speaker, dur in speaker_durations.items()},
            'transcription': transcription_text,
            'transcription_segments': combined_segments,
            'mask_data': mask_data,
            'audio_duration': duration
        }
        
        return output_path, main_speaker, segments_info
        
    except Exception as e:
        print(f"‚ùå Error in enhance_main_speaker_audio: {e}")
        import traceback
        traceback.print_exc()
        sys.stdout.flush()
        raise


@app.route('/api/enhance-main-speaker', methods=['POST', 'OPTIONS'])
def api_enhance_main_speaker():
    """
    –ï–Ω–¥–ø–æ—ó–Ω—Ç –¥–ª—è –≤–∏–¥—ñ–ª–µ–Ω–Ω—è –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ –≤ –∞—É–¥—ñ–æ.
    –ü—Ä–∏–π–º–∞—î –∞—É–¥—ñ–æ—Ñ–∞–π–ª, –≤–∏–∫–æ–Ω—É—î –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—é, –≤–∏–∑–Ω–∞—á–∞—î –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
    —ñ –ø–æ–≤–µ—Ä—Ç–∞—î –æ–±—Ä–æ–±–ª–µ–Ω–∏–π –∞—É–¥—ñ–æ—Ñ–∞–π–ª –∑ –ø—Ä–∏–≥–ª—É—à–µ–Ω–∏–º–∏ –Ω–µ–æ—Å–Ω–æ–≤–Ω–∏–º–∏ —Å–ø—ñ–∫–µ—Ä–∞–º–∏.
    
    –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ (multipart/form-data):
    - file: –∞—É–¥—ñ–æ—Ñ–∞–π–ª (–æ–±–æ–≤'—è–∑–∫–æ–≤–æ)
    - num_speakers: –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Å–ø—ñ–∫–µ—Ä—ñ–≤ (–æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ, –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ)
    - suppression_factor: –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç –ø—Ä–∏–≥–ª—É—à–µ–Ω–Ω—è 0.0-1.0 (–æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ, –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º 0.1)
    
    Returns:
    - –û–±—Ä–æ–±–ª–µ–Ω–∏–π –∞—É–¥—ñ–æ—Ñ–∞–π–ª (WAV) –∞–±–æ JSON –∑ –ø–æ–º–∏–ª–∫–æ—é
    """
    import sys
    
    print(f"üîµ [API] /api/enhance-main-speaker called - Method: {request.method}, Remote: {request.remote_addr}")
    sys.stdout.flush()
    
    # –û–±—Ä–æ–±–∫–∞ OPTIONS –¥–ª—è preflight –∑–∞–ø–∏—Ç—ñ–≤ (CORS)
    if request.method == 'OPTIONS':
        print("‚úÖ OPTIONS preflight request received from", request.remote_addr)
        response = jsonify({})
        response.headers.add('Access-Control-Allow-Origin', '*')
        response.headers.add('Access-Control-Allow-Headers', 'Content-Type')
        response.headers.add('Access-Control-Allow-Methods', 'POST, OPTIONS')
        sys.stdout.flush()
        return response
    
    print(f"üì• POST /api/enhance-main-speaker request received from {request.remote_addr}")
    sys.stdout.flush()
    
    filepath = None
    
    try:
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –Ω–∞—è–≤–Ω—ñ—Å—Ç—å —Ñ–∞–π–ª—É
        if 'file' not in request.files:
            return jsonify({
                'success': False,
                'error': 'No file provided. Send file in "file" field.',
                'code': 'NO_FILE'
            }), 400
        
        file = request.files['file']
        
        if file.filename == '':
            return jsonify({
                'success': False,
                'error': 'No file selected.',
                'code': 'EMPTY_FILENAME'
            }), 400
        
        if not allowed_file(file.filename):
            return jsonify({
                'success': False,
                'error': f'Invalid audio format. Allowed: {", ".join(ALLOWED_EXTENSIONS)}',
                'code': 'INVALID_FORMAT'
            }), 400
        
        # –û—Ç—Ä–∏–º—É—î–º–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
        num_speakers = request.form.get('num_speakers')
        if num_speakers:
            try:
                num_speakers = int(num_speakers)
            except ValueError:
                num_speakers = None
        else:
            num_speakers = None
        
        suppression_factor = request.form.get('suppression_factor', '0.1')
        try:
            suppression_factor = float(suppression_factor)
            # –û–±–º–µ–∂—É—î–º–æ –∑–Ω–∞—á–µ–Ω–Ω—è –≤—ñ–¥ 0.0 –¥–æ 1.0
            suppression_factor = max(0.0, min(1.0, suppression_factor))
        except ValueError:
            suppression_factor = 0.1
        
        # –û—Ç—Ä–∏–º—É—î–º–æ —Ä–µ–∂–∏–º LLM (fast, smart, smart-2, local)
        llm_mode = request.form.get('llm_mode', 'local')
        # –ù–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ —Ä–µ–∂–∏–º (smart-2 -> smart-2, smart2 -> smart-2)
        if llm_mode == 'smart2':
            llm_mode = 'smart-2'
        # –í–∞–ª—ñ–¥–∞—Ü—ñ—è —Ä–µ–∂–∏–º—É
        valid_modes = ['local', 'fast', 'smart', 'smart-2', 'test', 'test2']
        if llm_mode not in valid_modes:
            llm_mode = 'local'
        
        # –û—Ç—Ä–∏–º—É—î–º–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó (whisper, azure, speechmatics)
        transcription_provider = request.form.get('transcription_provider', 'whisper')
        # –í–∞–ª—ñ–¥–∞—Ü—ñ—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
        valid_providers = ['whisper', 'azure', 'speechmatics']
        if transcription_provider not in valid_providers:
            transcription_provider = 'whisper'
        
        print(f"üìã Parameters: num_speakers={num_speakers}, suppression_factor={suppression_factor}, llm_mode={llm_mode}, transcription_provider={transcription_provider}")
        sys.stdout.flush()
        
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∏–π —Ñ–∞–π–ª
        filename = secure_filename(file.filename)
        filepath = os.path.join(UPLOAD_FOLDER, filename)
        file.save(filepath)
        
        print(f"üíæ File saved to: {filepath}")
        sys.stdout.flush()
        
        # –û–±—Ä–æ–±–ª—è—î–º–æ –∞—É–¥—ñ–æ
        output_path, main_speaker, segments_info = enhance_main_speaker_audio(
            filepath,
            suppression_factor=suppression_factor,
            num_speakers=num_speakers,
            llm_mode=llm_mode,
            transcription_provider=transcription_provider
        )
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ –ø–æ—Ç—Ä—ñ–±–Ω–æ –ø–æ–≤–µ—Ä–Ω—É—Ç–∏ JSON –∑ –º–µ—Ç–∞–¥–∞–Ω–∏–º–∏
        return_json = request.form.get('return_json', 'false').lower() == 'true'
        
        if return_json:
            # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ JSON –∑ –º–µ—Ç–∞–¥–∞–Ω–∏–º–∏ —Ç–∞ URL —Ñ–∞–π–ª—É
            import base64
            
            # –ß–∏—Ç–∞—î–º–æ —Ñ–∞–π–ª —Ç–∞ –∫–æ–¥—É—î–º–æ –≤ base64 –¥–ª—è –ø–µ—Ä–µ–¥–∞—á—ñ
            print(f"üìÇ [File Return] Reading file for client: {output_path}")
            if not os.path.exists(output_path):
                print(f"‚ùå [File Return] ERROR: Output file does not exist: {output_path}")
                raise FileNotFoundError(f"Output file not found: {output_path}")
            
            file_size = os.path.getsize(output_path)
            print(f"üìÇ [File Return] File exists, size: {file_size} bytes")
            
            with open(output_path, 'rb') as f:
                audio_data = f.read()
                audio_base64 = base64.b64encode(audio_data).decode('utf-8')
            
            print(f"üìÇ [File Return] File read successfully, base64 length: {len(audio_base64)} chars")
            sys.stdout.flush()
            
            response_data = {
                'success': True,
                'audio_file_base64': audio_base64,
                'audio_filename': f"enhanced_main_speaker_{os.path.basename(filepath)}",
                'main_speaker': main_speaker,
                'segments_info': segments_info
            }
            
            response = jsonify(response_data)
            response.headers.add('Access-Control-Allow-Origin', '*')
            return response
        else:
            # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ —Ñ–∞–π–ª (legacy —Ä–µ–∂–∏–º)
            print(f"üì§ Sending enhanced audio file: {output_path}")
            sys.stdout.flush()
            
            response = send_file(
                output_path,
                mimetype='audio/wav',
                as_attachment=True,
                download_name=f"enhanced_main_speaker_{os.path.basename(filepath)}"
            )
            response.headers.add('Access-Control-Allow-Origin', '*')
            response.headers.add('X-Main-Speaker', str(main_speaker))
            response.headers.add('X-Segments-Count', str(segments_info['total_segments']))
            response.headers.add('X-Main-Speaker-Duration', f"{segments_info['main_speaker_duration']:.2f}")
            response.headers.add('X-Main-Speaker-Percentage', f"{segments_info['main_speaker_percentage']:.1f}")
            
            return response
        
    except ValueError as e:
        error_msg = str(e)
        print(f"‚ùå ValueError: {error_msg}")
        sys.stdout.flush()
        response = jsonify({
            'success': False,
            'error': error_msg,
            'code': 'PROCESSING_ERROR'
        })
        response.headers.add('Access-Control-Allow-Origin', '*')
        return response, 500
        
    except Exception as e:
        error_msg = str(e)
        print(f"‚ùå Error in /api/enhance-main-speaker: {error_msg}")
        import traceback
        traceback.print_exc()
        sys.stdout.flush()
        response = jsonify({
            'success': False,
            'error': f'Processing failed: {error_msg}',
            'code': 'INTERNAL_ERROR'
        })
        response.headers.add('Access-Control-Allow-Origin', '*')
        return response, 500
        
    finally:
        # –û—á–∏—â–∞—î–º–æ —Ç–∏–º—á–∞—Å–æ–≤—ñ —Ñ–∞–π–ª–∏ (–∑–∞–ª–∏—à–∞—î–º–æ –æ–±—Ä–æ–±–ª–µ–Ω–∏–π —Ñ–∞–π–ª –¥–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è)
        try:
            if filepath and os.path.exists(filepath):
                # –í–∏–¥–∞–ª—è—î–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∏–π —Ñ–∞–π–ª —á–µ—Ä–µ–∑ –¥–µ—è–∫–∏–π —á–∞—Å
                # (–Ω–µ –æ–¥—Ä–∞–∑—É, —â–æ–± –Ω–µ –≤–∏–¥–∞–ª–∏—Ç–∏ –ø—ñ–¥ —á–∞—Å –≤—ñ–¥–ø—Ä–∞–≤–∫–∏)
                pass  # –ú–æ–∂–Ω–∞ –¥–æ–¥–∞—Ç–∏ –æ—á–∏—â–µ–Ω–Ω—è –ø—ñ–∑–Ω—ñ—à–µ
        except Exception as e:
            print(f"‚ö†Ô∏è Could not clean up temp files: {e}")


if __name__ == '__main__':
    port = int(os.environ.get('IOS_SHORTCUTS_PORT', 5005))
    print(f"üöÄ Starting Flask server for iOS Shortcuts on port {port}")
    print(f"üìÇ Upload folder: {UPLOAD_FOLDER}")
    print(f"üåê Server will be accessible at: http://0.0.0.0:{port}")
    print(f"üì± Use your Mac's IP address for iOS Shortcuts")
    app.run(host='0.0.0.0', port=port, debug=False)

