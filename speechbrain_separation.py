#!/usr/bin/env python3
"""
SpeechBrain-based speaker separation for overlap diarization (Mode 3).

This script mirrors the JSON contract of pyannote_separation.py so that
the Node.js backend can swap separation engines without changing the
rest of the pipeline.

Usage:
    python speechbrain_separation.py <audio_path> <output_dir>
"""

import sys
import os
import json
import tempfile
import time
from pathlib import Path

import torch
import pyannote_patch  # noqa: F401  # ensures torchaudio compatibility on Python 3.14+
import torchaudio
import soundfile as sf
import numpy as np
from speechbrain.inference.separation import (
    SepformerSeparation as Separator,
)
import torch.nn.functional as F
try:
    from speechbrain.inference.VAD import VAD
    VAD_AVAILABLE = True
except (ImportError, AttributeError):
    try:
        from speechbrain.pretrained import VAD
        VAD_AVAILABLE = True
    except (ImportError, AttributeError):
        VAD_AVAILABLE = False
        # Log will be done later when needed


def log_error(message):
    print(message, file=sys.stderr)
    sys.stderr.flush()


def log_info(message):
    """Log info message with timestamp"""
    timestamp = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())
    log_error(f"[{timestamp}] {message}")


def ensure_output_dir(path: str) -> str:
    if not path:
        return tempfile.mkdtemp(prefix="speechbrain_separation_")
    os.makedirs(path, exist_ok=True)
    return path


def get_device():
    env_device = os.getenv("SPEECHBRAIN_DEVICE")
    if env_device:
        return env_device
    if torch.backends.mps.is_available():
        return "mps"
    if torch.cuda.is_available():
        return "cuda"
    return "cpu"


def load_waveform(audio_path: str, target_sample_rate: int = 16000):
    """
    Loads audio as torch tensor (shape [1, samples]) and sample rate.
    Prefers soundfile to avoid torchcodec dependency, falls back to torchaudio.
    –û–±–æ–≤'—è–∑–∫–æ–≤–æ —Ä–µ—Å–µ–º–ø–ª—é—î –¥–æ target_sample_rate (–∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º 16000 –ì—Ü).
    """
    log_info("üìÅ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∞—É–¥—ñ–æ —Ñ–∞–π–ª—É...")
    log_info(f"   –®–ª—è—Ö: {audio_path}")
    
    file_size = os.path.getsize(audio_path)
    file_size_mb = file_size / (1024 * 1024)
    log_info(f"   –†–æ–∑–º—ñ—Ä —Ñ–∞–π–ª—É: {file_size_mb:.2f} MB ({file_size} –±–∞–π—Ç)")
    
    try:
        load_start = time.time()
        data, sample_rate = sf.read(audio_path, dtype='float32', always_2d=True)
        waveform = torch.from_numpy(data.T)
        load_time = time.time() - load_start
        
        log_info(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ —á–µ—Ä–µ–∑ soundfile –∑–∞ {load_time:.2f} —Å–µ–∫")
        log_info(f"   –§–æ—Ä–º–∞ —Ö–≤–∏–ª—ñ: {waveform.shape}")
        log_info(f"   Sample rate: {sample_rate} Hz")
        log_info(f"   –ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–∞–Ω–∞–ª—ñ–≤: {waveform.shape[0]}")
        log_info(f"   –ö—ñ–ª—å–∫—ñ—Å—Ç—å –∑—Ä–∞–∑–∫—ñ–≤: {waveform.shape[1]}")
        log_info(f"   –¢—Ä–∏–≤–∞–ª—ñ—Å—Ç—å: {waveform.shape[1] / sample_rate:.2f} —Å–µ–∫")
    except Exception as sf_error:
        log_error(f"[SpeechBrain] soundfile load failed ({sf_error}), falling back to torchaudio")
        load_start = time.time()
        waveform, sample_rate = torchaudio.load(audio_path)
        load_time = time.time() - load_start
        
        log_info(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ —á–µ—Ä–µ–∑ torchaudio –∑–∞ {load_time:.2f} —Å–µ–∫")
        log_info(f"   –§–æ—Ä–º–∞ —Ö–≤–∏–ª—ñ: {waveform.shape}")
        log_info(f"   Sample rate: {sample_rate} Hz")
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—è –≤ –º–æ–Ω–æ, —è–∫—â–æ —Å—Ç–µ—Ä–µ–æ
    if waveform.shape[0] > 1:
        log_info(f"üîÑ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—è –∑ {waveform.shape[0]} –∫–∞–Ω–∞–ª—ñ–≤ –≤ –º–æ–Ω–æ")
        waveform = waveform.mean(dim=0, keepdim=True)
        log_info(f"   –ù–æ–≤–∞ —Ñ–æ—Ä–º–∞: {waveform.shape}")
    
    # –û–±–æ–≤'—è–∑–∫–æ–≤–∏–π —Ä–µ—Å–µ–º–ø–ª—ñ–Ω–≥ –¥–æ target_sample_rate
    if sample_rate != target_sample_rate:
        log_info(f"üîÑ –û–±–æ–≤'—è–∑–∫–æ–≤–∏–π —Ä–µ—Å–µ–º–ø–ª—ñ–Ω–≥ –∑ {sample_rate}Hz –Ω–∞ {target_sample_rate}Hz")
        resample_start = time.time()
        resampler = torchaudio.transforms.Resample(sample_rate, target_sample_rate)
        waveform = resampler(waveform)
        resample_time = time.time() - resample_start
        log_info(f"‚úÖ –†–µ—Å–µ–º–ø–ª—ñ–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {resample_time:.2f} —Å–µ–∫")
        sample_rate = target_sample_rate
    
    return waveform, sample_rate


def normalize_audio_level(waveform: torch.Tensor, method: str = 'peak', target_level: float = 0.80):
    """
    –ù–æ—Ä–º–∞–ª—ñ–∑—É—î —Ä—ñ–≤–µ–Ω—å –≥—É—á–Ω–æ—Å—Ç—ñ –∞—É–¥—ñ–æ –¥–ª—è —É–Ω–∏–∫–Ω–µ–Ω–Ω—è –ø–ª—É—Ç–∞–Ω–∏–Ω–∏ —Å–ø—ñ–∫–µ—Ä—ñ–≤ —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–ø–∞–¥–∏ –≥—É—á–Ω–æ—Å—Ç—ñ.
    –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –±—ñ–ª—å—à –∞–≥—Ä–µ—Å–∏–≤–Ω—É –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—é –¥–ª—è –∫—Ä–∞—â–æ–≥–æ –≤–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è –≥—É—á–Ω–æ—Å—Ç—ñ.
    
    Args:
        waveform: Tensor —Ñ–æ—Ä–º–∏ [channels, samples] –∞–±–æ [1, samples]
        method: –ú–µ—Ç–æ–¥ –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—ó ('peak' –∞–±–æ 'rms')
        target_level: –¶—ñ–ª—å–æ–≤–∏–π —Ä—ñ–≤–µ–Ω—å (0.0-1.0 –¥–ª—è peak, –¥–ë –¥–ª—è RMS)
    
    Returns:
        –ù–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω–∏–π waveform
    """
    if waveform.numel() == 0:
        return waveform
    
    # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω—É —Ñ–æ—Ä–º—É
    original_shape = waveform.shape
    was_2d = waveform.dim() == 2
    
    # –ü–µ—Ä–µ—Ç–≤–æ—Ä—é—î–º–æ –≤ 1D –¥–ª—è –æ–±—á–∏—Å–ª–µ–Ω—å
    if was_2d:
        flat_waveform = waveform.flatten()
    else:
        flat_waveform = waveform
    
    # –û–±—á–∏—Å–ª—é—î–º–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    max_val = torch.abs(flat_waveform).max()
    rms_val = torch.sqrt(torch.mean(flat_waveform ** 2))
    
    if method == 'peak':
        # Peak normalization - –≤–∏—Ä—ñ–≤–Ω—é—î–º–æ –¥–æ target_level –≤—ñ–¥ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–Ω—è
        # –ë—ñ–ª—å—à –∞–≥—Ä–µ—Å–∏–≤–Ω–∞ –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è –¥–ª—è –∫—Ä–∞—â–æ–≥–æ –≤–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è
        if max_val > 0:
            scale_factor = target_level / max_val
            normalized = flat_waveform * scale_factor
        else:
            normalized = flat_waveform
    elif method == 'rms':
        # RMS normalization - –≤–∏—Ä—ñ–≤–Ω—é—î–º–æ RMS –¥–æ —Ü—ñ–ª—å–æ–≤–æ–≥–æ —Ä—ñ–≤–Ω—è
        # –Ø–∫—â–æ target_level < 1.0, —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç—É—î–º–æ —è–∫ –ª—ñ–Ω—ñ–π–Ω–µ –∑–Ω–∞—á–µ–Ω–Ω—è (0.0-1.0)
        # –Ø–∫—â–æ target_level >= 1.0, —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç—É—î–º–æ —è–∫ –¥–ë
        if target_level < 1.0:
            target_rms = target_level  # –õ—ñ–Ω—ñ–π–Ω–µ –∑–Ω–∞—á–µ–Ω–Ω—è
        else:
            target_rms = 10 ** (target_level / 20)  # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ –¥–ë –≤ –ª—ñ–Ω—ñ–π–Ω—É —à–∫–∞–ª—É
        
        if rms_val > 0:
            scale_factor = target_rms / rms_val
            # –û–±–º–µ–∂—É—î–º–æ –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è, —â–æ–± —É–Ω–∏–∫–Ω—É—Ç–∏ –∫–ª—ñ–ø—ñ–Ω–≥—É (–∞–ª–µ –¥–æ–∑–≤–æ–ª—è—î–º–æ –±—ñ–ª—å—à –∞–≥—Ä–µ—Å–∏–≤–Ω—É –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—é)
            if scale_factor * max_val > 0.90:
                scale_factor = 0.90 / max_val
            normalized = flat_waveform * scale_factor
        else:
            normalized = flat_waveform
    else:
        # –ó–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º - peak normalization
        if max_val > 0:
            scale_factor = target_level / max_val
            normalized = flat_waveform * scale_factor
        else:
            normalized = flat_waveform
    
    # –í—ñ–¥–Ω–æ–≤–ª—é—î–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω—É —Ñ–æ—Ä–º—É
    if was_2d:
        normalized = normalized.reshape(original_shape)
    
    return normalized


def apply_spectral_gating(separated_sources, mixture, gate_threshold=0.1, gate_alpha=0.5):
    """
    –ó–∞—Å—Ç–æ—Å–æ–≤—É—î —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–µ –≥–µ–π—Ç—É–≤–∞–Ω–Ω—è –¥–ª—è –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –æ–¥–Ω–æ—á–∞—Å–Ω–∏—Ö –≥–æ–ª–æ—Å—ñ–≤.
    –í–∏–¥–∞–ª—è—î –∑–∞–ª–∏—à–∫–∏ —ñ–Ω—à–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ –∑ –∫–æ–∂–Ω–æ–≥–æ –¥–∂–µ—Ä–µ–ª–∞.
    
    Args:
        separated_sources: Tensor —Ñ–æ—Ä–º–∏ [speakers, samples] - —Ä–æ–∑–¥—ñ–ª–µ–Ω—ñ –¥–∂–µ—Ä–µ–ª–∞
        mixture: Tensor —Ñ–æ—Ä–º–∏ [1, samples] - –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∞ —Å—É–º—ñ—à
        gate_threshold: –ü–æ—Ä—ñ–≥ –¥–ª—è –≥–µ–π—Ç—É–≤–∞–Ω–Ω—è (0.0-1.0)
        gate_alpha: –ö–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç –¥–ª—è –º'—è–∫–æ–≥–æ –≥–µ–π—Ç—É–≤–∞–Ω–Ω—è (0.0-1.0)
    
    Returns:
        –ü–æ–∫—Ä–∞—â–µ–Ω—ñ —Ä–æ–∑–¥—ñ–ª–µ–Ω—ñ –¥–∂–µ—Ä–µ–ª–∞
    """
    if separated_sources.numel() == 0 or mixture.numel() == 0:
        return separated_sources
    
    # –ü–µ—Ä–µ–∫–æ–Ω—É—î–º–æ—Å—è, —â–æ mixture –º–∞—î —Ñ–æ—Ä–º—É [1, samples]
    if mixture.dim() == 1:
        mixture = mixture.unsqueeze(0)
    
    # –û–±—á–∏—Å–ª—é—î–º–æ –µ–Ω–µ—Ä–≥—ñ—é –∫–æ–∂–Ω–æ–≥–æ –¥–∂–µ—Ä–µ–ª–∞
    sources_energy = torch.abs(separated_sources)
    mixture_energy = torch.abs(mixture.squeeze(0))
    
    # –û–±—á–∏—Å–ª—é—î–º–æ –≤—ñ–¥–Ω–æ—Å–Ω—É –µ–Ω–µ—Ä–≥—ñ—é –∫–æ–∂–Ω–æ–≥–æ –¥–∂–µ—Ä–µ–ª–∞ –≤—ñ–¥–Ω–æ—Å–Ω–æ —Å—É–º—ñ—à—ñ
    total_energy = sources_energy.sum(dim=0, keepdim=True) + 1e-8
    relative_energy = sources_energy / total_energy
    
    # –°—Ç–≤–æ—Ä—é—î–º–æ –º'—è–∫—É –º–∞—Å–∫—É –≥–µ–π—Ç—É–≤–∞–Ω–Ω—è
    # –î–∂–µ—Ä–µ–ª–∞ –∑ –≤–∏—Å–æ–∫–æ—é –≤—ñ–¥–Ω–æ—Å–Ω–æ—é –µ–Ω–µ—Ä–≥—ñ—î—é –∑–±–µ—Ä—ñ–≥–∞—é—Ç—å—Å—è, –∑ –Ω–∏–∑—å–∫–æ—é - –ø—Ä–∏–≥–ª—É—à—É—é—Ç—å—Å—è
    gate_mask = torch.clamp((relative_energy - gate_threshold) / (1.0 - gate_threshold + 1e-8), 0, 1)
    gate_mask = gate_alpha + (1.0 - gate_alpha) * gate_mask
    
    # –ó–∞—Å—Ç–æ—Å–æ–≤—É—î–º–æ –º–∞—Å–∫—É
    gated_sources = separated_sources * gate_mask
    
    return gated_sources


def enhance_speaker_differences(separated_sources, enhancement_strength=0.3):
    """
    –ü—ñ–¥—Å–∏–ª—é—î –≤—ñ–¥–º—ñ–Ω–Ω–æ—Å—Ç—ñ –º—ñ–∂ —Å–ø—ñ–∫–µ—Ä–∞–º–∏ –¥–ª—è –∫—Ä–∞—â–æ–≥–æ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –æ–¥–Ω–æ—á–∞—Å–Ω–∏—Ö –≥–æ–ª–æ—Å—ñ–≤.
    –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–µ –ø—ñ–¥—Å–∏–ª–µ–Ω–Ω—è –¥–ª—è –ø—ñ–¥–∫—Ä–µ—Å–ª–µ–Ω–Ω—è —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –∫–æ–∂–Ω–æ–≥–æ –≥–æ–ª–æ—Å—É.
    
    Args:
        separated_sources: Tensor —Ñ–æ—Ä–º–∏ [speakers, samples] - —Ä–æ–∑–¥—ñ–ª–µ–Ω—ñ –¥–∂–µ—Ä–µ–ª–∞
        enhancement_strength: –°–∏–ª–∞ –ø—ñ–¥—Å–∏–ª–µ–Ω–Ω—è (0.0-1.0)
    
    Returns:
        –ü–æ–∫—Ä–∞—â–µ–Ω—ñ —Ä–æ–∑–¥—ñ–ª–µ–Ω—ñ –¥–∂–µ—Ä–µ–ª–∞
    """
    if separated_sources.numel() == 0 or enhancement_strength <= 0:
        return separated_sources
    
    num_speakers = separated_sources.shape[0]
    if num_speakers < 2:
        return separated_sources
    
    # –û–±—á–∏—Å–ª—é—î–º–æ —Å–µ—Ä–µ–¥–Ω—î –∑–Ω–∞—á–µ–Ω–Ω—è –≤—Å—ñ—Ö –¥–∂–µ—Ä–µ–ª
    mean_source = separated_sources.mean(dim=0, keepdim=True)
    
    # –í—ñ–¥–Ω—ñ–º–∞—î–º–æ —Å–µ—Ä–µ–¥–Ω—î –≤—ñ–¥ –∫–æ–∂–Ω–æ–≥–æ –¥–∂–µ—Ä–µ–ª–∞ –¥–ª—è –ø—ñ–¥—Å–∏–ª–µ–Ω–Ω—è –≤—ñ–¥–º—ñ–Ω–Ω–æ—Å—Ç–µ–π
    differences = separated_sources - mean_source
    
    # –ü—ñ–¥—Å–∏–ª—é—î–º–æ –≤—ñ–¥–º—ñ–Ω–Ω–æ—Å—Ç—ñ
    enhanced = separated_sources + enhancement_strength * differences
    
    # –û–±–º–µ–∂—É—î–º–æ, —â–æ–± —É–Ω–∏–∫–Ω—É—Ç–∏ –∫–ª—ñ–ø—ñ–Ω–≥—É
    max_val = torch.abs(enhanced).max()
    if max_val > 0.95:
        enhanced = enhanced * (0.95 / max_val)
    
    return enhanced


def adaptive_volume_tracking(separated_sources, speaker_energy_history=None, alpha=0.9):
    """
    –ê–¥–∞–ø—Ç–∏–≤–Ω–æ –≤—ñ–¥—Å—Ç–µ–∂—É—î —Ç–∞ –Ω–æ—Ä–º–∞–ª—ñ–∑—É—î –≥—É—á–Ω—ñ—Å—Ç—å –∫–æ–∂–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ –¥–ª—è –∑–∞–ø–æ–±—ñ–≥–∞–Ω–Ω—è —Å–≤—ñ—Ç—á—É –≥–æ–ª–æ—Å—ñ–≤
    –ø—Ä–∏ –∑–º—ñ–Ω—ñ –≥—É—á–Ω–æ—Å—Ç—ñ. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –µ–∫—Å–ø–æ–Ω–µ–Ω—Ü—ñ–π–Ω–µ –∑–≥–ª–∞–¥–∂—É–≤–∞–Ω–Ω—è –¥–ª—è –ø—ñ–¥—Ç—Ä–∏–º–∫–∏ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—ñ.
    
    Args:
        separated_sources: Tensor —Ñ–æ—Ä–º–∏ [speakers, samples] - —Ä–æ–∑–¥—ñ–ª–µ–Ω—ñ –¥–∂–µ—Ä–µ–ª–∞
        speaker_energy_history: –°–ª–æ–≤–Ω–∏–∫ –∑ —ñ—Å—Ç–æ—Ä—ñ—î—é –µ–Ω–µ—Ä–≥—ñ—ó –∫–æ–∂–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ {speaker_idx: energy}
        alpha: –ö–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç –∑–≥–ª–∞–¥–∂—É–≤–∞–Ω–Ω—è (0.0-1.0), –≤–∏—â–∏–π = –±—ñ–ª—å—à–µ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —ñ—Å—Ç–æ—Ä—ñ—ó
    
    Returns:
        –ù–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ –¥–∂–µ—Ä–µ–ª–∞ —Ç–∞ –æ–Ω–æ–≤–ª–µ–Ω–∞ —ñ—Å—Ç–æ—Ä—ñ—è –µ–Ω–µ—Ä–≥—ñ—ó
    """
    if separated_sources.numel() == 0:
        return separated_sources, speaker_energy_history or {}
    
    num_speakers = separated_sources.shape[0]
    if speaker_energy_history is None:
        speaker_energy_history = {}
    
    # –û–±—á–∏—Å–ª—é—î–º–æ –ø–æ—Ç–æ—á–Ω—É RMS –µ–Ω–µ—Ä–≥—ñ—é –∫–æ–∂–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
    current_energies = {}
    normalized_sources = separated_sources.clone()
    
    for speaker_idx in range(num_speakers):
        speaker_audio = separated_sources[speaker_idx, :]
        current_rms = torch.sqrt(torch.mean(speaker_audio ** 2))
        current_energies[speaker_idx] = current_rms.item()
        
        # –Ø–∫—â–æ —î —ñ—Å—Ç–æ—Ä—ñ—è, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –∞–¥–∞–ø—Ç–∏–≤–Ω—É –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—é
        if speaker_idx in speaker_energy_history:
            # –ï–∫—Å–ø–æ–Ω–µ–Ω—Ü—ñ–π–Ω–µ –∑–≥–ª–∞–¥–∂—É–≤–∞–Ω–Ω—è –¥–ª—è –æ–Ω–æ–≤–ª–µ–Ω–Ω—è —Ü—ñ–ª—å–æ–≤–æ—ó –µ–Ω–µ—Ä–≥—ñ—ó
            target_energy = alpha * speaker_energy_history[speaker_idx] + (1 - alpha) * current_rms.item()
            
            # –ù–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ –¥–æ —Ü—ñ–ª—å–æ–≤–æ—ó –µ–Ω–µ—Ä–≥—ñ—ó (—è–∫—â–æ –ø–æ—Ç–æ—á–Ω–∞ –∑–Ω–∞—á–Ω–æ –≤—ñ–¥—Ä—ñ–∑–Ω—è—î—Ç—å—Å—è)
            if current_rms > 1e-6 and target_energy > 1e-6:
                # –û–±—á–∏—Å–ª—é—î–º–æ –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—ó
                # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –º'—è–∫—É –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—é, —â–æ–± –Ω–µ –ø–µ—Ä–µ–∫—Ä—É—á—É–≤–∞—Ç–∏ —Å–∏–≥–Ω–∞–ª
                energy_ratio = target_energy / current_rms.item()
                
                # –ó–∞—Å—Ç–æ—Å–æ–≤—É—î–º–æ –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—é —Ç—ñ–ª—å–∫–∏ —è–∫—â–æ —Ä—ñ–∑–Ω–∏—Ü—è –∑–Ω–∞—á–Ω–∞ (>20%)
                if abs(energy_ratio - 1.0) > 0.2:
                    # –ú'—è–∫–∞ –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è - –Ω–µ –ø–æ–≤–Ω–∞, —â–æ–± –∑–±–µ—Ä–µ–≥—Ç–∏ –ø—Ä–∏—Ä–æ–¥–Ω—ñ—Å—Ç—å
                    normalization_factor = 0.7 + 0.3 * energy_ratio  # 70% –æ—Ä–∏–≥—ñ–Ω–∞–ª—É + 30% –Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω–æ–≥–æ
                    normalized_sources[speaker_idx, :] = speaker_audio * normalization_factor
                
                # –û–Ω–æ–≤–ª—é—î–º–æ —ñ—Å—Ç–æ—Ä—ñ—é
                speaker_energy_history[speaker_idx] = target_energy
            else:
                speaker_energy_history[speaker_idx] = current_rms.item()
        else:
            # –ü–µ—Ä—à–∏–π —Ä–∞–∑ - –∑–±–µ—Ä—ñ–≥–∞—î–º–æ –ø–æ—Ç–æ—á–Ω—É –µ–Ω–µ—Ä–≥—ñ—é —è–∫ –±–∞–∑–æ–≤—É
            speaker_energy_history[speaker_idx] = current_rms.item()
    
    # –û–±–º–µ–∂—É—î–º–æ, —â–æ–± —É–Ω–∏–∫–Ω—É—Ç–∏ –∫–ª—ñ–ø—ñ–Ω–≥—É
    max_val = torch.abs(normalized_sources).max()
    if max_val > 0.95:
        normalized_sources = normalized_sources * (0.95 / max_val)
    
    return normalized_sources, speaker_energy_history


def apply_dynamic_speaker_gating(separated_sources, mixture, speaker_energy_history=None, 
                                  min_energy_ratio=0.3, gate_strength=0.6):
    """
    –ó–∞—Å—Ç–æ—Å–æ–≤—É—î –¥–∏–Ω–∞–º—ñ—á–Ω–µ –≥–µ–π—Ç—É–≤–∞–Ω–Ω—è –Ω–∞ –æ—Å–Ω–æ–≤—ñ –ª–æ–∫–∞–ª—å–Ω–æ—ó –µ–Ω–µ—Ä–≥—ñ—ó –∫–æ–∂–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞.
    –ó–∞–ø–æ–±—ñ–≥–∞—î —Å–≤—ñ—Ç—á—É –≥–æ–ª–æ—Å—ñ–≤, –∫–æ–ª–∏ –æ–¥–∏–Ω —Å–ø—ñ–∫–µ—Ä —Å—Ç–∞—î —Ç–∏—Ö—ñ—à–∏–º.
    
    Args:
        separated_sources: Tensor —Ñ–æ—Ä–º–∏ [speakers, samples] - —Ä–æ–∑–¥—ñ–ª–µ–Ω—ñ –¥–∂–µ—Ä–µ–ª–∞
        mixture: Tensor —Ñ–æ—Ä–º–∏ [1, samples] - –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∞ —Å—É–º—ñ—à
        speaker_energy_history: –Ü—Å—Ç–æ—Ä—ñ—è –µ–Ω–µ—Ä–≥—ñ—ó —Å–ø—ñ–∫–µ—Ä—ñ–≤
        min_energy_ratio: –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–µ —Å–ø—ñ–≤–≤—ñ–¥–Ω–æ—à–µ–Ω–Ω—è –µ–Ω–µ—Ä–≥—ñ—ó –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Å–∏–≥–Ω–∞–ª—É (0.0-1.0)
        gate_strength: –°–∏–ª–∞ –≥–µ–π—Ç—É–≤–∞–Ω–Ω—è (0.0-1.0)
    
    Returns:
        –ü–æ–∫—Ä–∞—â–µ–Ω—ñ —Ä–æ–∑–¥—ñ–ª–µ–Ω—ñ –¥–∂–µ—Ä–µ–ª–∞
    """
    if separated_sources.numel() == 0 or mixture.numel() == 0:
        return separated_sources
    
    num_speakers = separated_sources.shape[0]
    if num_speakers < 2:
        return separated_sources
    
    # –ü–µ—Ä–µ–∫–æ–Ω—É—î–º–æ—Å—è, —â–æ mixture –º–∞—î —Ñ–æ—Ä–º—É [1, samples]
    if mixture.dim() == 1:
        mixture = mixture.unsqueeze(0)
    
    # –û–±—á–∏—Å–ª—é—î–º–æ –ª–æ–∫–∞–ª—å–Ω—É –µ–Ω–µ—Ä–≥—ñ—é –∫–æ–∂–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
    sources_energy = torch.abs(separated_sources)
    total_sources_energy = sources_energy.sum(dim=0, keepdim=True) + 1e-8
    
    # –í—ñ–¥–Ω–æ—Å–Ω–∞ –µ–Ω–µ—Ä–≥—ñ—è –∫–æ–∂–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
    relative_energy = sources_energy / total_sources_energy
    
    # –°—Ç–≤–æ—Ä—é—î–º–æ –¥–∏–Ω–∞–º—ñ—á–Ω—É –º–∞—Å–∫—É –≥–µ–π—Ç—É–≤–∞–Ω–Ω—è
    gated_sources = separated_sources.clone()
    
    for speaker_idx in range(num_speakers):
        speaker_relative_energy = relative_energy[speaker_idx, :]
        
        # –Ø–∫—â–æ —î —ñ—Å—Ç–æ—Ä—ñ—è, –≤—Ä–∞—Ö–æ–≤—É—î–º–æ —ó—ó –¥–ª—è —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ
        if speaker_energy_history and speaker_idx in speaker_energy_history:
            # –û–±—á–∏—Å–ª—é—î–º–æ –ª–æ–∫–∞–ª—å–Ω—É RMS –µ–Ω–µ—Ä–≥—ñ—é
            local_rms = torch.sqrt(torch.mean(separated_sources[speaker_idx, :] ** 2))
            historical_energy = speaker_energy_history[speaker_idx]
            
            # –Ø–∫—â–æ –ø–æ—Ç–æ—á–Ω–∞ –µ–Ω–µ—Ä–≥—ñ—è –∑–Ω–∞—á–Ω–æ –Ω–∏–∂—á–∞ –∑–∞ —ñ—Å—Ç–æ—Ä–∏—á–Ω—É, –∑–∞—Å—Ç–æ—Å–æ–≤—É—î–º–æ –≥–µ–π—Ç—É–≤–∞–Ω–Ω—è
            if historical_energy > 1e-6:
                energy_ratio = local_rms.item() / historical_energy
                
                # –Ø–∫—â–æ –µ–Ω–µ—Ä–≥—ñ—è –≤–ø–∞–ª–∞ –±—ñ–ª—å—à –Ω—ñ–∂ –Ω–∞ 50%, –∑–∞—Å—Ç–æ—Å–æ–≤—É—î–º–æ –≥–µ–π—Ç—É–≤–∞–Ω–Ω—è
                if energy_ratio < 0.5:
                    # –°—Ç–≤–æ—Ä—é—î–º–æ –º–∞—Å–∫—É, —è–∫–∞ –∑–±–µ—Ä—ñ–≥–∞—î —Å–∏–≥–Ω–∞–ª —Ç—ñ–ª—å–∫–∏ —Ç–∞–º, –¥–µ –≤—ñ–Ω –¥–æ—Å–∏—Ç—å —Å–∏–ª—å–Ω–∏–π
                    energy_mask = torch.clamp(
                        (speaker_relative_energy - min_energy_ratio) / (1.0 - min_energy_ratio + 1e-8),
                        0, 1
                    )
                    # –ö–æ–º–±—ñ–Ω—É—î–º–æ –∑ –≥–ª–æ–±–∞–ª—å–Ω–∏–º –≥–µ–π—Ç—É–≤–∞–Ω–Ω—è–º
                    combined_mask = gate_strength + (1.0 - gate_strength) * energy_mask
                    gated_sources[speaker_idx, :] = separated_sources[speaker_idx, :] * combined_mask
        
        # –ó–∞—Å—Ç–æ—Å–æ–≤—É—î–º–æ –±–∞–∑–æ–≤–µ –≥–µ–π—Ç—É–≤–∞–Ω–Ω—è –Ω–∞ –æ—Å–Ω–æ–≤—ñ –≤—ñ–¥–Ω–æ—Å–Ω–æ—ó –µ–Ω–µ—Ä–≥—ñ—ó
        energy_mask = torch.clamp(
            (speaker_relative_energy - min_energy_ratio) / (1.0 - min_energy_ratio + 1e-8),
            0, 1
        )
        base_mask = gate_strength + (1.0 - gate_strength) * energy_mask
        gated_sources[speaker_idx, :] = gated_sources[speaker_idx, :] * base_mask
    
    return gated_sources


def align_channels(prev_chunk, curr_chunk, overlap_len):
    """
    Fix channel permutation by comparing overlap regions.
    
    Args:
        prev_chunk: Previous chunk tensor [speakers, samples]
        curr_chunk: Current chunk tensor [speakers, samples]
        overlap_len: Length of overlap region in samples
    
    Returns:
        curr_chunk with potentially flipped channels
    """
    if prev_chunk is None or overlap_len <= 0:
        return curr_chunk
    
    # Extract overlap regions
    prev_overlap = prev_chunk[:, -overlap_len:]
    curr_overlap = curr_chunk[:, :overlap_len]
    
    # Calculate L1 distance for direct vs swapped assignment
    direct_dist = (prev_overlap - curr_overlap).abs().sum()
    
    # Flip channels (swap speakers)
    flipped_curr = torch.flip(curr_overlap, dims=[0])
    cross_dist = (prev_overlap - flipped_curr).abs().sum()
    
    # If swapped version is better, flip the entire current chunk
    if cross_dist < direct_dist:
        return torch.flip(curr_chunk, dims=[0])
    
    return curr_chunk


def separate(audio_path: str, output_dir: str, settings: dict = None):
    if settings is None:
        settings = {}
    
    process_start_time = time.time()
    
    log_info("‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê")
    log_info("üöÄ –ü–û–ß–ê–¢–û–ö –†–û–ó–î–Ü–õ–ï–ù–ù–Ø –¢–†–ï–ö–Ü–í (PYTHON)")
    log_info("‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê")
    
    # 2. –ü–µ—Ä–µ–¥ –ø–æ—á–∞—Ç–∫–æ–º —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è
    log_info("üîÄ –ï–¢–ê–ü 2: –ü–Ü–î–ì–û–¢–û–í–ö–ê –î–û –†–û–ó–î–Ü–õ–ï–ù–ù–Ø")
    log_info(f"   –°—Ç–∞—Ä—Ç–æ–≤–∞ —Ç–æ—á–∫–∞: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())}")
    log_info(f"   –í–∏—Ö—ñ–¥–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è: {output_dir}")
    
    # Get device from settings or environment
    if settings.get('device'):
        device = settings['device']
    else:
        device = get_device()
    
    cache_dir = os.path.expanduser(
        os.getenv("SPEECHBRAIN_CACHE_DIR", "~/.cache/speechbrain/sepformer-whamr16k")
    )

    log_info(f"   –ü—Ä–∏—Å—Ç—Ä—ñ–π: {device}")
    log_info(f"   –î–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è –∫–µ—à—É: {cache_dir}")
    log_info("   –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –∞–ª–≥–æ—Ä–∏—Ç–º—É:")
    log_info("     - –ú–æ–¥–µ–ª—å: SpeechBrain SepFormer WHAMR16k")
    
    # Get settings with defaults
    num_speakers = int(settings.get('numSpeakers', os.getenv("SPEECHBRAIN_NUM_SPEAKERS", "2")))
    target_sample_rate = int(settings.get('sampleRate', os.getenv("SPEECHBRAIN_SAMPLE_RATE", "16000")))
    # –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –¥–ª—è –±–µ–∑–ø–µ—á–Ω–æ–≥–æ chunking (sliding window)
    chunk_size_seconds = float(settings.get('chunkSeconds', os.getenv("SPEECHBRAIN_CHUNK_SECONDS", "10.0")))
    overlap_seconds = float(settings.get('overlapSeconds', os.getenv("SPEECHBRAIN_OVERLAP_SECONDS", "2.0")))
    
    # Quality settings (critical for separation quality)
    # –û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º –¥–ª—è –∫—Ä–∞—â–æ—ó —è–∫–æ—Å—Ç—ñ
    segment_overlap = float(settings.get('segmentOverlap', os.getenv("SPEECHBRAIN_SEGMENT_OVERLAP", "0.5")))  # –ë—ñ–ª—å—à–∏–π overlap –¥–ª—è –∫—Ä–∞—â–æ–≥–æ –∑—à–∏–≤–∞–Ω–Ω—è
    min_intersegment_gap = float(settings.get('minIntersegmentGap', os.getenv("SPEECHBRAIN_MIN_INTERSEGMENT_GAP", "0.1")))  # –ë—ñ–ª—å—à–∏–π gap –¥–ª—è –∫—Ä–∞—â–æ–≥–æ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è
    strict_mode = settings.get('strictMode', os.getenv("SPEECHBRAIN_STRICT_MODE", "true").lower() == "true")
    vad_threshold = float(settings.get('vadThreshold', os.getenv("SPEECHBRAIN_VAD_THRESHOLD", "0.7")))
    max_speech_duration = float(settings.get('maxSpeechDuration', os.getenv("SPEECHBRAIN_MAX_SPEECH_DURATION", "30")))  # –ù–µ –æ–±–º–µ–∂—É—î–º–æ, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ chunk size
    
    # Advanced settings
    batch_size = int(settings.get('batchSize', os.getenv("SPEECHBRAIN_BATCH_SIZE", "4")))
    dynamic_batching = settings.get('dynamicBatching', os.getenv("SPEECHBRAIN_DYNAMIC_BATCHING", "false").lower() == "true")
    vad_model = settings.get('vadModel', os.getenv("SPEECHBRAIN_VAD_MODEL", "speechbrain/vad-crdnn-libriparty"))
    diarization_model = settings.get('diarizationModel', os.getenv("SPEECHBRAIN_DIARIZATION_MODEL", "speechbrain/diarization-mfa"))
    
    # Post-processing settings for improving separation of similar voices
    enable_spectral_gating = settings.get('enableSpectralGating', os.getenv("SPEECHBRAIN_ENABLE_SPECTRAL_GATING", "true").lower() == "true")
    spectral_gate_threshold = float(settings.get('spectralGateThreshold', os.getenv("SPEECHBRAIN_SPECTRAL_GATE_THRESHOLD", "0.15")))
    spectral_gate_alpha = float(settings.get('spectralGateAlpha', os.getenv("SPEECHBRAIN_SPECTRAL_GATE_ALPHA", "0.4")))
    enable_speaker_enhancement = settings.get('enableSpeakerEnhancement', os.getenv("SPEECHBRAIN_ENABLE_SPEAKER_ENHANCEMENT", "true").lower() == "true")
    speaker_enhancement_strength = float(settings.get('speakerEnhancementStrength', os.getenv("SPEECHBRAIN_SPEAKER_ENHANCEMENT_STRENGTH", "0.4")))
    
    # Adaptive processing settings for preventing voice switching on volume changes
    enable_adaptive_volume_tracking = settings.get('enableAdaptiveVolumeTracking', os.getenv("SPEECHBRAIN_ENABLE_ADAPTIVE_VOLUME_TRACKING", "true").lower() == "true")
    adaptive_volume_alpha = float(settings.get('adaptiveVolumeAlpha', os.getenv("SPEECHBRAIN_ADAPTIVE_VOLUME_ALPHA", "0.85")))
    enable_dynamic_speaker_gating = settings.get('enableDynamicSpeakerGating', os.getenv("SPEECHBRAIN_ENABLE_DYNAMIC_SPEAKER_GATING", "true").lower() == "true")
    dynamic_gate_min_energy_ratio = float(settings.get('dynamicGateMinEnergyRatio', os.getenv("SPEECHBRAIN_DYNAMIC_GATE_MIN_ENERGY_RATIO", "0.25")))
    dynamic_gate_strength = float(settings.get('dynamicGateStrength', os.getenv("SPEECHBRAIN_DYNAMIC_GATE_STRENGTH", "0.5")))
    
    log_info(f"     - –û—á—ñ–∫—É–≤–∞–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Å–ø—ñ–∫–µ—Ä—ñ–≤: {num_speakers}")
    log_info(f"     - Target sample rate: {target_sample_rate} Hz")
    log_info(f"     - –†–æ–∑–º—ñ—Ä —á–∞–Ω–∫–∞ (sliding window): {chunk_size_seconds} —Å–µ–∫")
    log_info(f"     - Overlap –º—ñ–∂ —á–∞–Ω–∫–∞–º–∏: {overlap_seconds} —Å–µ–∫")
    log_info("")
    log_info("   üéØ –ö—Ä–∏—Ç–∏—á–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ —è–∫–æ—Å—Ç—ñ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è:")
    log_info(f"     - Segment Overlap: {segment_overlap} —Å–µ–∫ (–≤–ø–ª–∏–≤–∞—î –Ω–∞ –∑–∞–ª–∏—à–∫–∏ —Å–ø—ñ–∫–µ—Ä—ñ–≤)")
    log_info(f"     - Min Intersegment Gap: {min_intersegment_gap} —Å–µ–∫ (–≤–ø–ª–∏–≤–∞—î –Ω–∞ '—Å–ª–∏–ø–∞–Ω–Ω—è' –≥–æ–ª–æ—Å—ñ–≤)")
    log_info(f"     - Strict Mode: {strict_mode} (–∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è —è–∫–æ—Å—Ç—ñ!)")
    log_info(f"     - VAD Threshold: {vad_threshold} (—á—É—Ç–ª–∏–≤—ñ—Å—Ç—å –¥–æ –≥–æ–ª–æ—Å—É)")
    log_info(f"     - Max Speech Duration: {max_speech_duration} —Å–µ–∫ (–º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ —Ç—Ä–∏–≤–∞–ª—ñ—Å—Ç—å —Å–µ–≥–º–µ–Ω—Ç—É)")
    log_info("")
    log_info("   –î–æ–¥–∞—Ç–∫–æ–≤—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏:")
    log_info(f"     - Batch Size: {batch_size}")
    log_info(f"     - Dynamic Batching: {dynamic_batching}")
    log_info(f"     - VAD Model: {vad_model}")
    log_info(f"     - Diarization Model: {diarization_model}")
    log_info("")
    log_info("   üéØ –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –ø–æ—Å—Ç-–æ–±—Ä–æ–±–∫–∏ –¥–ª—è –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –æ–¥–Ω–æ—á–∞—Å–Ω–∏—Ö –≥–æ–ª–æ—Å—ñ–≤:")
    log_info(f"     - Spectral Gating: {enable_spectral_gating} (–≤–∏–¥–∞–ª–µ–Ω–Ω—è –∑–∞–ª–∏—à–∫—ñ–≤ —ñ–Ω—à–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞)")
    if enable_spectral_gating:
        log_info(f"       - Gate Threshold: {spectral_gate_threshold}")
        log_info(f"       - Gate Alpha: {spectral_gate_alpha}")
    log_info(f"     - Speaker Enhancement: {enable_speaker_enhancement} (–ø—ñ–¥—Å–∏–ª–µ–Ω–Ω—è –≤—ñ–¥–º—ñ–Ω–Ω–æ—Å—Ç–µ–π –º—ñ–∂ –≥–æ–ª–æ—Å–∞–º–∏)")
    if enable_speaker_enhancement:
        log_info(f"       - Enhancement Strength: {speaker_enhancement_strength}")
    log_info("")
    log_info("   üîÑ –ê–¥–∞–ø—Ç–∏–≤–Ω–∞ –æ–±—Ä–æ–±–∫–∞ –¥–ª—è –∑–∞–ø–æ–±—ñ–≥–∞–Ω–Ω—è —Å–≤—ñ—Ç—á—É –≥–æ–ª–æ—Å—ñ–≤ –ø—Ä–∏ –∑–º—ñ–Ω—ñ –≥—É—á–Ω–æ—Å—Ç—ñ:")
    log_info(f"     - Adaptive Volume Tracking: {enable_adaptive_volume_tracking} (–≤—ñ–¥—Å—Ç–µ–∂–µ–Ω–Ω—è —Ç–∞ –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è –≥—É—á–Ω–æ—Å—Ç—ñ)")
    if enable_adaptive_volume_tracking:
        log_info(f"       - Smoothing Alpha: {adaptive_volume_alpha}")
    log_info(f"     - Dynamic Speaker Gating: {enable_dynamic_speaker_gating} (–¥–∏–Ω–∞–º—ñ—á–Ω–µ –≥–µ–π—Ç—É–≤–∞–Ω–Ω—è –Ω–∞ –æ—Å–Ω–æ–≤—ñ –µ–Ω–µ—Ä–≥—ñ—ó)")
    if enable_dynamic_speaker_gating:
        log_info(f"       - Min Energy Ratio: {dynamic_gate_min_energy_ratio}")
        log_info(f"       - Gate Strength: {dynamic_gate_strength}")
    log_info("")

    # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è VAD –º–æ–¥–µ–ª—ñ (—è–∫—â–æ –¥–æ—Å—Ç—É–ø–Ω–∞ —Ç–∞ –ø–æ—Ç—Ä—ñ–±–Ω–∞)
    # –ü—Ä–∏–º—ñ—Ç–∫–∞: VAD —ñ–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—è –ø–æ–∫–∏ —â–æ –æ–±–º–µ–∂–µ–Ω–∞ —á–µ—Ä–µ–∑ —Å–∫–ª–∞–¥–Ω—ñ—Å—Ç—å API
    # –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ vad_threshold —Ç–∞ vad_model –∑–±–µ—Ä—ñ–≥–∞—é—Ç—å—Å—è –¥–ª—è –º–∞–π–±—É—Ç–Ω—å–æ—ó —ñ–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—ó
    vad_model_instance = None
    if VAD_AVAILABLE and vad_threshold > 0:
        try:
            log_info("üì¶ –°–ø—Ä–æ–±–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è VAD –º–æ–¥–µ–ª—ñ...")
            log_info(f"   VAD Model: {vad_model}")
            log_info(f"   VAD Threshold: {vad_threshold}")
            log_info("   ‚ö†Ô∏è  VAD —ñ–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—è –ø–æ–∫–∏ —â–æ –≤ —Ä–æ–∑—Ä–æ–±—Ü—ñ, –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –∑–±–µ—Ä—ñ–≥–∞—é—Ç—å—Å—è –¥–ª—è –ª–æ–≥—É–≤–∞–Ω–Ω—è")
            # TODO: –ü–æ–≤–Ω–∞ —ñ–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—è VAD –ø–æ—Ç—Ä–µ–±—É—î –¥–æ–¥–∞—Ç–∫–æ–≤–æ–≥–æ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è
            vad_model_instance = None
        except Exception as e:
            log_error(f"[SpeechBrain] VAD not available: {e}")
            log_error("   –ü—Ä–æ–¥–æ–≤–∂—É—î–º–æ –±–µ–∑ VAD...")
            vad_model_instance = None
    else:
        if not VAD_AVAILABLE:
            log_info("   ‚ÑπÔ∏è  VAD –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–∞ –≤ —Ü—ñ–π –≤–µ—Ä—Å—ñ—ó SpeechBrain")
        log_info(f"   VAD Threshold: {vad_threshold} (–ø–∞—Ä–∞–º–µ—Ç—Ä –∑–±–µ—Ä–µ–∂–µ–Ω–æ)")

    # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è
    log_info("üì¶ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è...")
    model_load_start = time.time()
    model = Separator.from_hparams(
        source="speechbrain/sepformer-whamr16k",
        savedir=cache_dir,
        run_opts={"device": device},
    )
    model_load_time = time.time() - model_load_start
    log_info(f"‚úÖ –ú–æ–¥–µ–ª—å —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞ –∑–∞ {model_load_time:.2f} —Å–µ–∫")
    log_info(f"   –ö—ñ–ª—å–∫—ñ—Å—Ç—å —Å–ø—ñ–∫–µ—Ä—ñ–≤ —É –º–æ–¥–µ–ª—ñ: {model.hparams.num_spks}")
    log_info("")

    # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∞—É–¥—ñ–æ (–∑ –æ–±–æ–≤'—è–∑–∫–æ–≤–∏–º —Ä–µ—Å–µ–º–ø–ª—ñ–Ω–≥–æ–º –¥–æ target_sample_rate)
    waveform, sample_rate = load_waveform(audio_path, target_sample_rate)
    log_info("")
    
    # –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è –≥—É—á–Ω–æ—Å—Ç—ñ –¥–ª—è —É–Ω–∏–∫–Ω–µ–Ω–Ω—è –ø–ª—É—Ç–∞–Ω–∏–Ω–∏ —Å–ø—ñ–∫–µ—Ä—ñ–≤ —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–ø–∞–¥–∏ –≥—É—á–Ω–æ—Å—Ç—ñ
    # –ë—ñ–ª—å—à –∞–≥—Ä–µ—Å–∏–≤–Ω–∞ –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º (0.80 –∑–∞–º—ñ—Å—Ç—å 0.95)
    normalization_method = settings.get('normalizationMethod', os.getenv("SPEECHBRAIN_NORMALIZATION_METHOD", "peak"))
    normalization_level = float(settings.get('normalizationLevel', os.getenv("SPEECHBRAIN_NORMALIZATION_LEVEL", "0.80")))
    
    log_info("üîä –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è —Ä—ñ–≤–Ω—è –≥—É—á–Ω–æ—Å—Ç—ñ...")
    log_info(f"   –ú–µ—Ç–æ–¥: {normalization_method}")
    log_info(f"   –¶—ñ–ª—å–æ–≤–∏–π —Ä—ñ–≤–µ–Ω—å: {normalization_level}")
    
    normalization_start = time.time()
    waveform_before_norm = waveform.clone()
    max_before = torch.abs(waveform_before_norm).max().item()
    rms_before = torch.sqrt(torch.mean(waveform_before_norm ** 2)).item()
    
    waveform = normalize_audio_level(waveform, method=normalization_method, target_level=normalization_level)
    
    max_after = torch.abs(waveform).max().item()
    rms_after = torch.sqrt(torch.mean(waveform ** 2)).item()
    normalization_time = time.time() - normalization_start
    
    log_info(f"‚úÖ –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {normalization_time:.2f} —Å–µ–∫")
    log_info(f"   Peak –¥–æ: {max_before:.4f}, –ø—ñ—Å–ª—è: {max_after:.4f}")
    log_info(f"   RMS –¥–æ: {rms_before:.4f}, –ø—ñ—Å–ª—è: {rms_after:.4f}")
    log_info("")
    
    # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ model_load_time –¥–ª—è –ø—ñ–¥—Å—É–º–∫—É
    model_load_time_for_summary = model_load_time

    # –ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è VAD (—è–∫—â–æ –¥–æ—Å—Ç—É–ø–Ω–∞)
    if vad_model_instance is not None:
        log_info("üé§ –ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è VAD –¥–ª—è –≤–∏—è–≤–ª–µ–Ω–Ω—è –∞–∫—Ç–∏–≤–Ω–∏—Ö —Å–µ–≥–º–µ–Ω—Ç—ñ–≤...")
        vad_start = time.time()
        try:
            # VAD –ø–æ–≤–µ—Ä—Ç–∞—î –º–∞—Å–∫—É –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ
            vad_boundaries = vad_model_instance(waveform.squeeze().numpy(), sample_rate)
            vad_time = time.time() - vad_start
            log_info(f"‚úÖ VAD –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {vad_time:.2f} —Å–µ–∫")
            log_info(f"   –ó–Ω–∞–π–¥–µ–Ω–æ {len(vad_boundaries)} –∞–∫—Ç–∏–≤–Ω–∏—Ö —Å–µ–≥–º–µ–Ω—Ç—ñ–≤")
            
            # –§—ñ–ª—å—Ç—Ä—É—î–º–æ —Å–µ–≥–º–µ–Ω—Ç–∏ –∑–∞ threshold
            filtered_segments = []
            for start, end in vad_boundaries:
                # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ confidence (—è–∫—â–æ –¥–æ—Å—Ç—É–ø–Ω–æ) –∞–±–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ threshold
                # –î–ª—è –ø—Ä–æ—Å—Ç–æ—Ç–∏, –ø—Ä–∏–π–º–∞—î–º–æ –≤—Å—ñ —Å–µ–≥–º–µ–Ω—Ç–∏, —è–∫—ñ VAD –≤–∏—è–≤–∏–≤
                if end - start >= 0.1:  # –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ —Ç—Ä–∏–≤–∞–ª—ñ—Å—Ç—å 100–º—Å
                    filtered_segments.append((start, end))
            
            log_info(f"   –ü—ñ—Å–ª—è —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó: {len(filtered_segments)} —Å–µ–≥–º–µ–Ω—Ç—ñ–≤")
        except Exception as e:
            log_error(f"[SpeechBrain] VAD processing failed: {e}")
            log_error("   –ü—Ä–æ–¥–æ–≤–∂—É—î–º–æ –±–µ–∑ VAD —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó...")
            vad_boundaries = None
    else:
        vad_boundaries = None

    total_samples = waveform.shape[1]
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –¥–ª—è sliding window chunking
    chunk_size_samples = int(chunk_size_seconds * sample_rate)
    overlap_samples = int(overlap_seconds * sample_rate)
    step_size_samples = chunk_size_samples - overlap_samples  # –ö—Ä–æ–∫ –º—ñ–∂ —á–∞–Ω–∫–∞–º–∏
    
    # Store final sample rate for saving files
    final_sample_rate = sample_rate

    log_info(f"üìä –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ —Ö–≤–∏–ª—ñ –ø–µ—Ä–µ–¥ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è–º:")
    log_info(f"   –§–æ—Ä–º–∞: {waveform.shape}")
    log_info(f"   –¢–∏–ø –¥–∞–Ω–∏—Ö: {waveform.dtype}")
    log_info(f"   –ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑—Ä–∞–∑–∫—ñ–≤: {total_samples}")
    log_info(f"   –†–æ–∑–º—ñ—Ä —á–∞–Ω–∫–∞: {chunk_size_samples} –∑—Ä–∞–∑–∫—ñ–≤ ({chunk_size_seconds} —Å–µ–∫)")
    log_info(f"   Overlap: {overlap_samples} –∑—Ä–∞–∑–∫—ñ–≤ ({overlap_seconds} —Å–µ–∫)")
    log_info(f"   Step size: {step_size_samples} –∑—Ä–∞–∑–∫—ñ–≤ ({step_size_samples / sample_rate:.2f} —Å–µ–∫)")
    if vad_boundaries:
        log_info(f"   VAD –∞–∫—Ç–∏–≤–Ω—ñ —Å–µ–≥–º–µ–Ω—Ç–∏: {len(vad_boundaries)}")
    log_info("")

    # 3. –ü—ñ–¥ —á–∞—Å –∞–Ω–∞–ª—ñ–∑—É –∞—É–¥—ñ–æ
    log_info("üîÄ –ï–¢–ê–ü 3: –ê–ù–ê–õ–Ü–ó –ê–£–î–Ü–û –¢–ê –†–û–ó–î–Ü–õ–ï–ù–ù–Ø")
    
    def separate_chunk(chunk_tensor: torch.Tensor, chunk_index: int = None, total_chunks: int = None):
        chunk_start_time = time.time()
        
        # –ù–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ —Ñ–æ—Ä–º—É —Ç–µ–Ω–∑–æ—Ä–∞ –¥–æ [channels, samples]
        original_shape = chunk_tensor.shape
        log_info(f"   –ü–æ—á–∞—Ç–∫–æ–≤–∞ —Ñ–æ—Ä–º–∞ —Ç–µ–Ω–∑–æ—Ä–∞: {original_shape}")
        
        # –í–∏–¥–∞–ª—è—î–º–æ –≤—Å—ñ –∑–∞–π–≤—ñ –≤–∏–º—ñ—Ä–∏, –ø–æ–∫–∏ –Ω–µ –æ—Ç—Ä–∏–º–∞—î–º–æ 2D
        while chunk_tensor.dim() > 2:
            chunk_tensor = chunk_tensor.squeeze(0)
        
        # –ü–µ—Ä–µ–∫–æ–Ω—É—î–º–æ—Å—è, —â–æ –º–∞—î–º–æ —Ñ–æ—Ä–º—É [channels, samples]
        if chunk_tensor.dim() == 1:
            # –Ø–∫—â–æ 1D, –¥–æ–¥–∞—î–º–æ channel dimension
            chunk_tensor = chunk_tensor.unsqueeze(0)  # [1, samples]
        elif chunk_tensor.dim() != 2:
            raise ValueError(f"Unexpected tensor dimension after normalization: {chunk_tensor.dim()}, shape: {chunk_tensor.shape}")
        
        # –ü–µ—Ä–µ–∫–æ–Ω—É—î–º–æ—Å—è, —â–æ –º–∞—î–º–æ –æ–¥–∏–Ω –∫–∞–Ω–∞–ª (–º–æ–Ω–æ)
        if chunk_tensor.shape[0] > 1:
            # –Ø–∫—â–æ –±–∞–≥–∞—Ç–æ –∫–∞–Ω–∞–ª—ñ–≤, –æ–±'—î–¥–Ω—É—î–º–æ –≤ –º–æ–Ω–æ
            chunk_tensor = chunk_tensor.mean(dim=0, keepdim=True)
        
        chunk_size = chunk_tensor.shape[1]
        chunk_duration = chunk_size / sample_rate
        
        if chunk_index is not None:
            log_info(f"üì¶ –û–±—Ä–æ–±–∫–∞ —Å–µ–≥–º–µ–Ω—Ç–∞ {chunk_index + 1}/{total_chunks}")
        else:
            log_info(f"üì¶ –û–±—Ä–æ–±–∫–∞ —Å–µ–≥–º–µ–Ω—Ç–∞")
        log_info(f"   –†–æ–∑–º—ñ—Ä —Å–µ–≥–º–µ–Ω—Ç–∞: {chunk_size} –∑—Ä–∞–∑–∫—ñ–≤ ({chunk_duration:.2f} —Å–µ–∫)")
        log_info(f"   –ù–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω–∞ —Ñ–æ—Ä–º–∞: {chunk_tensor.shape} (–æ—á—ñ–∫—É—î—Ç—å—Å—è [1, samples])")
        
        # –ü–µ—Ä–µ–∫–æ–Ω—É—î–º–æ—Å—è, —â–æ —Ñ–æ—Ä–º–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–∞ –ø–µ—Ä–µ–¥ –ø–µ—Ä–µ–¥–∞—á–µ—é –≤ –º–æ–¥–µ–ª—å
        if chunk_tensor.shape[0] != 1:
            raise ValueError(f"Expected 1 channel, got {chunk_tensor.shape[0]} channels. Shape: {chunk_tensor.shape}")
        
        chunk_tensor = chunk_tensor.to(device)
        recognition_start = time.time()
        
        # separate_batch –æ—á—ñ–∫—É—î [batch, channels, samples] –∞–±–æ [channels, samples]
        # –£ –Ω–∞—Å –∑–∞—Ä–∞–∑ [1, samples] = [channels, samples]
        # –°–ø—Ä–æ–±—É—î–º–æ –ø–µ—Ä–µ–¥–∞—Ç–∏ —è–∫ [batch, channels, samples] = [1, 1, samples]
        with torch.no_grad():
            # –ü–µ—Ä–µ–∫–æ–Ω—É—î–º–æ—Å—è, —â–æ –º–∞—î–º–æ –ø—Ä–∞–≤–∏–ª—å–Ω—É —Ñ–æ—Ä–º—É [channels, samples]
            log_info(f"   –§–æ—Ä–º–∞ –ø–µ—Ä–µ–¥ batch: {chunk_tensor.shape} (–æ—á—ñ–∫—É—î—Ç—å—Å—è [1, samples])")
            
            if chunk_tensor.dim() != 2:
                raise ValueError(f"Expected 2D tensor [channels, samples] for model, got {chunk_tensor.dim()}D. Shape: {chunk_tensor.shape}")
            
            if chunk_tensor.shape[0] != 1:
                raise ValueError(f"Expected 1 channel, got {chunk_tensor.shape[0]} channels. Shape: {chunk_tensor.shape}")
            
            # separate_batch –º–æ–∂–µ –ø—Ä–∏–π–º–∞—Ç–∏ [channels, samples] —ñ —Å–∞–º –¥–æ–¥–∞—Å—Ç—å batch dimension
            # –ê–ë–û [batch, channels, samples] —è–∫—â–æ –≤–∂–µ —î batch dimension
            # –°–ø—Ä–æ–±—É—î–º–æ –ø–µ—Ä–µ–¥–∞—Ç–∏ –ø—Ä–æ—Å—Ç–æ [channels, samples] = [1, samples]
            # –Ø–∫—â–æ —Ü–µ –Ω–µ —Å–ø—Ä–∞—Ü—é—î, –º–æ–¥–µ–ª—å —Å–∞–º–∞ –¥–æ–¥–∞—Å—Ç—å batch dimension
            log_info(f"   –ü–µ—Ä–µ–¥–∞—î–º–æ –≤ separate_batch —Ñ–æ—Ä–º—É: {chunk_tensor.shape}")
            
            # –í–∏–∫–ª–∏–∫–∞—î–º–æ separate_batch –∑ [channels, samples] - –≤—ñ–Ω —Å–∞–º –æ–±—Ä–æ–±–∏—Ç—å batch
            result = model.separate_batch(chunk_tensor)
            
            # –†–µ–∑—É–ª—å—Ç–∞—Ç –º–∞—î —Ñ–æ—Ä–º—É [batch, samples, speakers] –∞–±–æ [samples, speakers]
            log_info(f"   –§–æ—Ä–º–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É –≤—ñ–¥ –º–æ–¥–µ–ª—ñ: {result.shape}")
            
            # –ù–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ —Ñ–æ—Ä–º—É –¥–æ [speakers, samples]
            if result.dim() == 3:
                # [batch, samples, speakers] -> [samples, speakers]
                result = result[0]
            elif result.dim() == 2:
                # [samples, speakers] - –≤–∂–µ –ø—Ä–∞–≤–∏–ª—å–Ω–∞ —Ñ–æ—Ä–º–∞
                pass
            else:
                raise ValueError(f"Unexpected result dimension: {result.dim()}, shape: {result.shape}")
            
            # –¢—Ä–∞–Ω—Å–ø–æ–Ω—É—î–º–æ –∑ [samples, speakers] –≤ [speakers, samples]
            # SpeechBrain separate_batch –ø–æ–≤–µ—Ä—Ç–∞—î [samples, speakers] –∞–±–æ [batch, samples, speakers]
            # –ù–∞–º –ø–æ—Ç—Ä—ñ–±–Ω–æ [speakers, samples]
            if result.shape[1] == num_speakers and result.shape[0] != num_speakers:
                # –Ø–∫—â–æ –¥—Ä—É–≥–∏–π –≤–∏–º—ñ—Ä = num_speakers, –∞ –ø–µ—Ä—à–∏–π != num_speakers
                # —Ç–æ —Ü–µ [samples, speakers], –ø–æ—Ç—Ä—ñ–±–Ω–æ —Ç—Ä–∞–Ω—Å–ø–æ–Ω—É–≤–∞—Ç–∏
                result = result.transpose(0, 1)  # [samples, speakers] -> [speakers, samples]
                log_info(f"   –ü—ñ—Å–ª—è —Ç—Ä–∞–Ω—Å–ø–æ–Ω—É–≤–∞–Ω–Ω—è: {result.shape} (–æ—á—ñ–∫—É—î—Ç—å—Å—è [speakers, samples])")
            elif result.shape[0] == num_speakers:
                # –í–∂–µ –ø—Ä–∞–≤–∏–ª—å–Ω–∞ —Ñ–æ—Ä–º–∞ [speakers, samples]
                log_info(f"   –§–æ—Ä–º–∞ –≤–∂–µ –ø—Ä–∞–≤–∏–ª—å–Ω–∞: {result.shape} (–æ—á—ñ–∫—É—î—Ç—å—Å—è [speakers, samples])")
            else:
                # –Ø–∫—â–æ –Ω–µ –º–æ–∂–µ–º–æ –≤–∏–∑–Ω–∞—á–∏—Ç–∏, —Å–ø—Ä–æ–±—É—î–º–æ —Ç—Ä–∞–Ω—Å–ø–æ–Ω—É–≤–∞—Ç–∏ —è–∫—â–æ –¥—Ä—É–≥–∏–π –≤–∏–º—ñ—Ä = num_speakers
                if result.shape[1] == num_speakers:
                    result = result.transpose(0, 1)
                    log_info(f"   –ü—ñ—Å–ª—è —Ç—Ä–∞–Ω—Å–ø–æ–Ω—É–≤–∞–Ω–Ω—è (fallback): {result.shape}")
                else:
                    log_info(f"   ‚ö†Ô∏è  –ù–µ–æ—á—ñ–∫—É–≤–∞–Ω–∞ —Ñ–æ—Ä–º–∞: {result.shape}, num_speakers: {num_speakers}")
                    # –°–ø—Ä–æ–±—É—î–º–æ —Ç—Ä–∞–Ω—Å–ø–æ–Ω—É–≤–∞—Ç–∏ –Ω–∞–≤–ø–∞–∫–∏
                    if result.shape[0] == num_speakers:
                        log_info(f"   –§–æ—Ä–º–∞ –≤–∂–µ –ø—Ä–∞–≤–∏–ª—å–Ω–∞ (fallback): {result.shape}")
                    else:
                        raise ValueError(f"Cannot determine correct shape. Result: {result.shape}, num_speakers: {num_speakers}")
        
        recognition_time = time.time() - recognition_start
        result_cpu = result.cpu()
        
        log_info(f"   –ß–∞—Å —Ä–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è —Å–ø—ñ–∫–µ—Ä—ñ–≤: {recognition_time:.2f} —Å–µ–∫")
        log_info(f"   –§—ñ–Ω–∞–ª—å–Ω–∞ —Ñ–æ—Ä–º–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É: {result_cpu.shape}")
        
        # –ü–µ—Ä–µ–∫–æ–Ω—É—î–º–æ—Å—è, —â–æ —Ñ–æ—Ä–º–∞ [speakers, samples]
        if result_cpu.dim() != 2:
            raise ValueError(f"Expected 2D result [speakers, samples], got {result_cpu.dim()}D. Shape: {result_cpu.shape}")
        if result_cpu.shape[0] != num_speakers:
            raise ValueError(f"Expected {num_speakers} speakers in first dimension, got {result_cpu.shape[0]}. Shape: {result_cpu.shape}")
        
        return result_cpu

    separation_start = time.time()
    
    log_info(f"üì¶ –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ sliding window chunking:")
    log_info(f"   Chunk size: {chunk_size_samples} –∑—Ä–∞–∑–∫—ñ–≤ ({chunk_size_seconds} —Å–µ–∫)")
    log_info(f"   Overlap: {overlap_samples} –∑—Ä–∞–∑–∫—ñ–≤ ({overlap_seconds} —Å–µ–∫)")
    log_info(f"   Step size: {step_size_samples} –∑—Ä–∞–∑–∫—ñ–≤ ({step_size_samples / sample_rate:.2f} —Å–µ–∫)")
    log_info("")
    
    # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ sliding window –¥–ª—è –≤—Å—ñ—Ö —Ñ–∞–π–ª—ñ–≤ (–Ω–∞–≤—ñ—Ç—å –∫–æ—Ä–æ—Ç–∫–∏—Ö)
    if total_samples > chunk_size_samples:
        # –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ –∫—ñ–ª—å–∫–æ—Å—Ç—ñ —á–∞–Ω–∫—ñ–≤
        num_chunks = (total_samples - overlap_samples + step_size_samples - 1) // step_size_samples
        if num_chunks == 0:
            num_chunks = 1
        
        log_info(f"üì¶ –û–±—Ä–æ–±–∫–∞ –≤ {num_chunks} —á–∞–Ω–∫–∞—Ö (sliding window)")
        log_info(f"   –ó–∞–≥–∞–ª—å–Ω–∞ —Ç—Ä–∏–≤–∞–ª—ñ—Å—Ç—å: {total_samples / sample_rate:.2f} —Å–µ–∫")
        log_info("")
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ –≤—ñ–∫–Ω–æ –•–∞–Ω–Ω–∞ –¥–ª—è –ø–ª–∞–≤–Ω–æ–≥–æ –∑—à–∏–≤–∞–Ω–Ω—è
        hann_window = torch.hann_window(overlap_samples * 2)
        hann_left = hann_window[:overlap_samples]  # –î–ª—è –ø–æ—á–∞—Ç–∫—É overlap
        hann_right = hann_window[overlap_samples:]  # –î–ª—è –∫—ñ–Ω—Ü—è overlap
        
        # –ë—É—Ñ–µ—Ä –¥–ª—è –Ω–∞–∫–æ–ø–∏—á–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
        output_buffer = torch.zeros((num_speakers, total_samples))
        prev_chunk_result = None
        
        # –Ü—Å—Ç–æ—Ä—ñ—è –µ–Ω–µ—Ä–≥—ñ—ó —Å–ø—ñ–∫–µ—Ä—ñ–≤ –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ—ó –æ–±—Ä–æ–±–∫–∏
        speaker_energy_history = {}
        
        for chunk_idx in range(num_chunks):
            # –í–∏–∑–Ω–∞—á–∞—î–º–æ –ø–æ–∑–∏—Ü—ñ—ó —á–∞–Ω–∫–∞
            start = chunk_idx * step_size_samples
            end = min(start + chunk_size_samples, total_samples)
            
            # –í–∏—Ç—è–≥—É—î–º–æ chunk
            chunk = waveform[:, start:end]
            
            # –ü–µ—Ä–µ–∫–æ–Ω—É—î–º–æ—Å—è, —â–æ chunk –º–∞—î —Ñ–æ—Ä–º—É [1, samples]
            if chunk.dim() > 2:
                chunk = chunk.squeeze(0)
            if chunk.dim() == 1:
                chunk = chunk.unsqueeze(0)
            if chunk.shape[0] > 1:
                chunk = chunk.mean(dim=0, keepdim=True)
            
            log_info(f"üì¶ –û–±—Ä–æ–±–∫–∞ —á–∞–Ω–∫–∞ {chunk_idx + 1}/{num_chunks}")
            log_info(f"   –ü–æ–∑–∏—Ü—ñ—è: {start}-{end} –∑—Ä–∞–∑–∫—ñ–≤ ({start/sample_rate:.2f}-{end/sample_rate:.2f} —Å–µ–∫)")
            
            # –õ–æ–∫–∞–ª—å–Ω–∞ –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è —á–∞–Ω–∫–∞ –¥–ª—è –∫—Ä–∞—â–æ–≥–æ –≤–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è –≥—É—á–Ω–æ—Å—Ç—ñ
            # –¶–µ –¥–æ–ø–æ–º–∞–≥–∞—î —É–Ω–∏–∫–Ω—É—Ç–∏ –ø–µ—Ä–µ–ø–∞–¥—ñ–≤ –≥—É—á–Ω–æ—Å—Ç—ñ –º—ñ–∂ —á–∞–Ω–∫–∞–º–∏
            chunk = normalize_audio_level(chunk, method=normalization_method, target_level=normalization_level)
            
            # –û–±—Ä–æ–±–∫–∞ —á–∞–Ω–∫–∞ –≤ —Ä–µ–∂–∏–º—ñ no_grad –¥–ª—è –µ–∫–æ–Ω–æ–º—ñ—ó –ø–∞–º'—è—Ç—ñ
            with torch.no_grad():
                chunk_result = separate_chunk(chunk, chunk_idx, num_chunks)
            
            # –ü–æ—Å—Ç-–æ–±—Ä–æ–±–∫–∞ –¥–ª—è –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –æ–¥–Ω–æ—á–∞—Å–Ω–∏—Ö –≥–æ–ª–æ—Å—ñ–≤
            if enable_spectral_gating:
                log_info("   üîß –ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–æ–≥–æ –≥–µ–π—Ç—É–≤–∞–Ω–Ω—è...")
                chunk_result = apply_spectral_gating(
                    chunk_result, 
                    chunk, 
                    gate_threshold=spectral_gate_threshold,
                    gate_alpha=spectral_gate_alpha
                )
            
            if enable_speaker_enhancement:
                log_info("   üîß –ü—ñ–¥—Å–∏–ª–µ–Ω–Ω—è –≤—ñ–¥–º—ñ–Ω–Ω–æ—Å—Ç–µ–π –º—ñ–∂ —Å–ø—ñ–∫–µ—Ä–∞–º–∏...")
                chunk_result = enhance_speaker_differences(
                    chunk_result,
                    enhancement_strength=speaker_enhancement_strength
                )
            
            # –ê–¥–∞–ø—Ç–∏–≤–Ω–∞ –æ–±—Ä–æ–±–∫–∞ –¥–ª—è –∑–∞–ø–æ–±—ñ–≥–∞–Ω–Ω—è —Å–≤—ñ—Ç—á—É –≥–æ–ª–æ—Å—ñ–≤ –ø—Ä–∏ –∑–º—ñ–Ω—ñ –≥—É—á–Ω–æ—Å—Ç—ñ
            if enable_adaptive_volume_tracking:
                log_info("   üîÑ –ê–¥–∞–ø—Ç–∏–≤–Ω–µ –≤—ñ–¥—Å—Ç–µ–∂–µ–Ω–Ω—è –≥—É—á–Ω–æ—Å—Ç—ñ —Å–ø—ñ–∫–µ—Ä—ñ–≤...")
                chunk_result, speaker_energy_history = adaptive_volume_tracking(
                    chunk_result,
                    speaker_energy_history=speaker_energy_history,
                    alpha=adaptive_volume_alpha
                )
            
            if enable_dynamic_speaker_gating:
                log_info("   üîß –î–∏–Ω–∞–º—ñ—á–Ω–µ –≥–µ–π—Ç—É–≤–∞–Ω–Ω—è —Å–ø—ñ–∫–µ—Ä—ñ–≤...")
                chunk_result = apply_dynamic_speaker_gating(
                    chunk_result,
                    chunk,
                    speaker_energy_history=speaker_energy_history,
                    min_energy_ratio=dynamic_gate_min_energy_ratio,
                    gate_strength=dynamic_gate_strength
                )
            
            # –í–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è –∫–∞–Ω–∞–ª—ñ–≤ (—è–∫—â–æ –Ω–µ –ø–µ—Ä—à–∏–π —á–∞–Ω–∫)
            if prev_chunk_result is not None and overlap_samples > 0:
                chunk_result = align_channels(prev_chunk_result, chunk_result, overlap_samples)
            
            # Overlap-Add –∑—à–∏–≤–∞–Ω–Ω—è –∑ –≤—ñ–∫–Ω–æ–º –•–∞–Ω–Ω–∞
            chunk_samples = chunk_result.shape[1]
            actual_end = min(start + chunk_samples, total_samples)
            actual_chunk_samples = actual_end - start
            
            for speaker_idx in range(num_speakers):
                chunk_data = chunk_result[speaker_idx, :actual_chunk_samples]
                
                # –ó–∞—Å—Ç–æ—Å–æ–≤—É—î–º–æ –≤—ñ–∫–Ω–æ –•–∞–Ω–Ω–∞ –¥–ª—è overlap regions
                if chunk_idx > 0 and overlap_samples > 0:
                    # Fade in –Ω–∞ –ø–æ—á–∞—Ç–∫—É (–≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –ø—Ä–∞–≤—É —á–∞—Å—Ç–∏–Ω—É –≤—ñ–∫–Ω–∞)
                    fade_len = min(overlap_samples, actual_chunk_samples)
                    if fade_len > 0:
                        chunk_data[:fade_len] *= hann_right[:fade_len]
                
                if chunk_idx < num_chunks - 1 and overlap_samples > 0:
                    # Fade out –≤ –∫—ñ–Ω—Ü—ñ (–≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –ª—ñ–≤—É —á–∞—Å—Ç–∏–Ω—É –≤—ñ–∫–Ω–∞)
                    fade_start = max(0, actual_chunk_samples - overlap_samples)
                    fade_len = actual_chunk_samples - fade_start
                    if fade_len > 0:
                        chunk_data[fade_start:] *= hann_left[-fade_len:]
                
                # –î–æ–¥–∞—î–º–æ –¥–æ –±—É—Ñ–µ—Ä–∞
                output_buffer[speaker_idx, start:actual_end] += chunk_data
            
            prev_chunk_result = chunk_result
            log_info("")
        
        est_sources = output_buffer
        log_info("‚úÖ Sliding window –æ–±—Ä–æ–±–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
    else:
        log_info("üì¶ –û–±—Ä–æ–±–∫–∞ —è–∫ –æ–¥–∏–Ω —Å–µ–≥–º–µ–Ω—Ç (—Ñ–∞–π–ª –∑–∞–Ω–∞–¥—Ç–æ –∫–æ—Ä–æ—Ç–∫–∏–π –¥–ª—è chunking)")
        log_info("")
        # –î–æ–¥–∞—Ç–∫–æ–≤–∞ –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è –ø–µ—Ä–µ–¥ –æ–±—Ä–æ–±–∫–æ—é (–Ω–∞–≤—ñ—Ç—å –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ñ–∞–π–ª—ñ–≤)
        waveform = normalize_audio_level(waveform, method=normalization_method, target_level=normalization_level)
        with torch.no_grad():
            est_sources = separate_chunk(waveform)
        
        # –ü–æ—Å—Ç-–æ–±—Ä–æ–±–∫–∞ –¥–ª—è –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –æ–¥–Ω–æ—á–∞—Å–Ω–∏—Ö –≥–æ–ª–æ—Å—ñ–≤
        if enable_spectral_gating:
            log_info("üîß –ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–æ–≥–æ –≥–µ–π—Ç—É–≤–∞–Ω–Ω—è...")
            est_sources = apply_spectral_gating(
                est_sources,
                waveform,
                gate_threshold=spectral_gate_threshold,
                gate_alpha=spectral_gate_alpha
            )
        
        if enable_speaker_enhancement:
            log_info("üîß –ü—ñ–¥—Å–∏–ª–µ–Ω–Ω—è –≤—ñ–¥–º—ñ–Ω–Ω–æ—Å—Ç–µ–π –º—ñ–∂ —Å–ø—ñ–∫–µ—Ä–∞–º–∏...")
            est_sources = enhance_speaker_differences(
                est_sources,
                enhancement_strength=speaker_enhancement_strength
            )
    
    separation_time = time.time() - separation_start
    log_info("")
    log_info(f"‚úÖ –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {separation_time:.2f} —Å–µ–∫")
    log_info("")

    # –û–±—Ä–æ–±–∫–∞ —Ñ–æ—Ä–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É
    log_info("üîÑ –û–±—Ä–æ–±–∫–∞ —Ñ–æ—Ä–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É...")
    log_info(f"   –ü–æ—á–∞—Ç–∫–æ–≤–∞ —Ñ–æ—Ä–º–∞: {est_sources.shape}")
    log_info(f"   –ö—ñ–ª—å–∫—ñ—Å—Ç—å –≤–∏–º—ñ—Ä—ñ–≤: {est_sources.dim()}")
    
    if est_sources.dim() == 3:
        est_sources = est_sources[0]  # [time, num_speakers]
        log_info(f"   –ü—ñ—Å–ª—è –∑–º–µ–Ω—à–µ–Ω–Ω—è –≤–∏–º—ñ—Ä—ñ–≤: {est_sources.shape}")

    if est_sources.dim() == 2:
        if est_sources.shape[0] == model.hparams.num_spks:
            # shape [num_speakers, time]
            sources_tensor = est_sources
            log_info(f"   –§–æ—Ä–º–∞ [num_speakers, time]: {sources_tensor.shape}")
        elif est_sources.shape[1] == model.hparams.num_spks:
            sources_tensor = est_sources.transpose(0, 1)
            log_info(f"   –¢—Ä–∞–Ω—Å–ø–æ–Ω–æ–≤–∞–Ω–æ –¥–æ [num_speakers, time]: {sources_tensor.shape}")
        else:
            error_msg = f"Unexpected est_sources shape: {est_sources.shape}"
            log_info(f"‚ùå {error_msg}")
            raise ValueError(error_msg)
    else:
        error_msg = f"Unsupported est_sources dimension: {est_sources.dim()}"
        log_info(f"‚ùå {error_msg}")
        raise ValueError(error_msg)

    if est_sources is None or len(est_sources) == 0:
        log_info("‚ùå –°–ø—ñ–∫–µ—Ä–∏ –Ω–µ –≤–∏—è–≤–ª–µ–Ω—ñ –≤ –∞—É–¥—ñ–æ")
        return {
            "success": False,
            "error": "No speakers detected in audio",
        }

    log_info(f"‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ {len(sources_tensor)} –¥–∂–µ—Ä–µ–ª")
    log_info("")

    # 4. –ü—ñ—Å–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è
    log_info("‚úÖ –ï–¢–ê–ü 4: –ó–ê–í–ï–†–®–ï–ù–ù–Ø –†–û–ó–î–Ü–õ–ï–ù–ù–Ø")
    log_info(f"   –ö—ñ–ª—å–∫—ñ—Å—Ç—å –æ—Ç—Ä–∏–º–∞–Ω–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ñ–≤: {len(sources_tensor)}")
    log_info("")

    # 5. –ü—Ä–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
    log_info("üíæ –ï–¢–ê–ü 5: –ó–ë–ï–†–ï–ñ–ï–ù–ù–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–Ü–í")
    save_start_time = time.time()
    
    speakers = []
    timeline = []

    for idx, source in enumerate(sources_tensor):
        speaker_name = f"SPEAKER_{idx:02d}"
        output_path = os.path.join(output_dir, f"{speaker_name}.wav")
        
        log_info(f"üíæ –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è {idx + 1}/{len(sources_tensor)}: {speaker_name}")
        log_info(f"   –®–ª—è—Ö: {output_path}")

        source_np = source.squeeze().numpy()
        source_size = source_np.nbytes
        source_size_mb = source_size / (1024 * 1024)
        log_info(f"   –†–æ–∑–º—ñ—Ä –¥–∞–Ω–∏—Ö: {source_size_mb:.2f} MB ({source_size} –±–∞–π—Ç)")
        
        write_start = time.time()
        try:
            sf.write(output_path, source_np, final_sample_rate)
            write_time = time.time() - write_start
            
            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–æ–≥–æ —Ñ–∞–π–ª—É
            if os.path.exists(output_path):
                saved_file_size = os.path.getsize(output_path)
                saved_file_size_mb = saved_file_size / (1024 * 1024)
                log_info(f"   ‚úÖ –§–∞–π–ª –∑–±–µ—Ä–µ–∂–µ–Ω–æ –∑–∞ {write_time:.2f} —Å–µ–∫")
                log_info(f"   –†–æ–∑–º—ñ—Ä —Ñ–∞–π–ª—É: {saved_file_size_mb:.2f} MB ({saved_file_size} –±–∞–π—Ç)")
                log_info(f"   –°—Ç–∞—Ç—É—Å –∑–∞–ø–∏—Å—É: ‚úÖ –£—Å–ø—ñ—à–Ω–æ")
            else:
                log_info(f"   ‚ùå –§–∞–π–ª –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –ø—ñ—Å–ª—è –∑–∞–ø–∏—Å—É")
                log_info(f"   –°—Ç–∞—Ç—É—Å –∑–∞–ø–∏—Å—É: ‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è")
        except Exception as e:
            log_info(f"   ‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–ø–∏—Å—É: {str(e)}")
            log_info(f"   –°—Ç–∞—Ç—É—Å –∑–∞–ø–∏—Å—É: ‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è")
            raise

        duration = source.shape[-1] / sample_rate
        log_info(f"   –¢—Ä–∏–≤–∞–ª—ñ—Å—Ç—å: {duration:.2f} —Å–µ–∫")
        log_info("")

        speakers.append(
            {
                "name": speaker_name,
                "format": "wav",
                "local_path": output_path,
                "isBackground": False,
            }
        )

        timeline.append(
            {
                "speaker": speaker_name,
                "start": 0.0,
                "end": round(duration, 2),
                "duration": round(duration, 2),
            }
        )
    
    save_time = time.time() - save_start_time
    log_info(f"‚úÖ –í—Å—ñ —Ñ–∞–π–ª–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ –∑–∞ {save_time:.2f} —Å–µ–∫")
    log_info("")
    
    total_time = time.time() - process_start_time
    log_info("‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê")
    log_info("üìä –ü–Ü–î–°–£–ú–û–ö –ü–†–û–¶–ï–°–£")
    log_info("‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê")
    log_info(f"   –ó–∞–≥–∞–ª—å–Ω–∏–π —á–∞—Å –æ–±—Ä–æ–±–∫–∏: {total_time:.2f} —Å–µ–∫")
    log_info(f"   –ß–∞—Å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ: {model_load_time_for_summary:.2f} —Å–µ–∫")
    log_info(f"   –ß–∞—Å —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è: {separation_time:.2f} —Å–µ–∫")
    log_info(f"   –ß–∞—Å –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è: {save_time:.2f} —Å–µ–∫")
    log_info(f"   –ö—ñ–ª—å–∫—ñ—Å—Ç—å —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ñ–≤: {len(speakers)}")
    log_info(f"   –°—Ç–∞—Ç—É—Å: ‚úÖ –£—Å–ø—ñ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
    log_info("‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê")

    return {
        "success": True,
        "speakers": speakers,
        "timeline": timeline,
        "output_dir": output_dir,
        "num_speakers": len(speakers),
    }


def main():
    if len(sys.argv) < 3:
        print(
            json.dumps(
                {
                    "success": False,
                    "error": "Usage: python speechbrain_separation.py <audio_path> <output_dir> [settings_json]",
                }
            )
        )
        sys.exit(1)

    audio_path = sys.argv[1]
    output_dir = ensure_output_dir(sys.argv[2])
    
    # Parse settings from JSON (if provided)
    settings = {}
    if len(sys.argv) >= 4:
        try:
            settings = json.loads(sys.argv[3])
        except (json.JSONDecodeError, ValueError) as e:
            log_error(f"[SpeechBrain] Warning: Failed to parse settings JSON: {e}")
            settings = {}

    if not os.path.isfile(audio_path):
        print(
            json.dumps(
                {"success": False, "error": f"Audio file not found: {audio_path}"}
            )
        )
        sys.exit(1)

    try:
        result = separate(audio_path, output_dir, settings)
        print(json.dumps(result))
        sys.exit(0 if result.get("success") else 1)
    except Exception as exc:
        # 6. –£ –≤–∏–ø–∞–¥–∫—É –ø–æ–º–∏–ª–æ–∫
        import traceback
        log_info("")
        log_info("‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê")
        log_info("‚ùå –ü–û–ú–ò–õ–ö–ê –ü–†–û–¶–ï–°–£")
        log_info("‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê")
        log_info(f"   –ö–æ–¥ –ø–æ–º–∏–ª–∫–∏: {type(exc).__name__}")
        log_info(f"   –û–ø–∏—Å: {str(exc)}")
        log_info("   –°—Ç–µ–∫ —Ç—Ä–∞—Å—É–≤–∞–Ω–Ω—è:")
        for line in traceback.format_exc().split('\n'):
            if line.strip():
                log_info(f"      {line}")
        log_info("‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê")
        
        print(json.dumps({"success": False, "error": str(exc)}))
        sys.exit(1)


if __name__ == "__main__":
    main()

