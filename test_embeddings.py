#!/usr/bin/env python3
"""
–¢–µ—Å—Ç–æ–≤–∏–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –≤–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è –µ–º–±–µ–¥–¥–∏–Ω–≥—ñ–≤ SpeechBrain
"""

import os
import sys
import numpy as np
import torch
import traceback
import tempfile
import soundfile as sf

# –ü–∞—Ç—á –¥–ª—è torchaudio —Å—É–º—ñ—Å–Ω–æ—Å—Ç—ñ –∑ speechbrain
exec(open('patch_torchaudio.py').read())

from speechbrain.pretrained import SpeakerRecognition

print("="*60)
print("üß™ TEST: SpeechBrain Embedding Extraction")
print("="*60)

# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ
print("\nüîÑ Loading SpeechBrain model...")
try:
    speaker_model = SpeakerRecognition.from_hparams(
        source="speechbrain/spkrec-ecapa-voxceleb",
        savedir="pretrained_models/spkrec-ecapa-voxceleb"
    )
    print("‚úÖ Model loaded successfully!")
except Exception as e:
    print(f"‚ùå Error loading model: {e}")
    traceback.print_exc()
    sys.exit(1)

# Device –º–æ–¥–µ–ª—ñ
try:
    device = next(speaker_model.parameters()).device
    print(f"üì± Model device: {device}")
except:
    device = torch.device('cpu')
    print(f"üì± Using default device: {device}")

# –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞ (2 —Å–µ–∫—É–Ω–¥–∏, 16kHz)
print("\nüéµ Creating test audio segment (2 seconds, 16kHz)...")
sr = 16000
duration = 2.0
test_audio = np.random.randn(int(sr * duration)).astype(np.float32)
print(f"‚úÖ Test audio created: shape={test_audio.shape}, dtype={test_audio.dtype}, samples={len(test_audio)}")

# –¢–µ—Å—Ç 1: encode_batch –∑ normalize=False, —Ñ–æ—Ä–º–∞—Ç [1, 1, samples]
print("\n" + "-"*60)
print("TEST 1: encode_batch(tensor [1,1,samples], normalize=False)")
print("-"*60)
try:
    segment_tensor = torch.tensor(test_audio, dtype=torch.float32).unsqueeze(0).unsqueeze(0)
    print(f"üìä Tensor shape: {segment_tensor.shape}, dtype: {segment_tensor.dtype}, device: {segment_tensor.device}")
    
    embedding = speaker_model.encode_batch(segment_tensor, normalize=False)
    embedding = embedding.squeeze().cpu().detach().numpy()
    
    print(f"‚úÖ SUCCESS! Embedding shape: {embedding.shape}, dtype: {embedding.dtype}")
    print(f"üìä Embedding stats: min={embedding.min():.4f}, max={embedding.max():.4f}, mean={embedding.mean():.4f}")
    if np.any(np.isnan(embedding)) or np.any(np.isinf(embedding)):
        print("‚ö†Ô∏è  WARNING: NaN or Inf found in embedding!")
    else:
        print("‚úÖ No NaN or Inf in embedding")
except Exception as e:
    print(f"‚ùå FAILED: {e}")
    traceback.print_exc()

# –¢–µ—Å—Ç 2: encode_batch –±–µ–∑ normalize, —Ñ–æ—Ä–º–∞—Ç [1, 1, samples]
print("\n" + "-"*60)
print("TEST 2: encode_batch(tensor [1,1,samples]) - –±–µ–∑ normalize")
print("-"*60)
try:
    segment_tensor = torch.tensor(test_audio, dtype=torch.float32).unsqueeze(0).unsqueeze(0)
    print(f"üìä Tensor shape: {segment_tensor.shape}, dtype: {segment_tensor.dtype}, device: {segment_tensor.device}")
    
    embedding = speaker_model.encode_batch(segment_tensor)
    embedding = embedding.squeeze().cpu().detach().numpy()
    
    print(f"‚úÖ SUCCESS! Embedding shape: {embedding.shape}, dtype: {embedding.dtype}")
    print(f"üìä Embedding stats: min={embedding.min():.4f}, max={embedding.max():.4f}, mean={embedding.mean():.4f}")
    if np.any(np.isnan(embedding)) or np.any(np.isinf(embedding)):
        print("‚ö†Ô∏è  WARNING: NaN or Inf found in embedding!")
    else:
        print("‚úÖ No NaN or Inf in embedding")
except Exception as e:
    print(f"‚ùå FAILED: {e}")
    traceback.print_exc()

# –¢–µ—Å—Ç 3: encode_batch –∑ —Ñ–æ—Ä–º–∞—Ç [1, samples] (–±–µ–∑ –ø–æ–¥–≤—ñ–π–Ω–æ–≥–æ unsqueeze)
print("\n" + "-"*60)
print("TEST 3: encode_batch(tensor [1,samples]) - –±–µ–∑ –ø–æ–¥–≤—ñ–π–Ω–æ–≥–æ unsqueeze")
print("-"*60)
try:
    segment_tensor = torch.tensor(test_audio, dtype=torch.float32).unsqueeze(0)
    print(f"üìä Tensor shape: {segment_tensor.shape}, dtype: {segment_tensor.dtype}, device: {segment_tensor.device}")
    
    embedding = speaker_model.encode_batch(segment_tensor)
    embedding = embedding.squeeze().cpu().detach().numpy()
    
    print(f"‚úÖ SUCCESS! Embedding shape: {embedding.shape}, dtype: {embedding.dtype}")
    print(f"üìä Embedding stats: min={embedding.min():.4f}, max={embedding.max():.4f}, mean={embedding.mean():.4f}")
    if np.any(np.isnan(embedding)) or np.any(np.isinf(embedding)):
        print("‚ö†Ô∏è  WARNING: NaN or Inf found in embedding!")
    else:
        print("‚úÖ No NaN or Inf in embedding")
except Exception as e:
    print(f"‚ùå FAILED: {e}")
    traceback.print_exc()

# –¢–µ—Å—Ç 4: encode_file (—è–∫—â–æ –¥–æ—Å—Ç—É–ø–Ω–∏–π)
print("\n" + "-"*60)
print("TEST 4: encode_file() - —á–µ—Ä–µ–∑ —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª")
print("-"*60)
if hasattr(speaker_model, 'encode_file'):
    try:
        with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp_file:
            sf.write(tmp_file.name, test_audio, sr)
            tmp_path = tmp_file.name
        
        print(f"üìÅ Temporary file created: {tmp_path}")
        
        embedding = speaker_model.encode_file(tmp_path)
        embedding = embedding.squeeze().cpu().detach().numpy()
        
        print(f"‚úÖ SUCCESS! Embedding shape: {embedding.shape}, dtype: {embedding.dtype}")
        print(f"üìä Embedding stats: min={embedding.min():.4f}, max={embedding.max():.4f}, mean={embedding.mean():.4f}")
        if np.any(np.isnan(embedding)) or np.any(np.isinf(embedding)):
            print("‚ö†Ô∏è  WARNING: NaN or Inf found in embedding!")
        else:
            print("‚úÖ No NaN or Inf in embedding")
        
        # –í–∏–¥–∞–ª—è—î–º–æ —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª
        os.unlink(tmp_path)
    except Exception as e:
        print(f"‚ùå FAILED: {e}")
        traceback.print_exc()
        try:
            if 'tmp_path' in locals():
                os.unlink(tmp_path)
        except:
            pass
else:
    print("‚ùå Model does not have 'encode_file' method")

# –¢–µ—Å—Ç 5: –ü—Ä—è–º–∏–π –¥–æ—Å—Ç—É–ø —á–µ—Ä–µ–∑ encoder + embedding_model
print("\n" + "-"*60)
print("TEST 5: Direct access via mods.encoder() + mods.embedding_model()")
print("-"*60)
if hasattr(speaker_model, 'mods') and hasattr(speaker_model.mods, 'encoder'):
    try:
        segment_tensor = torch.tensor(test_audio, dtype=torch.float32).unsqueeze(0)
        wav_lens = torch.tensor([len(test_audio) / sr], dtype=torch.float32)
        
        print(f"üìä Tensor shape: {segment_tensor.shape}, dtype: {segment_tensor.dtype}, device: {segment_tensor.device}")
        print(f"üìä wav_lens: {wav_lens}")
        
        with torch.no_grad():
            features = speaker_model.mods.encoder(segment_tensor, wav_lens=wav_lens)
            print(f"üìä Features shape after encoder: {features.shape}")
            
            if hasattr(speaker_model.mods, 'embedding_model'):
                embedding = speaker_model.mods.embedding_model(features, wav_lens=wav_lens)
            else:
                embedding = features
            
            embedding = embedding.squeeze().cpu().detach().numpy()
        
        print(f"‚úÖ SUCCESS! Embedding shape: {embedding.shape}, dtype: {embedding.dtype}")
        print(f"üìä Embedding stats: min={embedding.min():.4f}, max={embedding.max():.4f}, mean={embedding.mean():.4f}")
        if np.any(np.isnan(embedding)) or np.any(np.isinf(embedding)):
            print("‚ö†Ô∏è  WARNING: NaN or Inf found in embedding!")
        else:
            print("‚úÖ No NaN or Inf in embedding")
    except Exception as e:
        print(f"‚ùå FAILED: {e}")
        traceback.print_exc()
else:
    print("‚ùå Model does not have 'mods.encoder'")

# –¢–µ—Å—Ç 6: –¢—ñ–ª—å–∫–∏ encoder –±–µ–∑ embedding_model
print("\n" + "-"*60)
print("TEST 6: Direct access via mods.encoder() only (no embedding_model)")
print("-"*60)
if hasattr(speaker_model, 'mods') and hasattr(speaker_model.mods, 'encoder'):
    try:
        segment_tensor = torch.tensor(test_audio, dtype=torch.float32).unsqueeze(0)
        wav_lens = torch.tensor([len(test_audio) / sr], dtype=torch.float32)
        
        print(f"üìä Tensor shape: {segment_tensor.shape}, dtype: {segment_tensor.dtype}, device: {segment_tensor.device}")
        
        with torch.no_grad():
            embedding = speaker_model.mods.encoder(segment_tensor, wav_lens=wav_lens)
            embedding = embedding.squeeze().cpu().detach().numpy()
        
        print(f"‚úÖ SUCCESS! Embedding shape: {embedding.shape}, dtype: {embedding.dtype}")
        print(f"üìä Embedding stats: min={embedding.min():.4f}, max={embedding.max():.4f}, mean={embedding.mean():.4f}")
        if np.any(np.isnan(embedding)) or np.any(np.isinf(embedding)):
            print("‚ö†Ô∏è  WARNING: NaN or Inf found in embedding!")
        else:
            print("‚úÖ No NaN or Inf in embedding")
    except Exception as e:
        print(f"‚ùå FAILED: {e}")
        traceback.print_exc()
else:
    print("‚ùå Model does not have 'mods.encoder'")

# –¢–µ—Å—Ç 7: –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ device (–∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—è –Ω–∞ device –º–æ–¥–µ–ª—ñ)
print("\n" + "-"*60)
print("TEST 7: encode_batch with tensor on model device")
print("-"*60)
try:
    segment_tensor = torch.tensor(test_audio, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)
    print(f"üìä Tensor shape: {segment_tensor.shape}, dtype: {segment_tensor.dtype}, device: {segment_tensor.device}")
    
    embedding = speaker_model.encode_batch(segment_tensor, normalize=False)
    embedding = embedding.squeeze().cpu().detach().numpy()
    
    print(f"‚úÖ SUCCESS! Embedding shape: {embedding.shape}, dtype: {embedding.dtype}")
    print(f"üìä Embedding stats: min={embedding.min():.4f}, max={embedding.max():.4f}, mean={embedding.mean():.4f}")
    if np.any(np.isnan(embedding)) or np.any(np.isinf(embedding)):
        print("‚ö†Ô∏è  WARNING: NaN or Inf found in embedding!")
    else:
        print("‚úÖ No NaN or Inf in embedding")
except Exception as e:
    print(f"‚ùå FAILED: {e}")
    traceback.print_exc()

print("\n" + "="*60)
print("üèÅ TESTING COMPLETE")
print("="*60)



