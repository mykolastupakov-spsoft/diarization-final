#!/usr/bin/env python3
"""
Flask —Å–µ—Ä–≤–µ—Ä –¥–ª—è demo2: SpeechBrain –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—è + Whisper —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—è
"""

import os
import json
import numpy as np
import torch
import librosa
import soundfile as sf
import requests
from flask import Flask, request, jsonify, send_from_directory
from flask_cors import CORS
import time
from werkzeug.utils import secure_filename
from cascading_diarization import CascadingDiarizationController, DiarizationSegment

# –ü–∞—Ç—á –¥–ª—è torchaudio —Å—É–º—ñ—Å–Ω–æ—Å—Ç—ñ –∑ speechbrain (–∑–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –î–û —ñ–º–ø–æ—Ä—Ç—É speechbrain)
exec(open('patch_torchaudio.py').read())

from speechbrain.pretrained import SpeakerRecognition
from sklearn.cluster import SpectralClustering
from scipy.spatial.distance import pdist, squareform
import whisper
import warnings
from pathlib import Path
import tempfile

warnings.filterwarnings("ignore")

app = Flask(__name__)
# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è CORS –¥–ª—è iOS Shortcuts (–ø—ñ–¥—Ç—Ä–∏–º–∫–∞ preflight OPTIONS –∑–∞–ø–∏—Ç—ñ–≤)
CORS(app, resources={
    r"/*": {
        "origins": "*",
        "methods": ["GET", "POST", "OPTIONS"],
        "allow_headers": ["Content-Type", "Authorization"]
    }
})
app.config['MAX_CONTENT_LENGTH'] = 100 * 1024 * 1024  # 100 MB max file size

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç–∏ –¥–ª—è iOS Shortcuts API
MAX_FILE_SIZE = 100 * 1024 * 1024  # 100 MB
ALLOWED_EXTENSIONS = {'wav', 'mp3', 'm4a', 'flac', 'ogg', 'aac'}
PROCESSING_TIMEOUT = 300  # 5 —Ö–≤–∏–ª–∏–Ω

# –î–æ–∑–≤–æ–ª–∏ –¥–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω—å
UPLOAD_FOLDER = 'temp_uploads'
os.makedirs(UPLOAD_FOLDER, exist_ok=True)

# –ì–ª–æ–±–∞–ª—å–Ω—ñ –∑–º—ñ–Ω–Ω—ñ –¥–ª—è –º–æ–¥–µ–ª–µ–π (–∑–∞–≤–∞–Ω—Ç–∞–∂—É—é—Ç—å—Å—è –æ–¥–∏–Ω —Ä–∞–∑)
speaker_model = None
whisper_model = None
separation_model = None

def diagnose_model_structure():
    """–î—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏ –º–æ–¥–µ–ª—ñ —Ç–∞ –≤–µ—Ä—Å—ñ–π –±—ñ–±–ª—ñ–æ—Ç–µ–∫"""
    global speaker_model
    if speaker_model is None:
        print("‚ö†Ô∏è  Cannot diagnose: speaker_model is None")
        return
    print("\n" + "="*60)
    print("üîç DIAGNOSTICS: Model Structure and Versions")
    print("="*60)
    # –í–µ—Ä—Å—ñ—ó –±—ñ–±–ª—ñ–æ—Ç–µ–∫
    try:
        import torch
        import torchaudio
        import speechbrain
        print(f"üì¶ torch version: {torch.__version__}")
        print(f"üì¶ torchaudio version: {torchaudio.__version__}")
        print(f"üì¶ speechbrain version: {speechbrain.__version__}")
    except Exception as e:
        print(f"‚ö†Ô∏è  Error getting versions: {e}")
    # –¢–∏–ø —Ç–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª—ñ
    print(f"\nüìã Model type: {type(speaker_model)}")
    print(f"üìã Model class: {speaker_model.__class__.__name__}")
    # –ê—Ç—Ä–∏–±—É—Ç–∏ –º–æ–¥–µ–ª—ñ
    print(f"\nüìã Model attributes (first 20): {[attr for attr in dir(speaker_model) if not attr.startswith('_')][:20]}")
    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ mods
    if hasattr(speaker_model, 'mods'):
        print(f"‚úÖ Model has 'mods' attribute")
        print(f"üìã Mods type: {type(speaker_model.mods)}")
        print(f"üìã Mods attributes: {[attr for attr in dir(speaker_model.mods) if not attr.startswith('_')][:20]}")
        if hasattr(speaker_model.mods, 'encoder'):
            print(f"‚úÖ Model has 'mods.encoder'")
            print(f"üìã Encoder type: {type(speaker_model.mods.encoder)}")
        else:
            print(f"‚ùå Model does NOT have 'mods.encoder'")
        if hasattr(speaker_model.mods, 'embedding_model'):
            print(f"‚úÖ Model has 'mods.embedding_model'")
            print(f"üìã Embedding model type: {type(speaker_model.mods.embedding_model)}")
        else:
            print(f"‚ùå Model does NOT have 'mods.embedding_model'")
    else:
        print(f"‚ùå Model does NOT have 'mods' attribute")
    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –º–µ—Ç–æ–¥—ñ–≤
    if hasattr(speaker_model, 'encode_batch'):
        print(f"‚úÖ Model has 'encode_batch' method")
        try:
            import inspect
            sig = inspect.signature(speaker_model.encode_batch)
            print(f"üìã encode_batch signature: {sig}")
        except Exception as e:
            print(f"‚ö†Ô∏è  Could not get signature: {e}")
    else:
        print(f"‚ùå Model does NOT have 'encode_batch' method")
    if hasattr(speaker_model, 'encode_file'):
        print(f"‚úÖ Model has 'encode_file' method")
        try:
            import inspect
            sig = inspect.signature(speaker_model.encode_file)
            print(f"üìã encode_file signature: {sig}")
        except Exception as e:
            print(f"‚ö†Ô∏è  Could not get signature: {e}")
    else:
        print(f"‚ùå Model does NOT have 'encode_file' method")
    # Device –º–æ–¥–µ–ª—ñ
    try:
        if hasattr(speaker_model, 'parameters'):
            device = next(speaker_model.parameters()).device
            print(f"üì± Model device: {device}")
        else:
            print(f"‚ö†Ô∏è  Cannot determine model device")
    except Exception as e:
        print(f"‚ö†Ô∏è  Error getting device: {e}")
    print("="*60 + "\n")


def load_models():
    """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –º–æ–¥–µ–ª—ñ SpeechBrain —Ç–∞ Whisper –æ–¥–∏–Ω —Ä–∞–∑ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç—ñ"""
    global speaker_model, whisper_model, separation_model
    if speaker_model is None:
        print("üîÑ Loading SpeechBrain speaker recognition model...")
        try:
            speaker_model = SpeakerRecognition.from_hparams(
                source="speechbrain/spkrec-ecapa-voxceleb",
                savedir="pretrained_models/spkrec-ecapa-voxceleb"
            )
            print("‚úÖ SpeechBrain model loaded successfully!")
            # –î—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø—ñ—Å–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è
            diagnose_model_structure()
        except Exception as e:
            print(f"‚ùå Error loading SpeechBrain model: {e}")
            raise
    if whisper_model is None:
        print("üîÑ Loading Whisper model...")
        try:
            # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ base –º–æ–¥–µ–ª—å –¥–ª—è –±–∞–ª–∞–Ω—Å—É –º—ñ–∂ —à–≤–∏–¥–∫—ñ—Å—Ç—é —Ç–∞ —è–∫—ñ—Å—Ç—é
            whisper_model = whisper.load_model("base")
            print("‚úÖ Whisper model loaded successfully!")
        except Exception as e:
            print(f"‚ùå Error loading Whisper model: {e}")
            raise
    # Separation model –∑–∞–≤–∞–Ω—Ç–∞–∂—É—î—Ç—å—Å—è –Ω–∞ –≤–∏–º–æ–≥—É (lazy loading)

# –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –º–æ–¥–µ–ª—ñ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç—ñ (–∑ –æ–±—Ä–æ–±–∫–æ—é –ø–æ–º–∏–ª–æ–∫)
try:
    load_models()
except Exception as e:
    print(f"‚ö†Ô∏è  Warning: Could not load models at startup: {e}")
    print("   Models will be loaded on first request")
    import traceback
    traceback.print_exc()


def extract_speaker_embeddings(audio_path, segment_duration=1.5, overlap=0.5):
    """
    –í–∏—Ç—è–≥—É—î –µ–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–ø—ñ–∫–µ—Ä–∞ –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ –∞—É–¥—ñ–æ.
    Args:
        audio_path: —à–ª—è—Ö –¥–æ –∞—É–¥—ñ–æ—Ñ–∞–π–ª—É
        segment_duration: –¥–æ–≤–∂–∏–Ω–∞ —Å–µ–≥–º–µ–Ω—Ç–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        overlap: –ø–µ—Ä–µ–∫—Ä–∏—Ç—Ç—è –º—ñ–∂ —Å–µ–≥–º–µ–Ω—Ç–∞–º–∏ (0-1)
    Returns:
        embeddings: –º–∞—Ç—Ä–∏—Ü—è –µ–º–±–µ–¥–∏–Ω–≥—ñ–≤ (N, 192)
        timestamps: —Å–ø–∏—Å–æ–∫ (start, end) –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞
    """
    global speaker_model
    if speaker_model is None:
        load_models()
    try:
        # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –∞—É–¥—ñ–æ
        print(f"üìÇ Loading audio from: {audio_path}")
        audio, sr = librosa.load(audio_path, sr=16000, mono=True)
        duration = librosa.get_duration(y=audio, sr=sr)
        print(f"‚è±Ô∏è  Audio duration: {duration:.2f} seconds, sample rate: {sr} Hz, samples: {len(audio)}")
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –º—ñ–Ω—ñ–º–∞–ª—å–Ω–æ—ó –¥–æ–≤–∂–∏–Ω–∏
        min_duration = 0.5  # –ú—ñ–Ω—ñ–º—É–º 0.5 —Å–µ–∫—É–Ω–¥–∏
        if duration < min_duration:
            print(f"‚ö†Ô∏è  Audio too short ({duration:.2f}s < {min_duration}s), using entire audio as single segment")
            # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –≤—Å–µ –∞—É–¥—ñ–æ —è–∫ –æ–¥–∏–Ω —Å–µ–≥–º–µ–Ω—Ç
            embedding = None
            try:
                # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ñ–æ—Ä–º–∞—Ç [1, samples] (–ø—Ä–∞—Ü—é—î –∑ –ø–æ—Ç–æ—á–Ω–æ—é –≤–µ—Ä—Å—ñ—î—é SpeechBrain)
                segment_tensor = torch.tensor(audio, dtype=torch.float32).unsqueeze(0)  # [1, samples]
                embedding = speaker_model.encode_batch(segment_tensor, normalize=False).squeeze().cpu().detach().numpy()
            except Exception as e1:
                try:
                    # Fallback –¥–æ encode_batch –±–µ–∑ normalize
                    segment_tensor = torch.tensor(audio, dtype=torch.float32).unsqueeze(0)  # [1, samples]
                    embedding = speaker_model.encode_batch(segment_tensor).squeeze().cpu().detach().numpy()
                except Exception as e2:
                    try:
                        # Fallback –¥–æ encode_file —á–µ—Ä–µ–∑ —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª
                        import tempfile
                        with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp_file:
                            sf.write(tmp_file.name, audio, sr)
                            tmp_path = tmp_file.name
                        if hasattr(speaker_model, 'encode_file'):
                            embedding = speaker_model.encode_file(tmp_path).squeeze().cpu().detach().numpy()
                        else:
                            embedding = None
                        try:
                            os.unlink(tmp_path)
                        except:
                            pass
                    except Exception as e3:
                        print(f"‚ùå Error processing short audio: Method1={e1}, Method2={e2}, Method3={e3}")
                        return None, []
            if embedding is not None and len(embedding) > 0:
                return np.array([embedding]), [(0.0, duration)]
            else:
                return None, []
        embeddings = []
        timestamps = []
        # –ö–æ–≤–∑–Ω—ñ –≤—ñ–∫–Ω–∞
        segment_samples = int(segment_duration * sr)
        stride_samples = int(segment_duration * (1 - overlap) * sr)
        print(f"üîç Processing with segment_duration={segment_duration}s, overlap={overlap}, segment_samples={segment_samples}, stride_samples={stride_samples}")
        # –Ø–∫—â–æ –∞—É–¥—ñ–æ –∫–æ—Ä–æ—Ç—à–µ –∑–∞ –æ–¥–∏–Ω —Å–µ–≥–º–µ–Ω—Ç, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –≤—Å–µ –∞—É–¥—ñ–æ
        if len(audio) < segment_samples:
            print(f"‚ö†Ô∏è  Audio shorter than segment ({len(audio)} < {segment_samples}), using entire audio")
            embedding = None
            try:
                # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ñ–æ—Ä–º–∞—Ç [1, samples] (–ø—Ä–∞—Ü—é—î –∑ –ø–æ—Ç–æ—á–Ω–æ—é –≤–µ—Ä—Å—ñ—î—é SpeechBrain)
                segment_tensor = torch.tensor(audio, dtype=torch.float32).unsqueeze(0)  # [1, samples]
                embedding = speaker_model.encode_batch(segment_tensor, normalize=False).squeeze().cpu().detach().numpy()
            except Exception as e1:
                try:
                    # Fallback –¥–æ encode_batch –±–µ–∑ normalize
                    segment_tensor = torch.tensor(audio, dtype=torch.float32).unsqueeze(0)  # [1, samples]
                    embedding = speaker_model.encode_batch(segment_tensor).squeeze().cpu().detach().numpy()
                except Exception as e2:
                    try:
                        # Fallback –¥–æ encode_file —á–µ—Ä–µ–∑ —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª
                        import tempfile
                        with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp_file:
                            sf.write(tmp_file.name, audio, sr)
                            tmp_path = tmp_file.name
                        if hasattr(speaker_model, 'encode_file'):
                            embedding = speaker_model.encode_file(tmp_path).squeeze().cpu().detach().numpy()
                        else:
                            embedding = None
                        try:
                            os.unlink(tmp_path)
                        except:
                            pass
                    except Exception as e3:
                        print(f"‚ùå Error processing short audio segment: Method1={e1}, Method2={e2}, Method3={e3}")
                        return None, []
            if embedding is not None and len(embedding) > 0:
                return np.array([embedding]), [(0.0, duration)]
            else:
                return None, []
        # –û–±—Ä–æ–±–∫–∞ —Å–µ–≥–º–µ–Ω—Ç—ñ–≤
        max_start = len(audio) - segment_samples
        if max_start < 0:
            max_start = 0
        segments_processed = 0
        for start_sample in range(0, max_start + 1, stride_samples):
            end_sample = min(start_sample + segment_samples, len(audio))
            segment = audio[start_sample:end_sample]
            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞, —â–æ —Å–µ–≥–º–µ–Ω—Ç –Ω–µ –ø–æ—Ä–æ–∂–Ω—ñ–π
            if len(segment) == 0:
                continue
            # –í–∏—Ç—è–≥—É—î–º–æ –µ–º–±–µ–¥–¥–∏–Ω–≥ —á–µ—Ä–µ–∑ SpeechBrain
            # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –∫—ñ–ª—å–∫–∞ fallback –º–µ—Ç–æ–¥—ñ–≤ –¥–ª—è —Å—É–º—ñ—Å–Ω–æ—Å—Ç—ñ –∑ —Ä—ñ–∑–Ω–∏–º–∏ –≤–µ—Ä—Å—ñ—è–º–∏
            embedding = None
            # –û—Ç—Ä–∏–º—É—î–º–æ device –º–æ–¥–µ–ª—ñ
            try:
                model_device = next(speaker_model.parameters()).device
            except:
                model_device = torch.device('cpu')
            try:
                # –ú–µ—Ç–æ–¥ 1: encode_batch –∑ —Ñ–æ—Ä–º–∞—Ç–æ–º [1, samples] (–ø—Ä–∞—Ü—é—î –∑ –ø–æ—Ç–æ—á–Ω–æ—é –≤–µ—Ä—Å—ñ—î—é SpeechBrain)
                segment_tensor = torch.tensor(segment, dtype=torch.float32).unsqueeze(0).to(model_device)  # [1, samples]
                if start_sample == 0 or segments_processed == 0:
                    print(f"üîç Method 1: tensor shape={segment_tensor.shape}, dtype={segment_tensor.dtype}, device={segment_tensor.device}")
                embedding = speaker_model.encode_batch(segment_tensor, normalize=False)
                embedding = embedding.squeeze().cpu().detach().numpy()
                if embedding is not None and len(embedding) > 0:
                    if np.any(np.isnan(embedding)) or np.any(np.isinf(embedding)):
                        print(f"‚ö†Ô∏è  Method 1: NaN or Inf found in embedding, trying next method...")
                        embedding = None
                    else:
                        if start_sample == 0 or segments_processed == 0:
                            print(f"‚úÖ Method 1 succeeded: embedding shape={embedding.shape}, dtype={embedding.dtype}")
            except Exception as e1:
                if start_sample == 0 or segments_processed == 0:
                    print(f"‚ö†Ô∏è  Method 1 (encode_batch [1,samples] normalize=False) failed: {e1}")
                try:
                    # –ú–µ—Ç–æ–¥ 2: encode_batch –±–µ–∑ normalize
                    segment_tensor = torch.tensor(segment, dtype=torch.float32).unsqueeze(0).to(model_device)  # [1, samples]
                    if start_sample == 0 or segments_processed == 0:
                        print(f"üîç Method 2: tensor shape={segment_tensor.shape}, dtype={segment_tensor.dtype}, device={segment_tensor.device}")
                    embedding = speaker_model.encode_batch(segment_tensor)
                    embedding = embedding.squeeze().cpu().detach().numpy()
                    if embedding is not None and len(embedding) > 0:
                        if np.any(np.isnan(embedding)) or np.any(np.isinf(embedding)):
                            print(f"‚ö†Ô∏è  Method 2: NaN or Inf found in embedding, trying next method...")
                            embedding = None
                        else:
                            if start_sample == 0 or segments_processed == 0:
                                print(f"‚úÖ Method 2 succeeded: embedding shape={embedding.shape}, dtype={embedding.dtype}")
                except Exception as e2:
                    if start_sample == 0 or segments_processed == 0:
                        print(f"‚ö†Ô∏è  Method 2 (encode_batch [1,samples] default) failed: {e2}")
                    # –ú–µ—Ç–æ–¥ 3: encode_file —á–µ—Ä–µ–∑ —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª (—è–∫—â–æ –¥–æ—Å—Ç—É–ø–Ω–∏–π)
                    try:
                        import tempfile
                        with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp_file:
                            sf.write(tmp_file.name, segment, sr)
                            tmp_path = tmp_file.name
                        if hasattr(speaker_model, 'encode_file'):
                            if start_sample == 0 or segments_processed == 0:
                                print(f"üîç Method 3: Using encode_file with temporary file")
                            embedding = speaker_model.encode_file(tmp_path)
                            embedding = embedding.squeeze().cpu().detach().numpy()
                            if embedding is not None and len(embedding) > 0:
                                if np.any(np.isnan(embedding)) or np.any(np.isinf(embedding)):
                                    print(f"‚ö†Ô∏è  Method 3: NaN or Inf found in embedding")
                                    embedding = None
                                else:
                                    if start_sample == 0 or segments_processed == 0:
                                        print(f"‚úÖ Method 3 succeeded: embedding shape={embedding.shape}, dtype={embedding.dtype}")
                            # –í–∏–¥–∞–ª—è—î–º–æ —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª
                            try:
                                os.unlink(tmp_path)
                            except:
                                pass
                        else:
                            embedding = None
                    except Exception as e3:
                        if start_sample == 0 or segments_processed == 0:
                            print(f"‚ö†Ô∏è  Method 3 (encode_file) failed: {e3}")
                        try:
                            if 'tmp_path' in locals():
                                os.unlink(tmp_path)
                        except:
                            pass
                        print(f"‚ùå All methods failed for segment at {start_sample}: Method1={type(e1).__name__}:{str(e1)[:100]}, Method2={type(e2).__name__}:{str(e2)[:100]}, Method3={type(e3).__name__}:{str(e3)[:100]}")
                        import traceback
                        traceback.print_exc()
                        continue
            if embedding is not None and len(embedding) > 0:
                embeddings.append(embedding)
                start_time = start_sample / sr
                end_time = end_sample / sr
                timestamps.append((start_time, min(end_time, duration)))
                segments_processed += 1
            else:
                print(f"‚ö†Ô∏è  No embedding extracted for segment at {start_sample}")
                continue
        print(f"‚úÖ Processed {segments_processed} segments, extracted {len(embeddings)} embeddings")
        if len(embeddings) == 0:
            print("‚ùå No embeddings extracted!")
            return None, []
        return np.array(embeddings), timestamps
    except Exception as e:
        print(f"‚ùå Error in extract_speaker_embeddings: {e}")
        import traceback
        traceback.print_exc()
        return None, []


def diarize_audio(embeddings, timestamps, num_speakers=None):
    """
    –í–∏–∫–æ–Ω—É—î –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—é —á–µ—Ä–µ–∑ spectral clustering –Ω–∞ –µ–º–±–µ–¥–∏–Ω–≥–∞—Ö.
    Args:
        embeddings: –º–∞—Ç—Ä–∏—Ü—è –µ–º–±–µ–¥–∏–Ω–≥—ñ–≤ (N, 192)
        timestamps: —Å–ø–∏—Å–æ–∫ (start, end) –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞
        num_speakers: –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Å–ø—ñ–∫–µ—Ä—ñ–≤ (—è–∫—â–æ None, –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≤–∏–∑–Ω–∞—á–∞—î—Ç—å—Å—è)
    Returns:
        segments: —Å–ø–∏—Å–æ–∫ {'speaker': int, 'start': float, 'end': float}
    """
    if embeddings is None:
        print("‚ùå No embeddings provided for diarization")
        return []
    if len(embeddings) < 2:
        print(f"‚ö†Ô∏è  Only {len(embeddings)} embedding(s) available, need at least 2 for clustering")
        # –Ø–∫—â–æ —Ç—ñ–ª—å–∫–∏ –æ–¥–∏–Ω —Å–µ–≥–º–µ–Ω—Ç, –ø–æ–≤–µ—Ä—Ç–∞—î–º–æ –π–æ–≥–æ —è–∫ –æ–¥–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
        if len(embeddings) == 1 and timestamps:
            return [{
                'speaker': 0,
                'start': round(timestamps[0][0], 2),
                'end': round(timestamps[0][1], 2)
            }]
        return []
    try:
        # –ù–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ –µ–º–±–µ–¥–¥–∏–Ω–≥–∏ (L2 –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è)
        from sklearn.preprocessing import normalize
        embeddings_normalized = normalize(embeddings, norm='l2')
        # –û–±—á–∏—Å–ª—é—î–º–æ –∫–æ—Å–∏–Ω—É—Å–Ω—É –≤—ñ–¥—Å—Ç–∞–Ω—å –º—ñ–∂ –µ–º–±–µ–¥–∏–Ω–≥–∞–º–∏
        distances = pdist(embeddings_normalized, metric='cosine')
        distance_matrix = squareform(distances)
        # –î—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞: –ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ä–æ–∑–ø–æ–¥—ñ–ª –≤—ñ–¥—Å—Ç–∞–Ω–µ–π
        mean_dist = np.mean(distances)
        std_dist = np.std(distances)
        print(f"üìä Distance stats: mean={mean_dist:.4f}, std={std_dist:.4f}, min={np.min(distances):.4f}, max={np.max(distances):.4f}")
        # –°—Ç–≤–æ—Ä—é—î–º–æ similarity matrix –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—ó
        if std_dist < 1e-6:
            print(f"‚ö†Ô∏è  All distances are nearly identical, using uniform similarity")
            similarity_matrix = np.ones_like(distance_matrix) * 0.5
        else:
            # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –∞–¥–∞–ø—Ç–∏–≤–Ω–µ –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è
            scale = mean_dist if mean_dist > 0.01 else 0.1
            similarity_matrix = np.exp(-distance_matrix / scale)
        # –í–∏–∑–Ω–∞—á–∞—î–º–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Å–ø—ñ–∫–µ—Ä—ñ–≤ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ, —è–∫—â–æ –Ω–µ –∑–∞–¥–∞–Ω–æ
        if num_speakers is None:
            # –ü–æ–∫—Ä–∞—â–µ–Ω–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –∫—ñ–ª—å–∫–æ—Å—Ç—ñ —Å–ø—ñ–∫–µ—Ä—ñ–≤
            from sklearn.metrics import silhouette_score, davies_bouldin_score
            best_k = 2
            best_score = -1
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ k –≤—ñ–¥ 2 –¥–æ min(5, –∫—ñ–ª—å–∫—ñ—Å—Ç—å_—Å–µ–≥–º–µ–Ω—Ç—ñ–≤/3)
            max_k = min(5, max(2, len(embeddings) // 3))
            scores = []
            for k in range(2, max_k + 1):
                try:
                    test_clustering = SpectralClustering(
                        n_clusters=k,
                        affinity='precomputed',
                        random_state=42,
                        assign_labels='kmeans',
                        n_init=10  # –ë—ñ–ª—å—à–µ —Å–ø—Ä–æ–± –¥–ª—è —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ
                    )
                    test_labels = test_clustering.fit_predict(similarity_matrix)
                    # –û–±—á–∏—Å–ª—é—î–º–æ silhouette score (–ø–æ—Ç—Ä–µ–±—É—î –ø—Ä–∏–Ω–∞–π–º–Ω—ñ 2 –∫–ª–∞—Å—Ç–µ—Ä–∏)
                    if len(np.unique(test_labels)) > 1:
                        sil_score = silhouette_score(embeddings_normalized, test_labels, metric='cosine')
                        db_score = davies_bouldin_score(embeddings_normalized, test_labels)
                        # –ö–æ–º–±—ñ–Ω–æ–≤–∞–Ω–∏–π score (silhouette –≤–∏—â–∏–π = –∫—Ä–∞—â–µ, DB –Ω–∏–∂—á–∏–π = –∫—Ä–∞—â–µ)
                        combined_score = sil_score - (db_score / 10)  # –ù–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ DB score
                        print(f"   k={k}: silhouette={sil_score:.4f}, DB={db_score:.4f}, combined={combined_score:.4f}")
                        scores.append((k, combined_score, sil_score, db_score))
                        if combined_score > best_score:
                            best_score = combined_score
                            best_k = k
                except Exception as e:
                    print(f"   k={k}: error - {e}")
                    continue
            # –î–æ–¥–∞—Ç–∫–æ–≤–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞: —è–∫—â–æ —Ä—ñ–∑–Ω–∏—Ü—è –º—ñ–∂ –Ω–∞–π–∫—Ä–∞—â–∏–º —Ç–∞ –¥—Ä—É–≥–∏–º –Ω–µ–≤–µ–ª–∏–∫–∞, –≤–∏–±–∏—Ä–∞—î–º–æ –º–µ–Ω—à–µ –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤
            if len(scores) > 1:
                scores.sort(key=lambda x: x[1], reverse=True)
                best_score_val = scores[0][1]
                second_score_val = scores[1][1] if len(scores) > 1 else -1
                # –Ø–∫—â–æ —Ä—ñ–∑–Ω–∏—Ü—è –º–µ–Ω—à–µ 0.1, –≤–∏–±–∏—Ä–∞—î–º–æ –º–µ–Ω—à–µ –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤ (–ø—Ä–æ—Å—Ç—ñ—à–µ = –∫—Ä–∞—â–µ)
                if best_score_val - second_score_val < 0.1:
                    best_k = min(scores[0][0], scores[1][0] if len(scores) > 1 else scores[0][0])
                    print(f"   ‚öñÔ∏è  Scores too close, choosing fewer clusters: k={best_k}")
            num_speakers = best_k
            print(f"üîç Auto-detected {num_speakers} speakers (best combined_score={best_score:.4f})")
            # –Ø–∫—â–æ –≤—Å—ñ –¥—É–∂–µ —Å—Ö–æ–∂—ñ, –ø—Ä–∏–º—É—Å–æ–≤–æ –≤—Å—Ç–∞–Ω–æ–≤–ª—é—î–º–æ –º—ñ–Ω—ñ–º—É–º 2
            if mean_dist < 0.05:
                num_speakers = 2
                print(f"‚ö†Ô∏è  Very low distance ({mean_dist:.4f}), forcing 2 speakers")
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ –¥–æ—Å—Ç–∞—Ç–Ω—å–æ —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—ó
        if len(embeddings) < num_speakers:
            print(f"‚ö†Ô∏è  Not enough segments ({len(embeddings)}) for {num_speakers} speakers, using {len(embeddings)}")
            num_speakers = len(embeddings)
        # –°–ø—Ä–æ–±—É—î–º–æ —Ä—ñ–∑–Ω—ñ –∞–ª–≥–æ—Ä–∏—Ç–º–∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—ó –∑ –ø–æ–∫—Ä–∞—â–µ–Ω–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        labels = None
        # –ú–µ—Ç–æ–¥ 1: Spectral clustering –∑ –ø–æ–∫—Ä–∞—â–µ–Ω–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        try:
            clustering = SpectralClustering(
                n_clusters=num_speakers,
                affinity='precomputed',
                random_state=42,
                assign_labels='kmeans',
                n_init=20,  # –ë—ñ–ª—å—à–µ —Å–ø—Ä–æ–± –¥–ª—è –∫—Ä–∞—â–æ—ó —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ
                n_jobs=-1  # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –≤—Å—ñ —è–¥—Ä–∞
            )
            labels = clustering.fit_predict(similarity_matrix)
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —è–∫—ñ—Å—Ç—å –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—ó
            unique_labels = np.unique(labels)
            if len(unique_labels) < num_speakers:
                print(f"‚ö†Ô∏è  Spectral clustering produced only {len(unique_labels)} clusters, expected {num_speakers}")
                # –Ø–∫—â–æ –æ—Ç—Ä–∏–º–∞–ª–∏ –º–µ–Ω—à–µ –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤, –ø—Ä–æ–±—É—î–º–æ —ñ–Ω—à–∏–π –º–µ—Ç–æ–¥
                raise ValueError(f"Only {len(unique_labels)} clusters found")
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤
            label_counts = {label: np.sum(labels == label) for label in unique_labels}
            min_count = min(label_counts.values())
            max_count = max(label_counts.values())
            # –Ø–∫—â–æ –æ–¥–∏–Ω –∫–ª–∞—Å—Ç–µ—Ä –∑–∞–Ω–∞–¥—Ç–æ –º–∞–ª–∏–π (< 5% —Å–µ–≥–º–µ–Ω—Ç—ñ–≤), —Ü–µ –ø—ñ–¥–æ–∑—Ä—ñ–ª–æ
            if min_count < len(embeddings) * 0.05:
                print(f"‚ö†Ô∏è  Unbalanced clusters detected (min={min_count}, max={max_count}), trying alternative method")
                # –ù–µ –≤–∏–∫–∏–¥–∞—î–º–æ –ø–æ–º–∏–ª–∫—É, –∞–ª–µ –ª–æ–≥—É—î–º–æ –ø–æ–ø–µ—Ä–µ–¥–∂–µ–Ω–Ω—è
            print(f"‚úÖ Used SpectralClustering: {len(unique_labels)} clusters, balance: {min_count}-{max_count} segments per cluster")
        except Exception as e:
            print(f"‚ö†Ô∏è  Spectral clustering failed: {e}, trying AgglomerativeClustering")
            # Fallback –¥–æ AgglomerativeClustering –∑ –ø–æ–∫—Ä–∞—â–µ–Ω–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            try:
                from sklearn.cluster import AgglomerativeClustering
                # –ü—Ä–æ–±—É—î–º–æ —Ä—ñ–∑–Ω—ñ —Ç–∏–ø–∏ –∑–≤'—è–∑–∫—É
                for linkage_type in ['ward', 'average', 'complete']:
                    try:
                        clustering = AgglomerativeClustering(
                            n_clusters=num_speakers,
                            linkage=linkage_type,
                            affinity='precomputed' if linkage_type != 'ward' else 'euclidean'
                        )
                        if linkage_type == 'ward':
                            # –î–ª—è ward –ø–æ—Ç—Ä—ñ–±–Ω–∞ –µ–≤–∫–ª—ñ–¥–æ–≤–∞ –≤—ñ–¥—Å—Ç–∞–Ω—å, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω—ñ –µ–º–±–µ–¥–¥–∏–Ω–≥–∏
                            labels = clustering.fit_predict(embeddings_normalized)
                        else:
                            labels = clustering.fit_predict(similarity_matrix)
                        unique_labels = np.unique(labels)
                        if len(unique_labels) == num_speakers:
                            print(f"‚úÖ Used AgglomerativeClustering (linkage={linkage_type})")
                            break
                    except Exception as e2:
                        print(f"   linkage={linkage_type} failed: {e2}")
                        continue
                if labels is None:
                    raise Exception("All AgglomerativeClustering methods failed")
            except Exception as e2:
                print(f"‚ùå AgglomerativeClustering also failed: {e2}")
                # –û—Å—Ç–∞–Ω–Ω—ñ–π fallback: –ø—Ä–æ—Å—Ç–∏–π k-means –Ω–∞ –Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω–∏—Ö –µ–º–±–µ–¥–¥–∏–Ω–≥–∞—Ö
                from sklearn.cluster import KMeans
                kmeans = KMeans(n_clusters=num_speakers, random_state=42, n_init=20)
                labels = kmeans.fit_predict(embeddings_normalized)
                print(f"‚úÖ Used KMeans as fallback")
        if labels is None:
            print("‚ùå Clustering failed completely")
            return []
        # –î—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞: –ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ä–æ–∑–ø–æ–¥—ñ–ª –ª–µ–π–±–ª—ñ–≤
        unique_labels, counts = np.unique(labels, return_counts=True)
        print(f"üìä Clustering result: {len(unique_labels)} unique speakers found")
        for label, count in zip(unique_labels, counts):
            print(f"   Speaker {label}: {count} segments ({count/len(labels)*100:.1f}%)")
        # –Ø–∫—â–æ –≤—Å—ñ —Å–µ–≥–º–µ–Ω—Ç–∏ –æ–¥–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞, —Å–ø—Ä–æ–±—É—î–º–æ —ñ–Ω—à–∏–π –ø—ñ–¥—Ö—ñ–¥
        if len(unique_labels) == 1 and num_speakers > 1:
            print(f"‚ö†Ô∏è  All segments assigned to one speaker, trying alternative clustering...")
            # –°–ø—Ä–æ–±—É—î–º–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ –≤—ñ–¥—Å—Ç–∞–Ω—ñ –±–µ–∑–ø–æ—Å–µ—Ä–µ–¥–Ω—å–æ
            from sklearn.cluster import AgglomerativeClustering
            clustering_alt = AgglomerativeClustering(
                n_clusters=num_speakers,
                linkage='ward',
                metric='euclidean'
            )
            labels_alt = clustering_alt.fit_predict(embeddings_normalized)
            unique_alt, counts_alt = np.unique(labels_alt, return_counts=True)
            if len(unique_alt) > 1:
                print(f"‚úÖ Alternative clustering found {len(unique_alt)} speakers")
                labels = labels_alt
                unique_labels, counts = unique_alt, counts_alt
        # –ü–æ–∫—Ä–∞—â–µ–Ω–∞ –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è –ª–µ–π–±–ª—ñ–≤: –∑–∞–≤–∂–¥–∏ –ø–æ—á–∏–Ω–∞—î–º–æ –∑ 0
        # –ü–µ—Ä–µ—ñ–º–µ–Ω–æ–≤—É—î–º–æ –ª–µ–π–±–ª–∏ —Ç–∞–∫, —â–æ–± –Ω–∞–π–±—ñ–ª—å—à —Ä–∞–Ω–Ω—ñ–π —Å–µ–≥–º–µ–Ω—Ç –º–∞–≤ –ª–µ–π–±–ª 0
        if len(unique_labels) > 0:
            # –ó–Ω–∞—Ö–æ–¥–∏–º–æ, —è–∫–∏–π –ª–µ–π–±–ª –º–∞—î –Ω–∞–π—Ä–∞–Ω–Ω—ñ—à–∏–π —Å–µ–≥–º–µ–Ω—Ç
            first_segment_label = labels[0]
            # –Ø–∫—â–æ –ø–µ—Ä—à–∏–π —Å–µ–≥–º–µ–Ω—Ç –Ω–µ –º–∞—î –ª–µ–π–±–ª 0, –º—ñ–Ω—è—î–º–æ –º—ñ—Å—Ü—è–º–∏
            if first_segment_label != 0:
                print(f"‚ö†Ô∏è  First segment has label {first_segment_label}, normalizing to start with label 0...")
                # –°—Ç–≤–æ—Ä—é—î–º–æ –º–∞–ø—É: first_segment_label ‚Üí 0, 0 ‚Üí first_segment_label
                label_map = {}
                for old_label in unique_labels:
                    if old_label == first_segment_label:
                        label_map[old_label] = 0
                    elif old_label == 0:
                        label_map[old_label] = first_segment_label
                    else:
                        label_map[old_label] = old_label
                # –ó–∞—Å—Ç–æ—Å–æ–≤—É—î–º–æ –º–∞–ø—É
                labels = np.array([label_map[label] for label in labels])
                unique_labels = np.array([label_map[label] for label in unique_labels])
                print(f"‚úÖ Normalized labels: first segment now has label 0")
        # –ó–ª–∏–≤–∞—î–º–æ —Å—É—Å—ñ–¥–Ω—ñ —Å–µ–≥–º–µ–Ω—Ç–∏ –æ–¥–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
        segments = []
        current_speaker = None
        current_start = None
        for i, (label, (start, end)) in enumerate(zip(labels, timestamps)):
            if label != current_speaker:
                if current_speaker is not None:
                    segments.append({
                        'speaker': int(current_speaker),
                        'start': round(current_start, 2),
                        'end': round(timestamps[i-1][1], 2)
                    })
                current_speaker = label
                current_start = start
        # –î–æ–¥–∞—î–º–æ –æ—Å—Ç–∞–Ω–Ω—ñ–π —Å–µ–≥–º–µ–Ω—Ç
        if current_speaker is not None:
            segments.append({
                'speaker': int(current_speaker),
                'start': round(current_start, 2),
                'end': round(timestamps[-1][1], 2)
            })
        print(f"‚úÖ Created {len(segments)} diarization segments")
        return segments
    except Exception as e:
        print(f"‚ùå Error in diarize_audio: {e}")
        import traceback
        traceback.print_exc()
        return []


def transcribe_audio(audio_path, language=None):
    """
    –¢—Ä–∞–Ω—Å–∫—Ä–∏–±—É—î –∞—É–¥—ñ–æ –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é Whisper –∑ word timestamps.
    Args:
        audio_path: —à–ª—è—Ö –¥–æ –∞—É–¥—ñ–æ—Ñ–∞–π–ª—É
        language: –∫–æ–¥ –º–æ–≤–∏ (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, 'uk', 'en', 'ar') –∞–±–æ None –¥–ª—è –∞–≤—Ç–æ-–≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è
    Returns:
        transcription: —Ç–µ–∫—Å—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó
        segments: —Å–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ –∑ —Ç–µ–∫—Å—Ç–æ–º —Ç–∞ —á–∞—Å–æ–≤–∏–º–∏ –º—ñ—Ç–∫–∞–º–∏
        words: —Å–ø–∏—Å–æ–∫ —Å–ª—ñ–≤ –∑ timestamps –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –º–∞—Ç—á–∏–Ω–≥—É
    """
    global whisper_model
    if whisper_model is None:
        load_models()
    try:
        print(f"üé§ Transcribing audio: {audio_path}")
        # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó
        transcribe_options = {
            'word_timestamps': True,
            'verbose': False,
            'task': 'transcribe'  # –ó–∞–≤–∂–¥–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±—É—î–º–æ, –Ω–µ –ø–µ—Ä–µ–∫–ª–∞–¥–∞—î–º–æ
        }
        if language:
            transcribe_options['language'] = language
            print(f"üåê Using specified language: {language}")
        else:
            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –º–æ–≤–∏ - Whisper –∑—Ä–æ–±–∏—Ç—å —Ü–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ
            print(f"üåê Auto-detecting language (Whisper will detect automatically)")
        # –¢—Ä–∞–Ω—Å–∫—Ä–∏–±—É—î–º–æ –∑ –¥–µ—Ç–∞–ª—å–Ω–∏–º–∏ —Å–µ–≥–º–µ–Ω—Ç–∞–º–∏ —Ç–∞ word timestamps
        result = whisper_model.transcribe(
            audio_path,
            **transcribe_options
        )
        detected_lang = result.get('language', 'unknown')
        print(f"üåê Detected language: {detected_lang}")
        transcription = result["text"]
        segments = []
        words = []
        # –§–æ—Ä–º—É—î–º–æ —Å–µ–≥–º–µ–Ω—Ç–∏ –∑ —Ç–µ–∫—Å—Ç–æ–º —Ç–∞ –∑–±–∏—Ä–∞—î–º–æ –≤—Å—ñ —Å–ª–æ–≤–∞
        for seg in result["segments"]:
            segments.append({
                'start': round(seg['start'], 2),
                'end': round(seg['end'], 2),
                'text': seg['text'].strip()
            })
            # –ó–±–∏—Ä–∞—î–º–æ —Å–ª–æ–≤–∞ –∑ timestamps
            if 'words' in seg:
                for word_info in seg['words']:
                    words.append({
                        'word': word_info.get('word', '').strip(),
                        'start': round(word_info.get('start', 0), 2),
                        'end': round(word_info.get('end', 0), 2)
                    })
        print(f"‚úÖ Transcribed {len(segments)} segments, language: {detected_lang}")
        return transcription, segments, words
    except Exception as e:
        print(f"‚ùå Error in transcribe_audio: {e}")
        import traceback
        traceback.print_exc()
        return "", [], []


def separate_speakers(audio_path, output_dir=None):
    """
    –†–æ–∑–¥—ñ–ª—è—î –∞—É–¥—ñ–æ –Ω–∞ –æ–∫—Ä–µ–º—ñ —Ç—Ä–µ–∫–∏ –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é SpeechBrain.
    Args:
        audio_path: —à–ª—è—Ö –¥–æ –∞—É–¥—ñ–æ—Ñ–∞–π–ª—É
        output_dir: –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–æ–∑–¥—ñ–ª–µ–Ω–∏—Ö —Ç—Ä–µ–∫—ñ–≤ (—è–∫—â–æ None, —Å—Ç–≤–æ—Ä—é—î—Ç—å—Å—è —Ç–∏–º—á–∞—Å–æ–≤–∞)
    Returns:
        dict –∑ –∫–ª—é—á–∞–º–∏:
            - success: bool
            - speakers: —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–Ω–∏–∫—ñ–≤ –∑ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—î—é –ø—Ä–æ –∫–æ–∂–µ–Ω —Ç—Ä–µ–∫
            - output_dir: —à–ª—è—Ö –¥–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó –∑ —Ç—Ä–µ–∫–∞–º–∏
            - num_speakers: –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Å–ø—ñ–∫–µ—Ä—ñ–≤
    """
    global separation_model
    if separation_model is None:
        print("üîÑ Loading SpeechBrain separation model...")
        try:
            from speechbrain.pretrained import SepformerSeparation as Separator
            device = 'cpu'
            if torch.cuda.is_available():
                device = 'cuda'
            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                device = 'mps'
            separation_model = Separator.from_hparams(
                source="speechbrain/sepformer-wsj02mix",
                savedir="pretrained_models/sepformer-wsj02mix",
                run_opts={"device": device}
            )
            print(f"‚úÖ SpeechBrain separation model loaded successfully on {device}!")
        except Exception as e:
            print(f"‚ùå Error loading SpeechBrain separation model: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    try:
        # –°—Ç–≤–æ—Ä—é—î–º–æ output –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—é, —è–∫—â–æ –Ω–µ –≤–∫–∞–∑–∞–Ω–∞
        if output_dir is None:
            output_dir = tempfile.mkdtemp(prefix="speechbrain_separation_")
        os.makedirs(output_dir, exist_ok=True)
        # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –∞—É–¥—ñ–æ (librosa –∑–∞–≤–∂–¥–∏ –ø–æ–≤–µ—Ä—Ç–∞—î mono —è–∫—â–æ –Ω–µ –≤–∫–∞–∑–∞–Ω–æ —ñ–Ω–∞–∫—à–µ)
        waveform, sample_rate = librosa.load(audio_path, sr=None, mono=True)
        # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ –≤ torch tensor [1, samples]
        waveform_tensor = torch.tensor(waveform, dtype=torch.float32).unsqueeze(0)
        # –†–µ—Å–µ–º–ø–ª—ñ–Ω–≥ –¥–æ 8kHz (–≤–∏–º–æ–≥–∞ –º–æ–¥–µ–ª—ñ)
        if sample_rate != 8000:
            print(f"üîÑ Resampling from {sample_rate}Hz to 8000Hz...")
            import torchaudio
            resampler = torchaudio.transforms.Resample(sample_rate, 8000)
            waveform_tensor = resampler(waveform_tensor)
            sample_rate = 8000
        # –†–æ–∑–¥—ñ–ª—è—î–º–æ —Å–ø—ñ–∫–µ—Ä—ñ–≤
        print("üîÄ Separating speakers...")
        device = next(separation_model.parameters()).device
        waveform_tensor = waveform_tensor.to(device)
        with torch.no_grad():
            est_sources = separation_model.separate_batch(waveform_tensor)
        # –û–±—Ä–æ–±–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É
        if est_sources.dim() == 3:
            est_sources = est_sources[0]  # [batch, num_speakers, time] -> [num_speakers, time]
        if est_sources.dim() == 2:
            if est_sources.shape[0] == separation_model.hparams.num_spks:
                # [num_speakers, time]
                sources_tensor = est_sources
            elif est_sources.shape[1] == separation_model.hparams.num_spks:
                # [time, num_speakers] -> —Ç—Ä–∞–Ω—Å–ø–æ–Ω—É—î–º–æ
                sources_tensor = est_sources.transpose(0, 1)
            else:
                raise ValueError(f"Unexpected est_sources shape: {est_sources.shape}")
        else:
            raise ValueError(f"Unsupported est_sources dimension: {est_sources.dim()}")
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –∫–æ–∂–µ–Ω —Ç—Ä–µ–∫
        speakers = []
        for idx, source in enumerate(sources_tensor):
            speaker_name = f"SPEAKER_{idx:02d}"
            output_path = os.path.join(output_dir, f"{speaker_name}.wav")
            # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ –Ω–∞–∑–∞–¥ –¥–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–æ—ó sample rate, —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ
            source_np = source.cpu().squeeze().numpy()
            # –†–µ—Å–µ–º–ø–ª—ñ–Ω–≥ –Ω–∞–∑–∞–¥ –¥–æ 16kHz –¥–ª—è –ø–æ–¥–∞–ª—å—à–æ—ó –æ–±—Ä–æ–±–∫–∏
            if sample_rate != 16000:
                import torchaudio
                source_tensor = torch.tensor(source_np, dtype=torch.float32).unsqueeze(0)
                resampler = torchaudio.transforms.Resample(sample_rate, 16000)
                source_tensor = resampler(source_tensor)
                source_np = source_tensor.squeeze().numpy()
                final_sr = 16000
            else:
                final_sr = sample_rate
            sf.write(output_path, source_np, final_sr)
            speakers.append({
                "name": speaker_name,
                "path": output_path,
                "index": idx
            })
        print(f"‚úÖ Separated into {len(speakers)} speaker tracks")
        return {
            "success": True,
            "speakers": speakers,
            "output_dir": output_dir,
            "num_speakers": len(speakers)
        }
    except Exception as e:
        print(f"‚ùå Error in separate_speakers: {e}")
        import traceback
        traceback.print_exc()
        return {
            "success": False,
            "error": str(e)
        }


# –ì–ª–æ–±–∞–ª—å–Ω–∞ –∑–º—ñ–Ω–Ω–∞ –¥–ª—è –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –ø—Ä–æ LLM —ñ—Ç–µ—Ä–∞—Ü—ñ—ó
_llm_iterations_cache = []


def detect_and_fix_speaker_mismatch_after_complete_statement(segments, max_gap=2.0):
    """
    –í–∏—è–≤–ª—è—î —Ç–∞ –≤–∏–ø—Ä–∞–≤–ª—è—î –ø–æ–º–∏–ª–∫–∏ –ø—Ä–∏–∑–Ω–∞—á–µ–Ω–Ω—è —Å–ø—ñ–∫–µ—Ä—ñ–≤, –∫–æ–ª–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –¥—É–º–∫–∞ (statement)
    –∑ –æ–¥–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ –∑–∞–∫—ñ–Ω—á—É—î—Ç—å—Å—è, –∞ –Ω–∞—Å—Ç—É–ø–Ω–µ –ø–∏—Ç–∞–Ω–Ω—è —Ç–∞–∫–æ–∂ –ø—Ä–∏–ø–∏—Å—É—î—Ç—å—Å—è —Ç–æ–º—É –∂ —Å–ø—ñ–∫–µ—Ä—É.
    
    –®–∞–±–ª–æ–Ω –≤–∏—è–≤–ª–µ–Ω–Ω—è:
    - –ü–µ—Ä—à–∞ —Ä–µ–ø–ª—ñ–∫–∞ –∑–∞–∫—ñ–Ω—á—É—î—Ç—å—Å—è –Ω–∞ –∑–Ω–∞–∫ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è —Ä–µ—á–µ–Ω–Ω—è (., !)
    - –î—Ä—É–≥–∞ —Ä–µ–ø–ª—ñ–∫–∞ –∑–∞–∫—ñ–Ω—á—É—î—Ç—å—Å—è –Ω–∞ –∑–Ω–∞–∫ –ø–∏—Ç–∞–Ω–Ω—è (?)
    - –û–±–∏–¥–≤—ñ —Ä–µ–ø–ª—ñ–∫–∏ –≤—ñ–¥ –æ–¥–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ (–ø–æ–º–∏–ª–∫–∞ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó)
    - –ú–∞–ª–∞ –ø–∞—É–∑–∞ –º—ñ–∂ –Ω–∏–º–∏ (< max_gap)
    
    –¢–∞–∫—ñ –≤–∏–ø–∞–¥–∫–∏ –ø–æ–∑–Ω–∞—á–∞—é—Ç—å—Å—è –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ —Å–ø—ñ–∫–µ—Ä–∞ —á–µ—Ä–µ–∑ LLM.
    
    Args:
        segments: —Å–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ [{'speaker': int, 'start': float, 'end': float, 'text': str}]
        max_gap: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∏–π –ø—Ä–æ–º—ñ–∂–æ–∫ –º—ñ–∂ —Å–µ–≥–º–µ–Ω—Ç–∞–º–∏ –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ (—Å–µ–∫—É–Ω–¥–∏)
    
    Returns:
        fixed_segments: —Å–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ –∑ –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–∏–º–∏ –∞–±–æ –ø–æ–∑–Ω–∞—á–µ–Ω–∏–º–∏ –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ —Å–µ–≥–º–µ–Ω—Ç–∞–º–∏
    """
    if not segments or len(segments) < 2:
        return segments
    
    print(f"üîç Detecting speaker mismatch after complete statements in {len(segments)} segments...")
    fixed = []
    i = 0
    mismatch_count = 0
    
    while i < len(segments):
        if i == len(segments) - 1:
            # –û—Å—Ç–∞–Ω–Ω—ñ–π —Å–µ–≥–º–µ–Ω—Ç - –ø—Ä–æ—Å—Ç–æ –¥–æ–¥–∞—î–º–æ
            fixed.append(segments[i])
            i += 1
            continue
        
        current = segments[i]
        next_seg = segments[i + 1]
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —É–º–æ–≤–∏ –¥–ª—è –ø–æ–º–∏–ª–∫–∏ –ø—Ä–∏–∑–Ω–∞—á–µ–Ω–Ω—è —Å–ø—ñ–∫–µ—Ä–∞
        current_text = current['text'].strip()
        next_text = next_seg['text'].strip()
        
        # –£–º–æ–≤–∞ 1: –ü–µ—Ä—à–∞ –∑–∞–∫—ñ–Ω—á—É—î—Ç—å—Å—è –Ω–∞ –∑–Ω–∞–∫ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è —Ä–µ—á–µ–Ω–Ω—è (statement)
        ends_with_statement = current_text.endswith(('.', '!'))
        
        # –£–º–æ–≤–∞ 2: –î—Ä—É–≥–∞ –∑–∞–∫—ñ–Ω—á—É—î—Ç—å—Å—è –Ω–∞ –∑–Ω–∞–∫ –ø–∏—Ç–∞–Ω–Ω—è
        ends_with_question = next_text.endswith('?')
        
        # –£–º–æ–≤–∞ 3: –û–¥–∏–Ω —ñ —Ç–æ–π –∂–µ —Å–ø—ñ–∫–µ—Ä (–ø–æ–º–∏–ª–∫–∞ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó)
        same_speaker = current['speaker'] == next_seg['speaker']
        
        # –£–º–æ–≤–∞ 4: –ú–∞–ª–∞ –ø–∞—É–∑–∞ –º—ñ–∂ —Å–µ–≥–º–µ–Ω—Ç–∞–º–∏
        pause = next_seg['start'] - current['end']
        short_pause = pause >= 0 and pause < max_gap
        
        # –î–æ–¥–∞—Ç–∫–æ–≤–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞: –ø–∏—Ç–∞–Ω–Ω—è –≤–∏–≥–ª—è–¥–∞—î —è–∫ –æ–∫—Ä–µ–º–µ –ø–∏—Ç–∞–Ω–Ω—è (–ø–æ—á–∏–Ω–∞—î—Ç—å—Å—è –∑ –≤–µ–ª–∏–∫–æ—ó –ª—ñ—Ç–µ—Ä–∏ –∞–±–æ –ø–∏—Ç–∞–ª—å–Ω–æ–≥–æ —Å–ª–æ–≤–∞)
        looks_like_separate_question = (
            len(next_text) > 0 and (
                next_text[0].isupper() or
                next_text.lower().startswith(('did you', 'can you', 'will you', 'have you', 'are you', 
                                             'is it', 'do you', 'would you', 'could you', 'should you',
                                             'what', 'when', 'where', 'who', 'which', 'how', 'why'))
            )
        )
        
        if (ends_with_statement and 
            ends_with_question and 
            same_speaker and 
            short_pause and
            looks_like_separate_question):
            
            # –¶–µ –ø–æ–º–∏–ª–∫–∞ –ø—Ä–∏–∑–Ω–∞—á–µ–Ω–Ω—è —Å–ø—ñ–∫–µ—Ä–∞ - –ø–∏—Ç–∞–Ω–Ω—è –º–∞—î –±—É—Ç–∏ –≤—ñ–¥ —ñ–Ω—à–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
            mismatch_count += 1
            print(f"  ‚ö†Ô∏è  Detected speaker mismatch after complete statement:")
            print(f"     Statement: Speaker {current['speaker']+1} [{current['start']:.2f}s-{current['end']:.2f}s]: \"{current_text}\"")
            print(f"     Question: Speaker {next_seg['speaker']+1} [{next_seg['start']:.2f}s-{next_seg['end']:.2f}s]: \"{next_text}\"")
            
            # –ü–æ–∑–Ω–∞—á–∞—î–º–æ –Ω–∞—Å—Ç—É–ø–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç (–ø–∏—Ç–∞–Ω–Ω—è) –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ —Å–ø—ñ–∫–µ—Ä–∞
            # –ü–∏—Ç–∞–Ω–Ω—è –º–∞—î –±—É—Ç–∏ –≤—ñ–¥ —ñ–Ω—à–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ (1 - current['speaker'])
            other_speaker = 1 - current['speaker']  # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∏–π —Å–ø—ñ–∫–µ—Ä (0 ‚Üî 1)
            
            # –°—Ç–≤–æ—Ä—é—î–º–æ –∫–æ–ø—ñ—é –Ω–∞—Å—Ç—É–ø–Ω–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞ –∑ –ø–æ–∑–Ω–∞—á–∫–æ—é –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏
            next_seg_fixed = next_seg.copy()
            next_seg_fixed['speaker'] = other_speaker  # –¢–∏–º—á–∞—Å–æ–≤–æ –ø—Ä–∏–∑–Ω–∞—á–∞—î–º–æ —ñ–Ω—à–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
            next_seg_fixed['needs_role_verification'] = True  # –ü–æ–∑–Ω–∞—á–∞—î–º–æ –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ —Ä–æ–ª—ñ
            next_seg_fixed['reassignment_reason'] = 'question_after_complete_statement'
            next_seg_fixed['original_speaker'] = next_seg['speaker']  # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
            
            fixed.append(current)
            fixed.append(next_seg_fixed)
            print(f"     ‚úÖ Marked question for verification: reassigned to Speaker {other_speaker+1} (was {next_seg['speaker']+1})")
            i += 2  # –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –æ–±–∏–¥–≤–∞ —Å–µ–≥–º–µ–Ω—Ç–∏
            continue
        
        # –Ø–∫—â–æ –Ω–µ –≤–∏—è–≤–ª–µ–Ω–æ –ø–æ–º–∏–ª–∫—É - –ø—Ä–æ—Å—Ç–æ –¥–æ–¥–∞—î–º–æ –ø–æ—Ç–æ—á–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç
        fixed.append(current)
        i += 1
    
    if mismatch_count > 0:
        print(f"‚úÖ Detected and marked {mismatch_count} speaker mismatch(es) after complete statements")
    else:
        print(f"‚úÖ No speaker mismatches after complete statements detected")
    
    return fixed


def fix_answer_after_question_speaker_assignment_v2(segments, max_gap=3.0):
    """
    –ü–æ–∫—Ä–∞—â–µ–Ω–∞ –≤–µ—Ä—Å—ñ—è, —è–∫–∞ –æ–±—Ä–æ–±–ª—è—î –≤–∏–ø–∞–¥–æ–∫:
    "Hey, did you try to reset your modem? Yes," (–°–ø—ñ–∫–µ—Ä 2)
    "I tried, but the problem is still existing..." (–°–ø—ñ–∫–µ—Ä 1)
    
    –ü—Ä–∞–≤–∏–ª–æ: "Yes," –º–∞—î –Ω–∞–ª–µ–∂–∞—Ç–∏ –°–ø—ñ–∫–µ—Ä—É 1 (—Ç–æ–º—É, —Ö—Ç–æ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î).
    """
    if not segments or len(segments) < 2:
        return segments
    
    print(f"üîç Fixing answer-after-question speaker assignments (v2) in {len(segments)} segments...")
    fixed = []
    i = 0
    fixed_count = 0
    
    while i < len(segments):
        if i == len(segments) - 1:
            fixed.append(segments[i])
            i += 1
            continue
        
        current = segments[i]
        next_seg = segments[i + 1]
        
        current_text = (current.get('text') or '').strip()
        next_text = (next_seg.get('text') or '').strip()
        
        if not current_text or not next_text:
            fixed.append(current)
            i += 1
            continue
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ –ø–æ—Ç–æ—á–Ω–∞ —Ä–µ–ø–ª—ñ–∫–∞ –º—ñ—Å—Ç–∏—Ç—å –ø–∏—Ç–∞–Ω–Ω—è + –∫–æ—Ä–æ—Ç–∫—É –≤—ñ–¥–ø–æ–≤—ñ–¥—å
        # –ù–∞–ø—Ä–∏–∫–ª–∞–¥: "Hey, did you try to reset your modem? Yes,"
        if '?' in current_text:
            # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –æ—Å—Ç–∞–Ω–Ω—ñ–π –∑–Ω–∞–∫ –ø–∏—Ç–∞–Ω–Ω—è (–Ω–∞ –≤–∏–ø–∞–¥–æ–∫, —è–∫—â–æ —ó—Ö –∫—ñ–ª—å–∫–∞)
            last_question_mark = current_text.rfind('?')
            if last_question_mark >= 0:
                question_part = current_text[:last_question_mark + 1].strip()
                answer_part = current_text[last_question_mark + 1:].strip()
                
                # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ –ø—ñ—Å–ª—è –ø–∏—Ç–∞–Ω–Ω—è —î –∫–æ—Ä–æ—Ç–∫–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å
                short_answers = ['yes', 'no', 'sure', 'okay', 'ok', 'alright', 'yeah', 'yep', 'of course', 'certainly']
                answer_lower = answer_part.lower().rstrip(',.!?;:').strip()
                
                # –î—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞
                if answer_lower:
                    print(f"  üîç Checking segment {i+1}: \"{current_text[:60]}...\" (Speaker {current.get('speaker')+1})")
                    print(f"     Question part: \"{question_part}\"")
                    print(f"     Answer part: \"{answer_part}\" (lower: \"{answer_lower}\")")
                
                is_short_answer = any(answer_lower.startswith(short) for short in short_answers)
                
                if is_short_answer:
                    print(f"     ‚úÖ Detected short answer: \"{answer_lower}\"")
                    
                    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ –Ω–∞—Å—Ç—É–ø–Ω–∞ —Ä–µ–ø–ª—ñ–∫–∞ —î –ø—Ä–æ–¥–æ–≤–∂–µ–Ω–Ω—è–º –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ
                    answer_continuations = [
                        'i tried', 'i did', 'i have', 'i can', 'i will', 
                        'i think', 'i believe', 'i guess', 'i know', 'i see',
                        'i understand', 'i need', 'i want', 'i\'m', 'i am',
                        'i was', 'i would', 'i could', 'i should', 'i might'
                    ]
                    next_lower = next_text.lower().strip()
                    next_is_continuation = any(next_lower.startswith(cont) for cont in answer_continuations)
                    
                    print(f"     Next segment: \"{next_text[:60]}...\" (Speaker {next_seg.get('speaker')+1})")
                    print(f"     Is continuation: {next_is_continuation}")
                    
                    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –ø–∞—É–∑—É
                    pause = next_seg.get('start', 0) - current.get('end', 0)
                    short_pause = 0 <= pause <= max_gap
                    print(f"     Pause: {pause:.2f}s (max: {max_gap}s, short: {short_pause})")
                    
                    # –Ø–∫—â–æ –Ω–∞—Å—Ç—É–ø–Ω–∞ —Ä–µ–ø–ª—ñ–∫–∞ —î –ø—Ä–æ–¥–æ–≤–∂–µ–Ω–Ω—è–º –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ
                    if next_is_continuation and short_pause:
                        # –ó–Ω–∞—Ö–æ–¥–∏–º–æ —ñ–Ω—à–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ (—Ç–æ–≥–æ, —Ö—Ç–æ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î)
                        current_speaker = current.get('speaker')
                        answer_speaker = next_seg.get('speaker')
                        
                        fixed_count += 1
                        print(f"  üîß Moving answer fragment to answer speaker:")
                        print(f"     Question: Speaker {current_speaker+1} [{current.get('start'):.2f}s-{current.get('end'):.2f}s]: \"{question_part}\"")
                        print(f"     Answer fragment: \"{answer_part}\" (currently with Speaker {current_speaker+1})")
                        print(f"     Answer continuation: Speaker {answer_speaker+1} [{next_seg.get('start'):.2f}s-{next_seg.get('end'):.2f}s]: \"{next_text}\"")
                        
                        # –†–æ–∑–¥—ñ–ª—è—î–º–æ: –ø–∏—Ç–∞–Ω–Ω—è –∑–∞–ª–∏—à–∞—î—Ç—å—Å—è –∑ –ø–æ—Ç–æ—á–Ω–∏–º —Å–ø—ñ–∫–µ—Ä–æ–º
                        # –û—Ü—ñ–Ω—é—î–º–æ —á–∞—Å –¥–ª—è –ø–∏—Ç–∞–Ω–Ω—è (–ø—Ä–∏–±–ª–∏–∑–Ω–æ 80-85% –≤—ñ–¥ –∑–∞–≥–∞–ª—å–Ω–æ–≥–æ —á–∞—Å—É)
                        question_duration = current.get('end') - current.get('start')
                        question_end_time = current.get('start') + question_duration * 0.85
                        
                        question_seg = {
                            'speaker': current_speaker,
                            'start': current.get('start'),
                            'end': question_end_time,
                            'text': question_part,
                            'question_answer_split': True
                        }
                        
                        # –í—ñ–¥–ø–æ–≤—ñ–¥—å (–∫–æ—Ä–æ—Ç–∫–∞ —á–∞—Å—Ç–∏–Ω–∞ + –ø—Ä–æ–¥–æ–≤–∂–µ–Ω–Ω—è) –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç—å –¥–æ —Å–ø—ñ–∫–µ—Ä–∞, —è–∫–∏–π –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î
                        combined_answer = (answer_part + ' ' + next_text).strip()
                        answer_seg = {
                            'speaker': answer_speaker,  # –°–ø—ñ–∫–µ—Ä, —è–∫–∏–π –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î
                            'start': question_end_time,
                            'end': next_seg.get('end'),
                            'text': combined_answer,
                            'question_answer_split': True,
                            'original_speakers': [current_speaker, answer_speaker]
                        }
                        
                        fixed.append(question_seg)
                        fixed.append(answer_seg)
                        print(f"     ‚úÖ Fixed: Question ‚Üí Speaker {current_speaker+1}, Answer ‚Üí Speaker {answer_speaker+1}")
                        print(f"        Combined answer: \"{combined_answer}\"")
                        
                        i += 2
                        continue
                    else:
                        print(f"     ‚ö†Ô∏è  Conditions not met: continuation={next_is_continuation}, pause={short_pause}")
        
        # –Ø–∫—â–æ –Ω–µ –≤–∏–ø—Ä–∞–≤–ª—è—î–º–æ - –¥–æ–¥–∞—î–º–æ —è–∫ —î
        fixed.append(current)
        i += 1
    
    if fixed_count > 0:
        print(f"‚úÖ Fixed {fixed_count} answer-after-question speaker assignment(s)")
    else:
        print(f"‚úÖ No answer-after-question issues detected")
    
    return fixed


def enforce_speaker_continuity_rule(segments, max_gap=3.0):
    """
    –ó–∞—Å—Ç–æ—Å–æ–≤—É—î –ø—Ä–∞–≤–∏–ª–æ: —è–∫—â–æ —Å–ø—ñ–∫–µ—Ä –ø–æ—á–∏–Ω–∞—î —Ñ—Ä–∞–∑—É, –≤—ñ–Ω –º–∞—î —ó—ó –∑–∞–∫—ñ–Ω—á–∏—Ç–∏.
    –ù–æ–≤–∏–π —Å–ø—ñ–∫–µ—Ä –Ω–µ –º–æ–∂–µ –ø–æ—á–∞—Ç–∏—Å—è, –ø–æ–∫–∏ —Ä–µ—á–µ–Ω–Ω—è –Ω–µ –¥–æ–≤–µ–¥–µ–Ω–æ –¥–æ –∫—Ä–∞–ø–∫–∏.
    
    –ö—Ä–∏—Ç–µ—Ä—ñ—ó –¥–ª—è –æ–±'—î–¥–Ω–∞–Ω–Ω—è:
    1. –ü–æ—Ç–æ—á–Ω–∞ —Ä–µ–ø–ª—ñ–∫–∞ –ù–ï –∑–∞–∫—ñ–Ω—á—É—î—Ç—å—Å—è –Ω–∞ –∫—Ä–∞–ø–∫—É/–∑–Ω–∞–∫ –ø–∏—Ç–∞–Ω–Ω—è/–≤–∏–≥—É–∫
    2. –ù–∞—Å—Ç—É–ø–Ω–∞ —Ä–µ–ø–ª—ñ–∫–∞ –≥—Ä–∞–º–∞—Ç–∏—á–Ω–æ –ø—Ä–æ–¥–æ–≤–∂—É—î –¥—É–º–∫—É
    3. –ú—ñ–∂ –Ω–∏–º–∏ –º–∞–ª–∞ –ø–∞—É–∑–∞ (< max_gap)
    4. –†—ñ–∑–Ω—ñ —Å–ø—ñ–∫–µ—Ä–∏ (–ø–æ–º–∏–ª–∫–∞ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó)
    
    Args:
        segments: —Å–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ [{'speaker': int, 'start': float, 'end': float, 'text': str}]
        max_gap: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∏–π –ø—Ä–æ–º—ñ–∂–æ–∫ –º—ñ–∂ —Å–µ–≥–º–µ–Ω—Ç–∞–º–∏ –¥–ª—è –æ–±'—î–¥–Ω–∞–Ω–Ω—è (—Å–µ–∫—É–Ω–¥–∏)
    
    Returns:
        merged_segments: —Å–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ –∑ –æ–±'—î–¥–Ω–∞–Ω–∏–º–∏ –Ω–µ–∑–∞–≤–µ—Ä—à–µ–Ω–∏–º–∏ —Ñ—Ä–∞–∑–∞–º–∏
    """
    if not segments or len(segments) < 2:
        return segments
    
    print(f"üîç Enforcing speaker continuity rule in {len(segments)} segments...")
    merged = []
    i = 0
    merged_count = 0
    
    while i < len(segments):
        if i == len(segments) - 1:
            # –û—Å—Ç–∞–Ω–Ω—ñ–π —Å–µ–≥–º–µ–Ω—Ç - –ø—Ä–æ—Å—Ç–æ –¥–æ–¥–∞—î–º–æ
            merged.append(segments[i])
            i += 1
            continue
        
        current = segments[i]
        next_seg = segments[i + 1]
        
        current_text = (current.get('text') or '').strip()
        next_text = (next_seg.get('text') or '').strip()
        
        if not current_text or not next_text:
            merged.append(current)
            i += 1
            continue
        
        # –ö—Ä–∏—Ç–µ—Ä—ñ–π 1: –ü–æ—Ç–æ—á–Ω–∞ —Ä–µ–ø–ª—ñ–∫–∞ –ù–ï –∑–∞–∫—ñ–Ω—á—É—î—Ç—å—Å—è –Ω–∞ –∑–∞–≤–µ—Ä—à–∞–ª—å–Ω–∏–π –∑–Ω–∞–∫
        sentence_endings = ['.', '!', '?']
        current_ends_properly = any(current_text.endswith(ending) for ending in sentence_endings)
        
        # –ö—Ä–∏—Ç–µ—Ä—ñ–π 2: –†—ñ–∑–Ω—ñ —Å–ø—ñ–∫–µ—Ä–∏ (–ø–æ—Ç–µ–Ω—Ü—ñ–π–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó)
        different_speakers = current.get('speaker') != next_seg.get('speaker')
        
        # –ö—Ä–∏—Ç–µ—Ä—ñ–π 3: –ú–∞–ª–∞ –ø–∞—É–∑–∞ –º—ñ–∂ —Å–µ–≥–º–µ–Ω—Ç–∞–º–∏
        pause = next_seg.get('start', 0) - current.get('end', 0)
        short_pause = 0 <= pause <= max_gap
        
        # –ö—Ä–∏—Ç–µ—Ä—ñ–π 4: –ù–∞—Å—Ç—É–ø–Ω–∞ —Ä–µ–ø–ª—ñ–∫–∞ –≥—Ä–∞–º–∞—Ç–∏—á–Ω–æ –ø—Ä–æ–¥–æ–≤–∂—É—î –¥—É–º–∫—É
        is_grammatical_continuation = False
        
        if not current_ends_properly and different_speakers and short_pause:
            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –≥—Ä–∞–º–∞—Ç–∏—á–Ω–æ—ó –∑–≤'—è–∑–Ω–æ—Å—Ç—ñ
            current_lower = current_text.lower()
            next_lower = next_text.lower()
            
            # –û—Å—Ç–∞–Ω–Ω—ñ —Å–ª–æ–≤–∞ –ø–æ—Ç–æ—á–Ω–æ—ó —Ä–µ–ø–ª—ñ–∫–∏ (–±–µ–∑ –ø—É–Ω–∫—Ç—É–∞—Ü—ñ—ó)
            current_words = current_lower.split()
            next_words = next_lower.split()
            
            if current_words and next_words:
                # –ú–∞—Ä–∫–µ—Ä–∏ –Ω–µ–∑–∞–≤–µ—Ä—à–µ–Ω–æ—ó —Ñ—Ä–∞–∑–∏
                incomplete_markers = [
                    'the', 'a', 'an', 'this', 'that', 'these', 'those',
                    'but', 'and', 'or', 'so', 'because', 'although',
                    'to', 'for', 'with', 'from', 'in', 'on', 'at',
                    'i', 'you', 'he', 'she', 'it', 'we', 'they',
                    'is', 'are', 'was', 'were', 'has', 'have', 'had',
                    'can', 'could', 'will', 'would', 'should', 'may', 'might'
                ]
                
                last_word = current_words[-1].rstrip('.,!?;:')
                first_word = next_words[0].rstrip('.,!?;:')
                
                # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ 1: –û—Å—Ç–∞–Ω–Ω—î —Å–ª–æ–≤–æ –ø–æ—Ç–æ—á–Ω–æ—ó —Ä–µ–ø–ª—ñ–∫–∏ - –º–∞—Ä–∫–µ—Ä –Ω–µ–∑–∞–≤–µ—Ä—à–µ–Ω–æ—Å—Ç—ñ
                if last_word in incomplete_markers:
                    is_grammatical_continuation = True
                
                # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ 2: –ü–µ—Ä—à–µ —Å–ª–æ–≤–æ –Ω–∞—Å—Ç—É–ø–Ω–æ—ó —Ä–µ–ø–ª—ñ–∫–∏ –Ω–µ –∑ –≤–µ–ª–∏–∫–æ—ó –ª—ñ—Ç–µ—Ä–∏
                # (—è–∫—â–æ –Ω–µ –ø–æ—á–∏–Ω–∞—î—Ç—å—Å—è –∑ –≤–µ–ª–∏–∫–æ—ó, —Ü–µ –ø—Ä–æ–¥–æ–≤–∂–µ–Ω–Ω—è)
                elif not next_text[0].isupper() and len(next_words) < 15:
                    is_grammatical_continuation = True
                
                # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ 3: –†–∞–∑–æ–º —É—Ç–≤–æ—Ä—é—é—Ç—å –≥—Ä–∞–º–∞—Ç–∏—á–Ω–æ –ø—Ä–∞–≤–∏–ª—å–Ω–µ —Ä–µ—á–µ–Ω–Ω—è
                combined_text = (current_text + ' ' + next_text).strip()
                combined_words = combined_text.split()
                
                # –Ø–∫—â–æ –æ–±'—î–¥–Ω–∞–Ω–∏–π —Ç–µ–∫—Å—Ç –º–∞—î —Å–µ–Ω—Å (–Ω–µ –¥—É–∂–µ –¥–æ–≤–≥–∏–π, –Ω–µ–º–∞—î –ø–æ–¥–≤—ñ–π–Ω–∏—Ö –ø—Ä–æ–±—ñ–ª—ñ–≤)
                if (len(combined_words) < 30 and 
                    '  ' not in combined_text and
                    not combined_text.startswith(next_text.split()[0] if next_text.split() else '')):
                    
                    # –î–æ–¥–∞—Ç–∫–æ–≤–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞: —á–∏ –∑–∞–∫—ñ–Ω—á—É—î—Ç—å—Å—è –æ–±'—î–¥–Ω–∞–Ω–∞ —Ñ—Ä–∞–∑–∞ –Ω–∞ –∫—Ä–∞–ø–∫—É
                    if any(combined_text.endswith(ending) for ending in sentence_endings):
                        is_grammatical_continuation = True
                
                # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ 4: –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ñ –≤–∏–ø–∞–¥–∫–∏ –∑ –ø—Ä–∏–∫–ª–∞–¥—É
                # "I tried, but the" + "problem is still existing" = –ø—Ä–æ–¥–æ–≤–∂–µ–Ω–Ω—è
                if (last_word in ['the', 'a', 'an', 'this', 'that'] and 
                    first_word in ['problem', 'issue', 'connection', 'device', 'modem', 'router']):
                    is_grammatical_continuation = True
        
        # –Ø–∫—â–æ –≤—Å—ñ –∫—Ä–∏—Ç–µ—Ä—ñ—ó –≤–∏–∫–æ–Ω–∞–Ω—ñ - –æ–±'—î–¥–Ω—É—î–º–æ
        if (not current_ends_properly and 
            different_speakers and 
            short_pause and 
            is_grammatical_continuation):
            
            merged_count += 1
            print(f"  üîó Merging incomplete phrase:")
            print(f"     Segment 1: Speaker {current.get('speaker')+1} [{current.get('start'):.2f}s-{current.get('end'):.2f}s]: \"{current_text}\"")
            print(f"     Segment 2: Speaker {next_seg.get('speaker')+1} [{next_seg.get('start'):.2f}s-{next_seg.get('end'):.2f}s]: \"{next_text}\"")
            
            # –û–±'—î–¥–Ω—É—î–º–æ —Å–µ–≥–º–µ–Ω—Ç–∏, –∑–∞–ª–∏—à–∞—é—á–∏ —Å–ø—ñ–∫–µ—Ä–∞, —è–∫–∏–π –ø–æ—á–∞–≤ —Ñ—Ä–∞–∑—É
            merged_seg = {
                'speaker': current.get('speaker'),  # –°–ø—ñ–∫–µ—Ä, —è–∫–∏–π –ø–æ—á–∞–≤ —Ñ—Ä–∞–∑—É
                'start': current.get('start'),
                'end': next_seg.get('end'),
                'text': (current_text + ' ' + next_text).strip(),
                'speaker_continuity_fix': True,  # –ü–æ–∑–Ω–∞—á–∞—î–º–æ –¥–ª—è –ª–æ–≥—É–≤–∞–Ω–Ω—è
                'original_speakers': [current.get('speaker'), next_seg.get('speaker')]
            }
            merged.append(merged_seg)
            print(f"     ‚úÖ Merged: \"{merged_seg['text']}\" ‚Üí Speaker {merged_seg['speaker']+1} (started the phrase)")
            i += 2  # –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –æ–±–∏–¥–≤–∞ —Å–µ–≥–º–µ–Ω—Ç–∏
            continue
        
        # –Ø–∫—â–æ –Ω–µ –æ–±'—î–¥–Ω—É—î–º–æ - –¥–æ–¥–∞—î–º–æ –ø–æ—Ç–æ—á–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç
        merged.append(current)
        i += 1
    
    if merged_count > 0:
        print(f"‚úÖ Speaker continuity rule applied: merged {merged_count} incomplete phrase(s)")
    else:
        print(f"‚úÖ No incomplete phrases detected")
    
    return merged


def combine_diarization_and_transcription(diarization_segments, words):
    """
    –û–±'—î–¥–Ω—É—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó —Ç–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó –Ω–∞ —Ä—ñ–≤–Ω—ñ —Å–ª—ñ–≤ –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç—ñ.
    Args:
        diarization_segments: —Å–µ–≥–º–µ–Ω—Ç–∏ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó [{'speaker': int, 'start': float, 'end': float}]
        words: —Å–ø–∏—Å–æ–∫ —Å–ª—ñ–≤ –∑ timestamps [{'word': str, 'start': float, 'end': float}]
    Returns:
        combined: —Å–ø–∏—Å–æ–∫ –æ–±'—î–¥–Ω–∞–Ω–∏—Ö —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ –∑ —Å–ø—ñ–∫–µ—Ä–æ–º —Ç–∞ —Ç–µ–∫—Å—Ç–æ–º
    """
    if not words:
        print("‚ö†Ô∏è  No words provided for combination")
        return []
    if not diarization_segments:
        print("‚ö†Ô∏è  No diarization segments provided, using default speaker 0")
        # –Ø–∫—â–æ –Ω–µ–º–∞—î –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó, –ø–æ–≤–µ—Ä—Ç–∞—î–º–æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—é –∑ –æ–¥–Ω–∏–º —Å–ø—ñ–∫–µ—Ä–æ–º
        combined = []
        current_start = words[0]['start']
        current_words = []
        for word in words:
            if not word['word'].strip():
                continue
            current_words.append(word['word'])
            # –°—Ç–≤–æ—Ä—é—î–º–æ —Å–µ–≥–º–µ–Ω—Ç–∏ –ø–æ –ø—Ä–æ–º—ñ–∂–∫–∞—Ö >1 —Å–µ–∫
            if len(combined) == 0 or (word['start'] - combined[-1]['end'] > 1.0):
                if current_words:
                    combined.append({
                        'speaker': 0,
                        'start': round(current_start, 2),
                        'end': round(word['end'], 2),
                        'text': ' '.join(current_words).strip()
                    })
                    current_words = []
                    current_start = word['start']
        if current_words:
            combined.append({
                'speaker': 0,
                'start': round(current_start, 2),
                'end': round(words[-1]['end'], 2),
                'text': ' '.join(current_words).strip()
            })
        return combined
    print(f"üîó Combining {len(words)} words with {len(diarization_segments)} diarization segments")
    # –°–æ—Ä—Ç—É—î–º–æ —Å–µ–≥–º–µ–Ω—Ç–∏ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó –∑–∞ —á–∞—Å–æ–º –¥–ª—è —à–≤–∏–¥—à–æ–≥–æ –ø–æ—à—É–∫—É
    sorted_diar_segments = sorted(diarization_segments, key=lambda x: x['start'])
    # –î–ª—è –∫–æ–∂–Ω–æ–≥–æ —Å–ª–æ–≤–∞ –∑–Ω–∞—Ö–æ–¥–∏–º–æ –Ω–∞–π–∫—Ä–∞—â–µ –ø–µ—Ä–µ–∫—Ä–∏—Ç—Ç—è –∑ —Å–µ–≥–º–µ–Ω—Ç–∞–º–∏ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó
    word_speakers = []
    for word_idx, word in enumerate(words):
        word_start = word['start']
        word_end = word['end']
        word_center = (word_start + word_end) / 2.0
        word_text = word['word']
        if not word_text.strip():
            continue
        # –ó–Ω–∞—Ö–æ–¥–∏–º–æ —Å–ø—ñ–∫–µ—Ä–∞ –∑ –Ω–∞–π–±—ñ–ª—å—à–∏–º –ø–µ—Ä–µ–∫—Ä–∏—Ç—Ç—è–º
        best_speaker = None
        best_overlap = 0
        best_overlap_ratio = 0
        # –°–ø–æ—á–∞—Ç–∫—É —à—É–∫–∞—î–º–æ —Å–µ–≥–º–µ–Ω—Ç–∏, —è–∫—ñ –ø–æ–≤–Ω—ñ—Å—Ç—é –º—ñ—Å—Ç—è—Ç—å —Å–ª–æ–≤–æ
        fully_contained_segments = []
        for diar_seg in sorted_diar_segments:
            diar_start = diar_seg['start']
            diar_end = diar_seg['end']
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —Å–ª–æ–≤–æ –ø–æ–≤–Ω—ñ—Å—Ç—é –≤ –º–µ–∂–∞—Ö —Å–µ–≥–º–µ–Ω—Ç–∞
            if word_start >= diar_start and word_end <= diar_end:
                fully_contained_segments.append((diar_seg, diar_seg['speaker']))
        # –Ø–∫—â–æ —î —Å–µ–≥–º–µ–Ω—Ç–∏, —â–æ –ø–æ–≤–Ω—ñ—Å—Ç—é –º—ñ—Å—Ç—è—Ç—å —Å–ª–æ–≤–æ, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —ó—Ö
        if fully_contained_segments:
            # –Ø–∫—â–æ –∫—ñ–ª—å–∫–∞ —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ –º—ñ—Å—Ç—è—Ç—å —Å–ª–æ–≤–æ (overlap), –≤–∏–±–∏—Ä–∞—î–º–æ —Ç–æ–π, –¥–µ —Ü–µ–Ω—Ç—Ä —Å–ª–æ–≤–∞
            for diar_seg, speaker in fully_contained_segments:
                diar_start = diar_seg['start']
                diar_end = diar_seg['end']
                # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —Ü–µ–Ω—Ç—Ä —Å–ª–æ–≤–∞ –≤ –º–µ–∂–∞—Ö —Å–µ–≥–º–µ–Ω—Ç–∞
                if word_center >= diar_start and word_center <= diar_end:
                    best_speaker = speaker
                    best_overlap = word_end - word_start  # –ü–æ–≤–Ω–µ –ø–µ—Ä–µ–∫—Ä–∏—Ç—Ç—è
                    best_overlap_ratio = 1.0
                    break
            # –Ø–∫—â–æ —Ü–µ–Ω—Ç—Ä –Ω–µ –≤ –∂–æ–¥–Ω–æ–º—É —Å–µ–≥–º–µ–Ω—Ç—ñ, –≤–∏–±–∏—Ä–∞—î–º–æ –Ω–∞–π–±–ª–∏–∂—á–∏–π
            if best_speaker is None:
                min_distance = float('inf')
                for diar_seg, speaker in fully_contained_segments:
                    diar_start = diar_seg['start']
                    diar_end = diar_seg['end']
                    # –í—ñ–¥—Å—Ç–∞–Ω—å –≤—ñ–¥ —Ü–µ–Ω—Ç—Ä—É —Å–ª–æ–≤–∞ –¥–æ —Ü–µ–Ω—Ç—Ä—É —Å–µ–≥–º–µ–Ω—Ç–∞
                    seg_center = (diar_start + diar_end) / 2.0
                    distance = abs(word_center - seg_center)
                    if distance < min_distance:
                        min_distance = distance
                        best_speaker = speaker
                        best_overlap = word_end - word_start
                        best_overlap_ratio = 1.0
        # –Ø–∫—â–æ —Å–ª–æ–≤–æ –Ω–µ –ø–æ–≤–Ω—ñ—Å—Ç—é –≤ –∂–æ–¥–Ω–æ–º—É —Å–µ–≥–º–µ–Ω—Ç—ñ, —à—É–∫–∞—î–º–æ –Ω–∞–π–∫—Ä–∞—â–µ –ø–µ—Ä–µ–∫—Ä–∏—Ç—Ç—è
        if best_speaker is None:
            for diar_seg in sorted_diar_segments:
                diar_start = diar_seg['start']
                diar_end = diar_seg['end']
                # –û–±—á–∏—Å–ª—é—î–º–æ –ø–µ—Ä–µ–∫—Ä–∏—Ç—Ç—è
                overlap_start = max(word_start, diar_start)
                overlap_end = min(word_end, diar_end)
                overlap = max(0, overlap_end - overlap_start)
                if overlap > 0:
                    # –û–±—á–∏—Å–ª—é—î–º–æ –≤—ñ–¥–Ω–æ—à–µ–Ω–Ω—è –ø–µ—Ä–µ–∫—Ä–∏—Ç—Ç—è –¥–æ –¥–æ–≤–∂–∏–Ω–∏ —Å–ª–æ–≤–∞
                    word_duration = word_end - word_start
                    overlap_ratio = overlap / word_duration if word_duration > 0 else 0
                    # –í—Ä–∞—Ö–æ–≤—É—î–º–æ —Ç–∞–∫–æ–∂, —á–∏ —Ü–µ–Ω—Ç—Ä —Å–ª–æ–≤–∞ –≤ –º–µ–∂–∞—Ö —Å–µ–≥–º–µ–Ω—Ç–∞
                    center_in_segment = (word_center >= diar_start and word_center <= diar_end)
                    # –ü—Ä—ñ–æ—Ä–∏—Ç–µ—Ç: —Ü–µ–Ω—Ç—Ä –≤ —Å–µ–≥–º–µ–Ω—Ç—ñ > –±—ñ–ª—å—à–µ –ø–µ—Ä–µ–∫—Ä–∏—Ç—Ç—è > –±—ñ–ª—å—à–µ –≤—ñ–¥–Ω–æ—à–µ–Ω–Ω—è
                    if (center_in_segment and best_overlap_ratio < 0.5) or \
                       (overlap > best_overlap) or \
                       (overlap == best_overlap and overlap_ratio > best_overlap_ratio):
                        best_overlap = overlap
                        best_overlap_ratio = overlap_ratio
                        best_speaker = diar_seg['speaker']
        # –Ø–∫—â–æ –≤—Å–µ —â–µ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –Ω–∞–π–±–ª–∏–∂—á–∏–π —Å–µ–≥–º–µ–Ω—Ç –∑–∞ —á–∞—Å–æ–º
        if best_speaker is None:
            min_distance = float('inf')
            for diar_seg in sorted_diar_segments:
                diar_start = diar_seg['start']
                diar_end = diar_seg['end']
                # –í—ñ–¥—Å—Ç–∞–Ω—å –≤—ñ–¥ —Ü–µ–Ω—Ç—Ä—É —Å–ª–æ–≤–∞ –¥–æ –Ω–∞–π–±–ª–∏–∂—á–æ—ó —Ç–æ—á–∫–∏ —Å–µ–≥–º–µ–Ω—Ç–∞
                if word_center < diar_start:
                    distance = diar_start - word_center
                elif word_center > diar_end:
                    distance = word_center - diar_end
                else:
                    distance = 0
                if distance < min_distance:
                    min_distance = distance
                    best_speaker = diar_seg['speaker']
        speaker_id = best_speaker if best_speaker is not None else 0
        word_speakers.append({
            'word': word_text,
            'start': word_start,
            'end': word_end,
            'speaker': speaker_id
        })
    # –î–æ–¥–∞—Ç–∫–æ–≤–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞: –≤–∏–ø—Ä–∞–≤–ª—è—î–º–æ –ø—Ä–∏–≤'—è–∑–∫—É —Å–ª—ñ–≤ –Ω–∞ –º–µ–∂–∞—Ö —Å–µ–≥–º–µ–Ω—Ç—ñ–≤
    # –Ø–∫—â–æ —Å–ª–æ–≤–æ –ø–æ—á–∏–Ω–∞—î—Ç—å—Å—è –ø—ñ—Å–ª—è –∑–∞–∫—ñ–Ω—á–µ–Ω–Ω—è —Å–µ–≥–º–µ–Ω—Ç–∞ –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞,
    # –≤–æ–Ω–æ –º–∞—î –Ω–∞–ª–µ–∂–∞—Ç–∏ –Ω–∞—Å—Ç—É–ø–Ω–æ–º—É —Å–ø—ñ–∫–µ—Ä—É
    for i in range(len(word_speakers)):
        word_info = word_speakers[i]
        word_start = word_info['start']
        word_center = (word_info['start'] + word_info['end']) / 2.0
        # –ó–Ω–∞—Ö–æ–¥–∏–º–æ —Å–µ–≥–º–µ–Ω—Ç, —â–æ –∑–∞–∫—ñ–Ω—á—É—î—Ç—å—Å—è –Ω–∞–π–±–ª–∏–∂—á–µ –ø–µ—Ä–µ–¥ –ø–æ—á–∞—Ç–∫–æ–º —Ü—å–æ–≥–æ —Å–ª–æ–≤–∞
        segments_ending_before = [seg for seg in sorted_diar_segments 
                                   if seg['end'] <= word_start]
        # –ó–Ω–∞—Ö–æ–¥–∏–º–æ —Å–µ–≥–º–µ–Ω—Ç, —â–æ –ø–æ—á–∏–Ω–∞—î—Ç—å—Å—è –Ω–∞–π–±–ª–∏–∂—á–µ –ø—ñ—Å–ª—è –ø–æ—á–∞—Ç–∫—É —Ü—å–æ–≥–æ —Å–ª–æ–≤–∞
        segments_starting_after = [seg for seg in sorted_diar_segments 
                                     if seg['start'] >= word_start]
        # –Ø–∫—â–æ —î —Å–µ–≥–º–µ–Ω—Ç, —â–æ –ø–æ—á–∏–Ω–∞—î—Ç—å—Å—è –ø—ñ—Å–ª—è –∑–∞–∫—ñ–Ω—á–µ–Ω–Ω—è –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ–≥–æ
        if segments_ending_before and segments_starting_after:
            last_ending_seg = max(segments_ending_before, key=lambda x: x['end'])
            first_starting_seg = min(segments_starting_after, key=lambda x: x['start'])
            # –Ø–∫—â–æ —î —á—ñ—Ç–∫–∏–π –ø–µ—Ä–µ—Ö—ñ–¥ –º—ñ–∂ —Å–ø—ñ–∫–µ—Ä–∞–º–∏ (–ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π –∑–∞–∫—ñ–Ω—á–∏–≤—Å—è, –Ω–∞—Å—Ç—É–ø–Ω–∏–π –ø–æ—á–∞–≤—Å—è)
            if last_ending_seg['end'] <= first_starting_seg['start']:
                # –Ø–∫—â–æ —Ü–µ–Ω—Ç—Ä —Å–ª–æ–≤–∞ –∑–Ω–∞—Ö–æ–¥–∏—Ç—å—Å—è –ø—ñ—Å–ª—è –∑–∞–∫—ñ–Ω—á–µ–Ω–Ω—è –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞
                # –∞–±–æ —Å–ª–æ–≤–æ –ø–æ—á–∏–Ω–∞—î—Ç—å—Å—è –ø—ñ—Å–ª—è –∑–∞–∫—ñ–Ω—á–µ–Ω–Ω—è –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞,
                # –≤–æ–Ω–æ –º–∞—î –Ω–∞–ª–µ–∂–∞—Ç–∏ –Ω–∞—Å—Ç—É–ø–Ω–æ–º—É —Å–ø—ñ–∫–µ—Ä—É
                if word_center >= last_ending_seg['end'] or word_start >= last_ending_seg['end']:
                    # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –ø–æ—Ç–æ—á–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ –¥–ª—è –ª–æ–≥—É–≤–∞–Ω–Ω—è
                    current_speaker = word_info['speaker']
                    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ –ø–æ—Ç–æ—á–Ω–∏–π —Å–ø—ñ–∫–µ—Ä –Ω–µ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î –Ω–∞—Å—Ç—É–ø–Ω–æ–º—É —Å–µ–≥–º–µ–Ω—Ç—É
                    if current_speaker != first_starting_seg['speaker']:
                        # –Ø–∫—â–æ —Å–ª–æ–≤–æ –±–ª–∏–∂—á–µ –¥–æ –Ω–∞—Å—Ç—É–ø–Ω–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞, –ø—Ä–∏–≤'—è–∑—É—î–º–æ –¥–æ –Ω—å–æ–≥–æ
                        distance_to_prev = word_start - last_ending_seg['end']
                        distance_to_next = first_starting_seg['start'] - word_start
                        # –Ø–∫—â–æ —Å–ª–æ–≤–æ –ø–æ—á–∏–Ω–∞—î—Ç—å—Å—è –ø—ñ—Å–ª—è –∑–∞–∫—ñ–Ω—á–µ–Ω–Ω—è –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞
                        # —ñ –±–ª–∏–∂—á–µ –¥–æ –Ω–∞—Å—Ç—É–ø–Ω–æ–≥–æ, –ø—Ä–∏–≤'—è–∑—É—î–º–æ –¥–æ –Ω–∞—Å—Ç—É–ø–Ω–æ–≥–æ
                        if word_start >= last_ending_seg['end'] and (distance_to_next < distance_to_prev or distance_to_next < 0.3):
                            word_info['speaker'] = first_starting_seg['speaker']
                            print(f"üîß Fixed word '{word_info['word']}' at {word_start:.2f}s: assigned to speaker {first_starting_seg['speaker']} (was {current_speaker})")
    # –î—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞: –ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ä–æ–∑–ø–æ–¥—ñ–ª —Å–ø—ñ–∫–µ—Ä—ñ–≤
    speakers_found = set(w['speaker'] for w in word_speakers)
    print(f"üìä Word-level speakers: {len(speakers_found)} unique speakers found: {sorted(speakers_found)}")
    # –ì—Ä—É–ø—É—î–º–æ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω—ñ —Å–ª–æ–≤–∞ –æ–¥–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ –≤ —Å–µ–≥–º–µ–Ω—Ç–∏
    combined = []
    if not word_speakers:
        return combined
    current_speaker = word_speakers[0]['speaker']
    current_start = word_speakers[0]['start']
    current_words = [word_speakers[0]['word']]
    for i in range(1, len(word_speakers)):
        word_info = word_speakers[i]
        prev_word_info = word_speakers[i-1]
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —î —Å–µ–≥–º–µ–Ω—Ç –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó, —â–æ –ø–æ—á–∏–Ω–∞—î—Ç—å—Å—è –º—ñ–∂ —Å–ª–æ–≤–∞–º–∏
        # –Ø–∫—â–æ —Ç–∞–∫, —ñ —Å–ª–æ–≤–æ –Ω–∞–ª–µ–∂–∏—Ç—å —ñ–Ω—à–æ–º—É —Å–ø—ñ–∫–µ—Ä—É, —Ü–µ –ø–µ—Ä–µ–±–∏–≤–∫–∞
        gap_start = prev_word_info['end']
        gap_end = word_info['start']
        # –ó–Ω–∞—Ö–æ–¥–∏–º–æ —Å–µ–≥–º–µ–Ω—Ç–∏, —â–æ –ø–æ—á–∏–Ω–∞—é—Ç—å—Å—è –≤ –ø—Ä–æ–º—ñ–∂–∫—É –º—ñ–∂ —Å–ª–æ–≤–∞–º–∏
        segments_in_gap = [seg for seg in sorted_diar_segments 
                           if seg['start'] >= gap_start and seg['start'] <= gap_end]
        # –Ø–∫—â–æ —Å–ø—ñ–∫–µ—Ä –∑–º—ñ–Ω–∏–≤—Å—è –∞–±–æ –≤–µ–ª–∏–∫–∏–π –ø—Ä–æ–º—ñ–∂–æ–∫ (>1 —Å–µ–∫), —Å—Ç–≤–æ—Ä—é—î–º–æ –Ω–æ–≤–∏–π —Å–µ–≥–º–µ–Ω—Ç
        # –ê–±–æ —è–∫—â–æ —î —Å–µ–≥–º–µ–Ω—Ç —ñ–Ω—à–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ –≤ –ø—Ä–æ–º—ñ–∂–∫—É
        should_split = False
        if word_info['speaker'] != current_speaker:
            should_split = True
        elif gap_end - gap_start > 1.0:
            should_split = True
        elif segments_in_gap:
            # –Ø–∫—â–æ —î —Å–µ–≥–º–µ–Ω—Ç —ñ–Ω—à–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ –≤ –ø—Ä–æ–º—ñ–∂–∫—É, —Ä–æ–∑–¥—ñ–ª—è—î–º–æ
            for seg in segments_in_gap:
                if seg['speaker'] != current_speaker:
                    should_split = True
                    break
        if should_split:
            # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –ø–æ—Ç–æ—á–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç
            combined.append({
                'speaker': current_speaker,
                'start': round(current_start, 2),
                'end': round(prev_word_info['end'], 2),
                'text': ' '.join(current_words).strip()
            })
            # –ü–æ—á–∏–Ω–∞—î–º–æ –Ω–æ–≤–∏–π —Å–µ–≥–º–µ–Ω—Ç
            current_speaker = word_info['speaker']
            current_start = word_info['start']
            current_words = [word_info['word']]
        else:
            # –î–æ–¥–∞—î–º–æ —Å–ª–æ–≤–æ –¥–æ –ø–æ—Ç–æ—á–Ω–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞
            current_words.append(word_info['word'])
    # –î–æ–¥–∞—î–º–æ –æ—Å—Ç–∞–Ω–Ω—ñ–π —Å–µ–≥–º–µ–Ω—Ç
    if current_words:
        combined.append({
            'speaker': current_speaker,
            'start': round(current_start, 2),
            'end': round(word_speakers[-1]['end'], 2),
            'text': ' '.join(current_words).strip()
        })
    # –î—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞: –ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ñ—ñ–Ω–∞–ª—å–Ω–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    final_speakers = set(seg['speaker'] for seg in combined)
    print(f"‚úÖ Combined result: {len(combined)} segments, {len(final_speakers)} unique speakers: {sorted(final_speakers)}")
    
    # –ö–†–ò–¢–ò–ß–ù–ê –ü–ï–†–ï–í–Ü–†–ö–ê 0: –í–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è –ø–æ–º–∏–ª–æ–∫ "–ø–∏—Ç–∞–Ω–Ω—è + –∫–æ—Ä–æ—Ç–∫–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å" –≤ –æ–¥–Ω–æ–º—É —Å–µ–≥–º–µ–Ω—Ç—ñ
    print(f"üîç Fixing answer-after-question speaker assignments...")
    combined = fix_answer_after_question_speaker_assignment_v2(combined, max_gap=3.0)
    
    # –ö–†–ò–¢–ò–ß–ù–ê –ü–ï–†–ï–í–Ü–†–ö–ê 1: –ü—Ä–∞–≤–∏–ª–æ –Ω–µ–ø–µ—Ä–µ—Ä–≤–Ω–æ—Å—Ç—ñ —Å–ø—ñ–∫–µ—Ä–∞ (—Å–ø—ñ–∫–µ—Ä, —è–∫–∏–π –ø–æ—á–∞–≤ —Ñ—Ä–∞–∑—É, –º–∞—î —ó—ó –∑–∞–∫—ñ–Ω—á–∏—Ç–∏)
    print(f"üîç Applying speaker continuity rule (speaker who started phrase must finish it)...")
    combined = enforce_speaker_continuity_rule(combined, max_gap=3.0)
    
    # –û–±'—î–¥–Ω—É—î–º–æ —Å—É—Å—ñ–¥–Ω—ñ —Å–µ–≥–º–µ–Ω—Ç–∏ –æ–¥–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ –¥–ª—è –∑–º–µ–Ω—à–µ–Ω–Ω—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü—ñ—ó
    # –ê–õ–ï: –∑–±–µ—Ä—ñ–≥–∞—î–º–æ –≤—Å—ñ —Å–µ–≥–º–µ–Ω—Ç–∏, –Ω–µ –æ–±'—î–¥–Ω—É—î–º–æ –∑–∞–Ω–∞–¥—Ç–æ –∞–≥—Ä–µ—Å–∏–≤–Ω–æ
    print(f"üìä Before merging: {len(combined)} segments")
    combined = merge_consecutive_speaker_segments(combined, max_gap=1.5)  # –ó–º–µ–Ω—à—É—î–º–æ max_gap –¥–ª—è –º–µ–Ω—à –∞–≥—Ä–µ—Å–∏–≤–Ω–æ–≥–æ –æ–±'—î–¥–Ω–∞–Ω–Ω—è
    print(f"üìä After merging: {len(combined)} segments")
    
    # –ö–†–ò–¢–ò–ß–ù–ê –ü–ï–†–ï–í–Ü–†–ö–ê 2: –í–∏—è–≤–ª–µ–Ω–Ω—è –ø–æ–º–∏–ª–æ–∫ –ø—Ä–∏–∑–Ω–∞—á–µ–Ω–Ω—è —Å–ø—ñ–∫–µ—Ä—ñ–≤ (–∑–∞–≤–µ—Ä—à–µ–Ω–∞ –¥—É–º–∫–∞ ‚Üí –ø–∏—Ç–∞–Ω–Ω—è)
    print(f"üîç Checking for speaker assignment errors (complete statement ‚Üí question pattern)...")
    combined = detect_and_fix_speaker_mismatch_after_complete_statement(combined)
    
    # –ö–†–ò–¢–ò–ß–ù–ê –ü–ï–†–ï–í–Ü–†–ö–ê 3: –í–∏—è–≤–ª–µ–Ω–Ω—è —Ä–æ–∑–±–∏—Ç–∏—Ö —Ñ—Ä–∞–∑ (–ø–µ—Ä—à–∞ –ø–æ—á–∏–Ω–∞—î—Ç—å—Å—è –∑ –≤–µ–ª–∏–∫–æ—ó, –¥—Ä—É–≥–∞ –∑–∞–∫—ñ–Ω—á—É—î—Ç—å—Å—è –Ω–∞ ?)
    print(f"üîç Checking for fragmented phrases (split sentences)...")
    combined = detect_and_merge_fragmented_phrases(combined)
    
    # –ù–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ –ø–æ—Ä—è–¥–æ–∫ —Å–ø—ñ–∫–µ—Ä—ñ–≤ –ü–ï–†–ï–î LLM –æ–±—Ä–æ–±–∫–æ—é
    print(f"üîß Normalizing speaker order before LLM processing...")
    combined = normalize_speaker_order(combined)
    
    # –ù–û–í–ê –ö–ê–°–ö–ê–î–ù–ê –°–ò–°–¢–ï–ú–ê: –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –¥–≤–æ–µ—Ç–∞–ø–Ω—É —Å–∏—Å—Ç–µ–º—É –∑ –µ—Å–∫–∞–ª–∞—Ü—ñ—î—é
    try:
        global _llm_iterations_cache
        print(f"ü§ñ Starting Cascading Diarization System...")
        print(f"üìä Input: {len(combined)} segments")
        
        # –§–æ—Ä–º—É—î–º–æ —Ç–µ–∫—Å—Ç –¥–ª—è –∫–∞—Å–∫–∞–¥–Ω–æ—ó —Å–∏—Å—Ç–µ–º–∏
        full_text = "\n".join([
            f"Speaker {seg['speaker']+1} [{seg['start']:.2f}s-{seg['end']:.2f}s]: {seg['text']}"
            for seg in combined
        ])
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ –∞–¥–∞–ø—Ç–µ—Ä–∏ –¥–ª—è –º–æ–¥–µ–ª–µ–π
        def call_fast_model(prompt):
            """Adapter for 1B model (fast)"""
            return _llm_request(
                "http://127.0.0.1:3001/v1/chat/completions",
                "google/gemma-3-1b",
                "You are a fast diarization tool. Return only JSON.",
                prompt,
                max_tokens=500
            )
        
        def call_smart_model(prompt):
            """Adapter for 20B model (smart, high reasoning)"""
            return _llm_request(
                "http://127.0.0.1:3001/v1/chat/completions",
                "openai/gpt-oss-20b",
                "You are an expert dialogue analyst with advanced reasoning capabilities. Use heavy reasoning.",
                prompt,
                max_tokens=800
            )
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ –∫–æ–Ω—Ç—Ä–æ–ª–µ—Ä –∫–∞—Å–∫–∞–¥–Ω–æ—ó —Å–∏—Å—Ç–µ–º–∏
        controller = CascadingDiarizationController(
            fast_model_func=call_fast_model,
            smart_model_func=call_smart_model,
            agent_context="Customer service representative, professional, offers solutions",
            client_context="Customer seeking help, may be emotional, describes problems"
        )
        
        # –°–ø–æ—á–∞—Ç–∫—É –ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Å–µ–≥–º–µ–Ω—Ç–∏, —è–∫—ñ –ø–æ—Ç—Ä–µ–±—É—é—Ç—å –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ —Å–ø—ñ–∫–µ—Ä–∞
        # 1. Fragmented phrases
        segments_needing_verification = [
            seg for seg in combined 
            if seg.get('needs_speaker_verification', False) and seg.get('fragmented_merge', False)
        ]
        
        # 2. Segments with reassignment after complete statement
        segments_needing_role_verification = [
            seg for seg in combined 
            if seg.get('needs_role_verification', False) and seg.get('reassignment_reason') == 'question_after_complete_statement'
        ]
        
        # –û–±'—î–¥–Ω—É—î–º–æ –æ–±–∏–¥–≤–∞ —Ç–∏–ø–∏ –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏
        all_segments_for_verification = segments_needing_verification + segments_needing_role_verification
        
        if all_segments_for_verification:
            print(f"üîç Found {len(all_segments_for_verification)} segments needing speaker/role verification...")
            for seg in all_segments_for_verification:
                # –°—Ç–≤–æ—Ä—é—î–º–æ —Å–ø–µ—Ü—ñ–∞–ª—å–Ω–∏–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ —Å–ø—ñ–∫–µ—Ä–∞
                if seg.get('fragmented_merge', False):
                    # Fragmented phrase
                    verification_prompt = f"""You are an expert dialogue analyst.

CONTEXT:
- Agent Role: Customer service representative, professional, offers solutions, asks process questions.
- Client Role: Customer seeking help, may be emotional, describes problems, asks about product/service.

MERGED FRAGMENTED PHRASE (was split incorrectly by diarization):
"{seg['text']}"

This phrase was incorrectly split into two segments with different speakers:
- Original Speaker 1: {seg.get('original_speakers', [0, 1])[0] + 1}
- Original Speaker 2: {seg.get('original_speakers', [0, 1])[1] + 1}

CRITICAL: Determine the CORRECT speaker for this complete phrase.

Consider:
- Who typically asks questions like this? (Agent asks process questions, Client asks for help)
- The grammatical structure and tone
- Question patterns: "Did you try to..." is typically from Agent (checking what client tried)

Return ONLY a JSON object:
{{
  "speaker": "Agent" or "Client",
  "confidence": 0.0-1.0,
  "reasoning": "brief explanation"
}}"""
                else:
                    # Question after complete statement
                    verification_prompt = f"""You are an expert dialogue analyst.

CONTEXT:
- Agent Role: Customer service representative, professional, offers solutions, asks process questions.
- Client Role: Customer seeking help, may be emotional, describes problems, asks about product/service.

SEGMENT TO VERIFY:
"{seg['text']}"

This segment was detected as a QUESTION that follows a complete statement from the same speaker (likely a diarization error).

CRITICAL: Determine the CORRECT speaker for this question.

Consider:
- Questions like "Hey, did you try to...", "Can you...", "Have you..." are typically from Agent (checking what client tried)
- The context: if previous segment was a problem description, this question is likely from Agent
- Question patterns: process questions are typically from Agent

Return ONLY a JSON object:
{{
  "speaker": "Agent" or "Client",
  "confidence": 0.0-1.0,
  "reasoning": "brief explanation"
}}"""
                
                # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ smart model –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏
                smart_response = call_smart_model(verification_prompt)
                
                try:
                    import json
                    import re
                    json_match = re.search(r'\{.*?\}', smart_response, re.DOTALL)
                    if json_match:
                        verification_data = json.loads(json_match.group())
                        correct_speaker = verification_data.get('speaker', 'Client')
                        speaker_num = 0 if correct_speaker == 'Agent' else 1
                        seg['speaker'] = speaker_num
                        seg['verification_confidence'] = verification_data.get('confidence', 0.8)
                        seg['verification_reasoning'] = verification_data.get('reasoning', '')
                        print(f"  ‚úÖ Verified fragmented phrase: Speaker {speaker_num+1} ({correct_speaker}) - {verification_data.get('reasoning', '')}")
                except Exception as e:
                    print(f"  ‚ö†Ô∏è  Could not parse verification response: {e}")
                    # –ó–∞–ª–∏—à–∞—î–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ –ø–µ—Ä—à–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞
        
        # –û–±—Ä–æ–±–ª—è—î–º–æ —á–µ—Ä–µ–∑ –∫–∞—Å–∫–∞–¥–Ω—É —Å–∏—Å—Ç–µ–º—É (—è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ)
        # –ê–ª–µ —Å–ø–æ—á–∞—Ç–∫—É –ø–µ—Ä–µ–≤—ñ—Ä–∏–º–æ, —á–∏ —î —â–µ —Å–µ–≥–º–µ–Ω—Ç–∏ –¥–ª—è –æ–±—Ä–æ–±–∫–∏
        if len(combined) > 0:
            # –§–æ—Ä–º—É—î–º–æ —Ç–µ–∫—Å—Ç –¥–ª—è –∫–∞—Å–∫–∞–¥–Ω–æ—ó —Å–∏—Å—Ç–µ–º–∏ (–æ–Ω–æ–≤–ª–µ–Ω–∏–π –ø—ñ—Å–ª—è –æ–±'—î–¥–Ω–∞–Ω–Ω—è)
            full_text = "\n".join([
                f"Speaker {seg['speaker']+1} [{seg['start']:.2f}s-{seg['end']:.2f}s]: {seg['text']}"
                for seg in combined
            ])
            
            cascading_segments = controller.process_full_text(
                full_text=full_text,
                context_summary="Customer service conversation",
                max_chunk_size=1000
            )
        
        # –ö–†–ò–¢–ò–ß–ù–û: –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∫–∞—Å–∫–∞–¥–Ω–æ—ó —Å–∏—Å—Ç–µ–º–∏ –Ω–∞–∑–∞–¥ —É —Ñ–æ—Ä–º–∞—Ç combined
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –í–°–Ü –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω—ñ —Å–µ–≥–º–µ–Ω—Ç–∏ —Ç–∞ —ó—Ö timestamps
        # –ú–∞–ø–ø–∏–º–æ 'Agent' -> 0, 'Client' -> 1
        combined_cascading = []
        used_original_indices = set()  # –í—ñ–¥—Å—Ç–µ–∂—É—î–º–æ, —è–∫—ñ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω—ñ —Å–µ–≥–º–µ–Ω—Ç–∏ –≤–∂–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω—ñ
        
        for casc_seg in cascading_segments:
            speaker_num = 0 if casc_seg.speaker == 'Agent' else 1
            # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –Ω–∞–π–∫—Ä–∞—â–∏–π –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç —É combined –¥–ª—è timestamp
            matching_seg = None
            best_match_idx = -1
            best_similarity = 0.0
            
            for idx, orig_seg in enumerate(combined):
                if idx in used_original_indices:
                    continue
                
                orig_text = orig_seg['text'].strip().lower()
                casc_text = casc_seg.text.strip().lower()
                
                # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ä—ñ–∑–Ω—ñ —Ç–∏–ø–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–æ—Å—Ç—ñ
                if orig_text in casc_text or casc_text in orig_text:
                    # –¢–æ—á–Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω—ñ—Å—Ç—å
                    similarity = min(len(orig_text), len(casc_text)) / max(len(orig_text), len(casc_text), 1)
                    if similarity > best_similarity:
                        best_similarity = similarity
                        matching_seg = orig_seg
                        best_match_idx = idx
                elif len(orig_text) > 10 and len(casc_text) > 10:
                    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –ø–µ—Ä–µ—Ç–∏–Ω —Å–ª—ñ–≤
                    orig_words = set(orig_text.split())
                    casc_words = set(casc_text.split())
                    if orig_words and casc_words:
                        word_overlap = len(orig_words & casc_words) / len(orig_words | casc_words)
                        if word_overlap > 0.5 and word_overlap > best_similarity:
                            best_similarity = word_overlap
                            matching_seg = orig_seg
                            best_match_idx = idx
            
            if matching_seg and best_match_idx >= 0:
                used_original_indices.add(best_match_idx)
                combined_cascading.append({
                    'speaker': speaker_num,
                    'start': matching_seg['start'],
                    'end': matching_seg['end'],
                    'text': casc_seg.text,  # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ç–µ–∫—Å—Ç –∑ –∫–∞—Å–∫–∞–¥–Ω–æ—ó —Å–∏—Å—Ç–µ–º–∏ (–º–æ–∂–µ –±—É—Ç–∏ –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–∏–π)
                    'confidence': casc_seg.confidence,
                    'needs_escalation': casc_seg.needs_escalation,
                    'final_decision_basis': casc_seg.final_decision_basis
                })
            else:
                # –Ø–∫—â–æ –Ω–µ –∑–Ω–∞–π—à–ª–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω—ñ—Å—Ç—å, –Ω–∞–º–∞–≥–∞—î–º–æ—Å—è –∑–Ω–∞–π—Ç–∏ –Ω–∞–π–±–ª–∏–∂—á–∏–π —Å–µ–≥–º–µ–Ω—Ç –∑–∞ —á–∞—Å–æ–º
                # –ê–ª–µ —Ü–µ –Ω–µ–±–µ–∑–ø–µ—á–Ω–æ - –∫—Ä–∞—â–µ –Ω–µ –¥–æ–¥–∞–≤–∞—Ç–∏ —Å–µ–≥–º–µ–Ω—Ç–∏ –±–µ–∑ timestamps
                print(f"  ‚ö†Ô∏è  Could not match cascading segment: \"{casc_seg.text[:50]}...\" - skipping to preserve timestamps")
                # –ù–ï –¥–æ–¥–∞—î–º–æ —Å–µ–≥–º–µ–Ω—Ç–∏ –±–µ–∑ timestamps - –≤–æ–Ω–∏ –≤—Ç—Ä–∞—á–∞—é—Ç—å—Å—è
        
        # –î–æ–¥–∞—î–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω—ñ —Å–µ–≥–º–µ–Ω—Ç–∏, —è–∫—ñ –Ω–µ –±—É–ª–∏ –æ–±—Ä–æ–±–ª–µ–Ω—ñ –∫–∞—Å–∫–∞–¥–Ω–æ—é —Å–∏—Å—Ç–µ–º–æ—é
        for idx, orig_seg in enumerate(combined):
            if idx not in used_original_indices:
                print(f"  üìå Preserving original segment {idx+1}: Speaker {orig_seg['speaker']+1} [{orig_seg['start']:.2f}s-{orig_seg['end']:.2f}s]")
                combined_cascading.append(orig_seg)
        
        # –°–æ—Ä—Ç—É—î–º–æ –∑–∞ —á–∞—Å–æ–º –ø–æ—á–∞—Ç–∫—É
        combined_cascading.sort(key=lambda x: x.get('start', 0))
        
        # –ö–†–ò–¢–ò–ß–ù–û: –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ –∫–∞—Å–∫–∞–¥–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ –Ω–µ –≤—Ç—Ä–∞—Ç–∏–ª–∞ —Å–µ–≥–º–µ–Ω—Ç–∏
        if combined_cascading and len(combined_cascading) > 0:
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ –Ω–µ –∑–º–µ–Ω—à–∏–ª–∞—Å—è –∫—Ä–∏—Ç–∏—á–Ω–æ
            original_count = len(combined)
            cascading_count = len(combined_cascading)
            
            if cascading_count < original_count * 0.5:  # –í—Ç—Ä–∞—á–µ–Ω–æ –±—ñ–ª—å—à–µ 50% —Å–µ–≥–º–µ–Ω—Ç—ñ–≤
                print(f"‚ö†Ô∏è  Cascading system lost too many segments ({original_count} ‚Üí {cascading_count}), falling back to standard LLM...")
                combined, llm_iterations = fix_diarization_errors_with_llm(combined)
                _llm_iterations_cache = llm_iterations
            else:
                # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ –≤—Å—ñ —Å–µ–≥–º–µ–Ω—Ç–∏ –º–∞—é—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—ñ timestamps
                segments_with_zero_timestamp = [s for s in combined_cascading if s.get('start', 0) == 0.0 and s.get('end', 0) == 0.0]
                if len(segments_with_zero_timestamp) > len(combined_cascading) * 0.3:  # –ë—ñ–ª—å—à–µ 30% –±–µ–∑ timestamps
                    print(f"‚ö†Ô∏è  Too many segments without timestamps ({len(segments_with_zero_timestamp)}/{len(combined_cascading)}), falling back to standard LLM...")
                    combined, llm_iterations = fix_diarization_errors_with_llm(combined)
                    _llm_iterations_cache = llm_iterations
                else:
                    print(f"‚úÖ Cascading system completed: {len(combined_cascading)} segments (was {original_count})")
                    combined = combined_cascading
                    _llm_iterations_cache = []  # –ö–∞—Å–∫–∞–¥–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ –º–∞—î —Å–≤–æ—é –ª–æ–≥—ñ–∫—É
        else:
            # Fallback –¥–æ —Å—Ç–∞—Ä–æ—ó —Å–∏—Å—Ç–µ–º–∏
            print(f"‚ö†Ô∏è  Cascading system returned no results, falling back to standard LLM...")
            combined, llm_iterations = fix_diarization_errors_with_llm(combined)
            _llm_iterations_cache = llm_iterations
        
        # –ü—ñ—Å–ª—è LLM –æ–±—Ä–æ–±–∫–∏ –∑–Ω–æ–≤—É –Ω–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ
        combined = normalize_speaker_order(combined)
        
    except Exception as e:
        print(f"‚ö†Ô∏è  Error in cascading diarization system: {e}")
        import traceback
        error_traceback = traceback.format_exc()
        print(f"üìã Cascading Error traceback:\n{error_traceback}")
        traceback.print_exc()
        
        # Fallback –¥–æ —Å—Ç–∞—Ä–æ—ó —Å–∏—Å—Ç–µ–º–∏
        try:
            print(f"üîÑ Falling back to standard LLM system...")
            combined, llm_iterations = fix_diarization_errors_with_llm(combined)
            _llm_iterations_cache = llm_iterations
        except Exception as e2:
            print(f"‚ö†Ô∏è  Error in fallback LLM system: {e2}")
            _llm_iterations_cache = []
        
        # –Ø–∫—â–æ –≤—Å–µ –Ω–µ –≤–¥–∞–ª–æ—Å—è, –Ω–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ –ø–æ—Ä—è–¥–æ–∫
        combined = normalize_speaker_order(combined)
    
    return combined


def detect_and_merge_fragmented_phrases(segments, max_gap=2.0):
    """
    –í–∏—è–≤–ª—è—î —Ç–∞ –æ–±'—î–¥–Ω—É—î —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤–∞–Ω—ñ —Ñ—Ä–∞–∑–∏, —è–∫—ñ –±—É–ª–∏ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ —Ä–æ–∑–¥—ñ–ª–µ–Ω—ñ –¥—ñ–∞—Ä–∏–∑–∞—Ç–æ—Ä–æ–º.
    
    –®–∞–±–ª–æ–Ω –≤–∏—è–≤–ª–µ–Ω–Ω—è:
    - –î–≤—ñ —Ä–µ–ø–ª—ñ–∫–∏ —Å—Ç–æ—è—Ç—å –ø–æ—Ä—É—á (–º–∞–ª–∞ –ø–∞—É–∑–∞ < max_gap)
    - –ü–µ—Ä—à–∞ –ø–æ—á–∏–Ω–∞—î—Ç—å—Å—è –∑ –≤–µ–ª–∏–∫–æ—ó –ª—ñ—Ç–µ—Ä–∏ (–ø–æ—á–∞—Ç–æ–∫ —Ä–µ—á–µ–Ω–Ω—è)
    - –î—Ä—É–≥–∞ –∑–∞–∫—ñ–Ω—á—É—î—Ç—å—Å—è –Ω–∞ –∑–Ω–∞–∫ –ø–∏—Ç–∞–Ω–Ω—è (–∫—ñ–Ω–µ—Ü—å —Ä–µ—á–µ–Ω–Ω—è)
    - –†—ñ–∑–Ω—ñ —Å–ø—ñ–∫–µ—Ä–∏ (–ø–æ–º–∏–ª–∫–∞ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó)
    
    –¢–∞–∫—ñ —Å–µ–≥–º–µ–Ω—Ç–∏ –æ–±'—î–¥–Ω—É—é—Ç—å—Å—è —ñ –ø–æ–∑–Ω–∞—á–∞—é—Ç—å—Å—è –¥–ª—è –µ—Å–∫–∞–ª–∞—Ü—ñ—ó –¥–æ smart model.
    
    Args:
        segments: —Å–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ [{'speaker': int, 'start': float, 'end': float, 'text': str}]
        max_gap: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∏–π –ø—Ä–æ–º—ñ–∂–æ–∫ –º—ñ–∂ —Å–µ–≥–º–µ–Ω—Ç–∞–º–∏ –¥–ª—è –æ–±'—î–¥–Ω–∞–Ω–Ω—è (—Å–µ–∫—É–Ω–¥–∏)
    
    Returns:
        merged_segments: —Å–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ –∑ –æ–±'—î–¥–Ω–∞–Ω–∏–º–∏ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤–∞–Ω–∏–º–∏ —Ñ—Ä–∞–∑–∞–º–∏
    """
    if not segments or len(segments) < 2:
        return segments
    
    print(f"üîç Detecting fragmented phrases in {len(segments)} segments...")
    merged = []
    i = 0
    fragmented_count = 0
    
    while i < len(segments):
        if i == len(segments) - 1:
            # –û—Å—Ç–∞–Ω–Ω—ñ–π —Å–µ–≥–º–µ–Ω—Ç - –ø—Ä–æ—Å—Ç–æ –¥–æ–¥–∞—î–º–æ
            merged.append(segments[i])
            i += 1
            continue
        
        current = segments[i]
        next_seg = segments[i + 1]
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —É–º–æ–≤–∏ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤–∞–Ω–æ—ó —Ñ—Ä–∞–∑–∏
        current_text = current['text'].strip()
        next_text = next_seg['text'].strip()
        
        # –£–º–æ–≤–∞ 1: –ú–∞–ª–∞ –ø–∞—É–∑–∞ –º—ñ–∂ —Å–µ–≥–º–µ–Ω—Ç–∞–º–∏
        pause = next_seg['start'] - current['end']
        short_pause = pause >= 0 and pause < max_gap
        
        # –£–º–æ–≤–∞ 2: –ü–µ—Ä—à–∞ –ø–æ—á–∏–Ω–∞—î—Ç—å—Å—è –∑ –≤–µ–ª–∏–∫–æ—ó –ª—ñ—Ç–µ—Ä–∏ (–ø–æ—á–∞—Ç–æ–∫ —Ä–µ—á–µ–Ω–Ω—è)
        starts_with_capital = len(current_text) > 0 and current_text[0].isupper()
        
        # –£–º–æ–≤–∞ 3: –î—Ä—É–≥–∞ –∑–∞–∫—ñ–Ω—á—É—î—Ç—å—Å—è –Ω–∞ –∑–Ω–∞–∫ –ø–∏—Ç–∞–Ω–Ω—è
        ends_with_question = next_text.endswith('?')
        
        # –£–º–æ–≤–∞ 4: –†—ñ–∑–Ω—ñ —Å–ø—ñ–∫–µ—Ä–∏ (–ø–æ–º–∏–ª–∫–∞ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó)
        different_speakers = current['speaker'] != next_seg['speaker']
        
        # –£–º–æ–≤–∞ 5: –ü–µ—Ä—à–∞ –Ω–µ –∑–∞–∫—ñ–Ω—á—É—î—Ç—å—Å—è –Ω–∞ –∑–Ω–∞–∫ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è —Ä–µ—á–µ–Ω–Ω—è
        current_ends_properly = current_text.endswith(('.', '!', '?'))
        
        # –î–æ–¥–∞—Ç–∫–æ–≤–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞: –ø–µ—Ä—à–∞ —Ñ—Ä–∞–∑–∞ –≤–∏–≥–ª—è–¥–∞—î —è–∫ –ø–æ—á–∞—Ç–æ–∫ —Ä–µ—á–µ–Ω–Ω—è
        # (–Ω–µ –∑–∞–∫—ñ–Ω—á—É—î—Ç—å—Å—è –Ω–∞ –∑–Ω–∞–∫ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è, –∞–±–æ –∑–∞–∫—ñ–Ω—á—É—î—Ç—å—Å—è –Ω–∞ –∫–æ–º—É/—Ç–∏—Ä–µ)
        looks_like_start = not current_ends_properly or current_text.endswith((',', '-', '‚Äî', '‚Äì'))
        
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ –≥—Ä–∞–º–∞—Ç–∏—á–Ω—É –∑–≤'—è–∑–Ω—ñ—Å—Ç—å
        # –Ø–∫—â–æ —Ä–∞–∑–æ–º –≤–æ–Ω–∏ —É—Ç–≤–æ—Ä—é—é—Ç—å –≥—Ä–∞–º–∞—Ç–∏—á–Ω–æ –ø—Ä–∞–≤–∏–ª—å–Ω–µ —Ä–µ—á–µ–Ω–Ω—è
        combined_text = (current_text + ' ' + next_text).strip()
        is_grammatically_connected = (
            len(combined_text.split()) < 30 and  # –ù–µ –¥—É–∂–µ –¥–æ–≤–≥–µ
            '  ' not in combined_text and  # –ù–µ–º–∞—î –ø–æ–¥–≤—ñ–π–Ω–∏—Ö –ø—Ä–æ–±—ñ–ª—ñ–≤
            not combined_text.startswith(next_text.split()[0] if next_text.split() else '')  # –ù–µ –¥—É–±–ª—ñ–∫–∞—Ç
        )
        
        if (short_pause and 
            starts_with_capital and 
            ends_with_question and 
            different_speakers and 
            looks_like_start and
            is_grammatically_connected):
            
            # –¶–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤–∞–Ω–∞ —Ñ—Ä–∞–∑–∞ - –æ–±'—î–¥–Ω—É—î–º–æ
            fragmented_count += 1
            print(f"  üîó Detected fragmented phrase:")
            print(f"     Segment 1: Speaker {current['speaker']+1} [{current['start']:.2f}s-{current['end']:.2f}s]: \"{current_text}\"")
            print(f"     Segment 2: Speaker {next_seg['speaker']+1} [{next_seg['start']:.2f}s-{next_seg['end']:.2f}s]: \"{next_text}\"")
            
            # –û–±'—î–¥–Ω—É—î–º–æ —Å–µ–≥–º–µ–Ω—Ç–∏
            merged_seg = {
                'speaker': current['speaker'],  # –¢–∏–º—á–∞—Å–æ–≤–æ –∑–∞–ª–∏—à–∞—î–º–æ —Å–ø—ñ–∫–µ—Ä–∞ –ø–µ—Ä—à–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞
                'start': current['start'],
                'end': next_seg['end'],
                'text': combined_text,
                'needs_speaker_verification': True,  # –ü–æ–∑–Ω–∞—á–∞—î–º–æ –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ —Å–ø—ñ–∫–µ—Ä–∞
                'original_speakers': [current['speaker'], next_seg['speaker']],  # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏—Ö —Å–ø—ñ–∫–µ—Ä—ñ–≤
                'fragmented_merge': True  # –ü–æ–∑–Ω–∞—á–∞—î–º–æ —è–∫ –æ–±'—î–¥–Ω–∞–Ω–Ω—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ñ–≤
            }
            merged.append(merged_seg)
            print(f"     ‚úÖ Merged: \"{combined_text}\" (needs speaker verification)")
            i += 2  # –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –æ–±–∏–¥–≤–∞ —Å–µ–≥–º–µ–Ω—Ç–∏
            continue
        
        # –Ø–∫—â–æ –Ω–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤–∞–Ω–∞ —Ñ—Ä–∞–∑–∞ - –ø—Ä–æ—Å—Ç–æ –¥–æ–¥–∞—î–º–æ –ø–æ—Ç–æ—á–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç
        merged.append(current)
        i += 1
    
    if fragmented_count > 0:
        print(f"‚úÖ Detected and merged {fragmented_count} fragmented phrase(s)")
    else:
        print(f"‚úÖ No fragmented phrases detected")
    
    return merged


def merge_consecutive_speaker_segments(segments, max_gap=2.0):
    """
    –û–±'—î–¥–Ω—É—î —Å—É—Å—ñ–¥–Ω—ñ —Å–µ–≥–º–µ–Ω—Ç–∏ –æ–¥–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ –¥–ª—è –∑–º–µ–Ω—à–µ–Ω–Ω—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü—ñ—ó.
    Args:
        segments: —Å–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ [{'speaker': int, 'start': float, 'end': float, 'text': str}]
        max_gap: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∏–π –ø—Ä–æ–º—ñ–∂–æ–∫ –º—ñ–∂ —Å–µ–≥–º–µ–Ω—Ç–∞–º–∏ –¥–ª—è –æ–±'—î–¥–Ω–∞–Ω–Ω—è (—Å–µ–∫—É–Ω–¥–∏)
    Returns:
        merged_segments: –æ–±'—î–¥–Ω–∞–Ω–∏–π —Å–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç—ñ–≤
    """
    if not segments or len(segments) < 2:
        return segments
    print(f"üîó Merging consecutive segments from same speaker (max_gap={max_gap}s)...")
    merged = []
    current_seg = None
    for seg in segments:
        if current_seg is None:
            current_seg = seg.copy()
            continue
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ –º–æ–∂–Ω–∞ –æ–±'—î–¥–Ω–∞—Ç–∏
        gap = seg['start'] - current_seg['end']
        same_speaker = current_seg['speaker'] == seg['speaker']
        has_overlap = seg['start'] < current_seg['end']
        # –û–±'—î–¥–Ω—É—î–º–æ, —è–∫—â–æ:
        # 1. –û–¥–∏–Ω —ñ —Ç–æ–π –∂–µ —Å–ø—ñ–∫–µ—Ä
        # 2. –ü–µ—Ä–µ–∫—Ä–∏–≤–∞—é—Ç—å—Å—è –∞–±–æ –º–∞—é—Ç—å –Ω–µ–≤–µ–ª–∏–∫–∏–π –ø—Ä–æ–º—ñ–∂–æ–∫ (< max_gap)
        if same_speaker and (has_overlap or gap <= max_gap):
            # –û–±'—î–¥–Ω—É—î–º–æ —Å–µ–≥–º–µ–Ω—Ç–∏
            current_seg['end'] = max(current_seg['end'], seg['end'])
            # –û–±'—î–¥–Ω—É—î–º–æ —Ç–µ–∫—Å—Ç
            current_seg['text'] = (current_seg['text'] + ' ' + seg['text']).strip()
            print(f"  üîó Merged: Speaker {current_seg['speaker']} [{current_seg['start']:.2f}s-{seg['end']:.2f}s]")
        else:
            # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –ø–æ—Ç–æ—á–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç —ñ –ø–æ—á–∏–Ω–∞—î–º–æ –Ω–æ–≤–∏–π
            merged.append(current_seg)
            current_seg = seg.copy()
    # –î–æ–¥–∞—î–º–æ –æ—Å—Ç–∞–Ω–Ω—ñ–π —Å–µ–≥–º–µ–Ω—Ç
    if current_seg:
        merged.append(current_seg)
    if len(merged) < len(segments):
        print(f"‚úÖ Merged: {len(segments)} ‚Üí {len(merged)} segments")
    else:
        print(f"‚úÖ No merging needed: {len(segments)} segments")
    return merged


def _llm_request(lm_studio_url, model, system_prompt, user_prompt, max_tokens=500):
    """–î–æ–ø–æ–º—ñ–∂–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –¥–ª—è –≤—ñ–¥–ø—Ä–∞–≤–∫–∏ –∑–∞–ø–∏—Ç—É –¥–æ LLM"""
    try:
        payload = {
            "model": model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            "temperature": 0,
            "max_tokens": max_tokens
        }
        response = requests.post(
            lm_studio_url,
            json=payload,
            headers={"Content-Type": "application/json"},
            timeout=30  # –ö–æ—Ä–æ—Ç–∫–∏–π timeout –¥–ª—è –º—ñ–∫—Ä–æ–ø—Ä–æ–º–ø—Ç—ñ–≤
        )
        if response.status_code != 200:
            return None
        response_data = response.json()
        content = response_data.get("choices", [{}])[0].get("message", {}).get("content", "")
        return content.strip()
    except Exception as e:
        print(f"‚ö†Ô∏è  LLM request error: {e}")
        return None


def analyze_dialogue_zones(segments):
    """
    –ê–Ω–∞–ª—ñ–∑—É—î –¥—ñ–∞–ª–æ–≥ —ñ –≤–∏—è–≤–ª—è—î —Ä—ñ–∑–Ω—ñ —Ç–∏–ø–∏ –∑–æ–Ω:
    - overlaps: –ø–µ—Ä–µ—Ç–∏–Ω–∏ —Ä–µ–ø–ª—ñ–∫ (–∫–æ—Ä–æ—Ç–∫—ñ –ø–∞—É–∑–∏ –º—ñ–∂ —Ä—ñ–∑–Ω–∏–º–∏ —Å–ø—ñ–∫–µ—Ä–∞–º–∏)
    - clean_speech: —á–∏—Å—Ç–∞ –º–æ–≤–∞ –æ–¥–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ (–¥–æ–≤–≥—ñ —Å–µ–≥–º–µ–Ω—Ç–∏ –±–µ–∑ –ø–µ—Ä–µ—Ä–∏–≤–∞–Ω—å)
    - pauses: –ø–∞—É–∑–∏ —Ç–∞ —Ç–∏—à–∞ (–≤–µ–ª–∏–∫—ñ –ø—Ä–æ–º—ñ–∂–∫–∏ –º—ñ–∂ —Å–µ–≥–º–µ–Ω—Ç–∞–º–∏)
    - short_segments: –¥—É–∂–µ –∫–æ—Ä–æ—Ç–∫—ñ —Å–µ–≥–º–µ–Ω—Ç–∏ (–º–æ–∂–ª–∏–≤—ñ —à—É–º–∏ –∞–±–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∏)
    
    Returns:
        dict: {
            'overlaps': [{'start_idx': int, 'end_idx': int, 'segments': [...]}],
            'clean_speech': [...],
            'pauses': [...],
            'short_segments': [...]
        }
    """
    if not segments or len(segments) < 2:
        return {'overlaps': [], 'clean_speech': [], 'pauses': [], 'short_segments': []}
    
    overlaps = []
    clean_speech = []
    pauses = []
    short_segments = []
    
    # –í–∏—è–≤–ª—è—î–º–æ –ø–µ—Ä–µ—Ç–∏–Ω–∏ —Ä–µ–ø–ª—ñ–∫ (–∫–æ—Ä–æ—Ç–∫—ñ –ø–∞—É–∑–∏ –º—ñ–∂ —Ä—ñ–∑–Ω–∏–º–∏ —Å–ø—ñ–∫–µ—Ä–∞–º–∏)
    for i in range(len(segments) - 1):
        current = segments[i]
        next_seg = segments[i + 1]
        
        pause = next_seg['start'] - current['end']
        duration_current = current['end'] - current['start']
        duration_next = next_seg['end'] - next_seg['start']
        
        # –ü–µ—Ä–µ—Ç–∏–Ω: —Ä—ñ–∑–Ω—ñ —Å–ø—ñ–∫–µ—Ä–∏ + –∫–æ—Ä–æ—Ç–∫–∞ –ø–∞—É–∑–∞ (< 1.5 —Å–µ–∫) + –∫–æ—Ä–æ—Ç–∫—ñ —Å–µ–≥–º–µ–Ω—Ç–∏
        if current['speaker'] != next_seg['speaker'] and pause < 1.5 and pause >= 0:
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —Ü–µ –Ω–µ –ø–æ—á–∞—Ç–æ–∫ –Ω–æ–≤–æ–≥–æ –ø–µ—Ä–µ—Ç–∏–Ω—É
            if not overlaps or overlaps[-1]['end_idx'] < i:
                overlaps.append({
                    'start_idx': i,
                    'end_idx': i + 1,
                    'segments': [current, next_seg],
                    'pause': pause
                })
            else:
                # –†–æ–∑—à–∏—Ä—é—î–º–æ –ø–æ—Ç–æ—á–Ω–∏–π –ø–µ—Ä–µ—Ç–∏–Ω
                overlaps[-1]['end_idx'] = i + 1
                overlaps[-1]['segments'].append(next_seg)
        
        # –ü–∞—É–∑–∞: –≤–µ–ª–∏–∫–∏–π –ø—Ä–æ–º—ñ–∂–æ–∫ (> 2 —Å–µ–∫)
        if pause > 2.0:
            pauses.append({
                'start_idx': i,
                'end_idx': i + 1,
                'pause_duration': pause,
                'segments': [current, next_seg]
            })
        
        # –ö–æ—Ä–æ—Ç–∫—ñ —Å–µ–≥–º–µ–Ω—Ç–∏ (–º–æ–∂–ª–∏–≤—ñ —à—É–º–∏ –∞–±–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∏)
        if duration_current < 0.5 or duration_next < 0.5:
            if duration_current < 0.5:
                short_segments.append({
                    'idx': i,
                    'segment': current,
                    'duration': duration_current
                })
            if duration_next < 0.5 and i == len(segments) - 2:
                short_segments.append({
                    'idx': i + 1,
                    'segment': next_seg,
                    'duration': duration_next
                })
    
    # –í–∏—è–≤–ª—è—î–º–æ —á–∏—Å—Ç—É –º–æ–≤—É (–¥–æ–≤–≥—ñ —Å–µ–≥–º–µ–Ω—Ç–∏ –æ–¥–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ –±–µ–∑ –ø–µ—Ä–µ—Ä–∏–≤–∞–Ω—å)
    i = 0
    while i < len(segments):
        current = segments[i]
        duration = current['end'] - current['start']
        
        # –ß–∏—Å—Ç–∞ –º–æ–≤–∞: –¥–æ–≤–≥–∏–π —Å–µ–≥–º–µ–Ω—Ç (> 3 —Å–µ–∫) –∞–±–æ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω—ñ —Å–µ–≥–º–µ–Ω—Ç–∏ –æ–¥–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
        if duration > 3.0:
            clean_speech.append({
                'start_idx': i,
                'end_idx': i,
                'segments': [current],
                'speaker': current['speaker']
            })
        else:
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω—ñ —Å–µ–≥–º–µ–Ω—Ç–∏ –æ–¥–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
            clean_zone = [current]
            j = i + 1
            while j < len(segments):
                next_seg = segments[j]
                pause = next_seg['start'] - segments[j-1]['end']
                if next_seg['speaker'] == current['speaker'] and pause < 1.0:
                    clean_zone.append(next_seg)
                    j += 1
                else:
                    break
            
            if len(clean_zone) > 1:
                total_duration = clean_zone[-1]['end'] - clean_zone[0]['start']
                if total_duration > 2.0:
                    clean_speech.append({
                        'start_idx': i,
                        'end_idx': j - 1,
                        'segments': clean_zone,
                        'speaker': current['speaker']
                    })
                    i = j - 1
        
        i += 1
    
    return {
        'overlaps': overlaps,
        'clean_speech': clean_speech,
        'pauses': pauses,
        'short_segments': short_segments
    }


def assess_llm_confidence(response, expected_format="json", min_items=1):
    """
    –û—Ü—ñ–Ω—é—î –≤–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—å LLM —É —Å–≤–æ—ó–π –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ.
    
    Args:
        response: –≤—ñ–¥–ø–æ–≤—ñ–¥—å –≤—ñ–¥ LLM
        expected_format: –æ—á—ñ–∫—É–≤–∞–Ω–∏–π —Ñ–æ—Ä–º–∞—Ç ("json")
        min_items: –º—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –µ–ª–µ–º–µ–Ω—Ç—ñ–≤ –¥–ª—è –≤–ø–µ–≤–Ω–µ–Ω–æ—ó –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ
    
    Returns:
        tuple: (confidence_score: float 0-1, is_confident: bool, reason: str)
    """
    if not response:
        return 0.0, False, "No response"
    
    confidence = 0.5  # –ë–∞–∑–æ–≤–∏–π —Ä—ñ–≤–µ–Ω—å
    reasons = []
    
    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ 1: –ß–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ —Å—Ñ–æ—Ä–º–æ–≤–∞–Ω–∏–π JSON
    try:
        import json
        import re
        json_match = re.search(r'\[.*?\]', response, re.DOTALL)
        if json_match:
            data = json.loads(json_match.group())
            if isinstance(data, list):
                confidence += 0.3
                reasons.append("Valid JSON array")
                
                # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ 2: –ö—ñ–ª—å–∫—ñ—Å—Ç—å –µ–ª–µ–º–µ–Ω—Ç—ñ–≤
                if len(data) >= min_items:
                    confidence += 0.1
                    reasons.append(f"Has {len(data)} items")
                elif len(data) == 0:
                    confidence -= 0.2
                    reasons.append("Empty array (might be uncertain)")
        else:
            confidence -= 0.3
            reasons.append("No valid JSON found")
    except Exception as e:
        confidence -= 0.4
        reasons.append(f"JSON parse error: {str(e)[:30]}")
    
    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ 3: –ù–∞—è–≤–Ω—ñ—Å—Ç—å –º–∞—Ä–∫–µ—Ä—ñ–≤ –Ω–µ–≤–ø–µ–≤–Ω–µ–Ω–æ—Å—Ç—ñ
    uncertainty_markers = ["uncertain", "not sure", "maybe", "possibly", "might", "could be", "not confident"]
    response_lower = response.lower()
    for marker in uncertainty_markers:
        if marker in response_lower:
            confidence -= 0.2
            reasons.append(f"Uncertainty marker: {marker}")
            break
    
    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ 4: –ù–∞—è–≤–Ω—ñ—Å—Ç—å –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤ –ø—Ä–æ —Å–∫–ª–∞–¥–Ω—ñ—Å—Ç—å
    complexity_markers = ["difficult", "complex", "hard to determine", "ambiguous"]
    for marker in complexity_markers:
        if marker in response_lower:
            confidence -= 0.15
            reasons.append(f"Complexity marker: {marker}")
            break
    
    confidence = max(0.0, min(1.0, confidence))  # –û–±–º–µ–∂—É—î–º–æ 0-1
    is_confident = confidence >= 0.6  # –ü–æ—Ä—ñ–≥ –≤–ø–µ–≤–Ω–µ–Ω–æ—Å—Ç—ñ
    
    return confidence, is_confident, "; ".join(reasons) if reasons else "Basic assessment"


def fix_diarization_errors_with_llm(combined_segments, lm_studio_url="http://127.0.0.1:3001/v1/chat/completions", model="google/gemma-3-1b", escalation_model="openai/gpt-oss-20b"):
    """
    –í–∏–ø—Ä–∞–≤–ª—è—î –ø–æ–º–∏–ª–∫–∏ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é LLM (LM Studio) —á–µ—Ä–µ–∑ –ª–∞–Ω—Ü—é–∂–æ–∫ –º—ñ–∫—Ä–æ–ø—Ä–æ–º–ø—Ç—ñ–≤.
    –ù–û–í–ò–ô –ü–Ü–î–•–Ü–î: –î–∞—î–º–æ –ø–æ–≤–Ω–∏–π –¥—ñ–∞–ª–æ–≥ —è–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç, –∞–ª–µ –∑ –ø—Ä–æ—Å—Ç–∏–º–∏ —Ñ–æ–∫—É—Å–æ–≤–∞–Ω–∏–º–∏ —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—è–º–∏.
    –û–±—Ä–æ–±–ª—è—î–º–æ —Ç–æ–π —Å–∞–º–∏–π –¥—ñ–∞–ª–æ–≥ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ –∑ —Ä—ñ–∑–Ω–∏–º–∏ —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—è–º–∏.
    Args:
        combined_segments: —Å–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ [{'speaker': int, 'start': float, 'end': float, 'text': str}]
        lm_studio_url: URL –¥–ª—è LM Studio API
        model: –Ω–∞–∑–≤–∞ –º–æ–¥–µ–ª—ñ
    Returns:
        tuple: (fixed_segments, llm_iterations_info)
        - fixed_segments: –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–∏–π —Å–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç—ñ–≤
        - llm_iterations_info: —Å–ø–∏—Å–æ–∫ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –ø—Ä–æ —ñ—Ç–µ—Ä–∞—Ü—ñ—ó [{'iteration': int, 'total': int, 'result': str}]
    """
    try:
        if not combined_segments or len(combined_segments) < 2:
            print("‚ö†Ô∏è  LLM: Not enough segments to process")
            return combined_segments, []
        print(f"ü§ñ Fixing diarization errors with LLM ({model}) using full dialogue context with micro-instructions...")
        # –§–æ—Ä–º—É—î–º–æ –ø–æ–≤–Ω–∏–π –¥—ñ–∞–ª–æ–≥ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç—É
        full_dialogue = ""
        for idx, seg in enumerate(combined_segments):
            full_dialogue += f"{idx+1}. Speaker {seg['speaker']+1} [{seg['start']:.2f}s-{seg['end']:.2f}s]: \"{seg['text']}\"\n"
        llm_iterations = []  # –Ü–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ —ñ—Ç–µ—Ä–∞—Ü—ñ—ó –¥–ª—è –¥–µ–±–∞–≥ –∫–æ–Ω—Å–æ–ª—ñ
        # –Ü–¢–ï–†–ê–¶–Ü–Ø 1: –í–∏—è–≤–ª–µ–Ω–Ω—è —Å–µ–≥–º–µ–Ω—Ç—ñ–≤, —è–∫—ñ –Ω–∞–ª–µ–∂–∞—Ç—å –æ–¥–Ω–æ–º—É —Å–ø—ñ–∫–µ—Ä—É (–æ–±'—î–¥–Ω–∞–Ω–Ω—è)
        print("üìã LLM Iteration 1: Identifying segments that belong to the same speaker...")
        current_iteration = 1
        total_iterations = 3  # 3 —ñ—Ç–µ—Ä–∞—Ü—ñ—ó –∑ —Ä—ñ–∑–Ω–∏–º–∏ —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—è–º–∏
        system_prompt_1 = "You are a helpful assistant. Analyze the dialogue and identify which consecutive segments belong to the same speaker."
        user_prompt_1 = f"""FULL DIALOGUE:
{full_dialogue}

INSTRUCTION: Look at consecutive segments where the speaker changes. For each pair of consecutive segments with DIFFERENT speakers, determine if they actually belong to the SAME speaker (one person's speech was incorrectly split).

Return ONLY a JSON array of pairs that should be merged. Format:
[
  {{"segment1": 1, "segment2": 2, "should_merge": true}},
  {{"segment1": 5, "segment2": 6, "should_merge": false}},
  ...
]

If no merges needed, return empty array: []"""
        # –°–ø–æ—á–∞—Ç–∫—É –ø—Ä–æ–±—É—î–º–æ —Å–ª–∞–±–∫—É –º–æ–¥–µ–ª—å
        response_1 = _llm_request(lm_studio_url, model, system_prompt_1, user_prompt_1, max_tokens=500)
        confidence_1, is_confident_1, reason_1 = assess_llm_confidence(response_1, min_items=0)
        
        result_text_1 = f"Iteration 1 (Merge detection): {response_1[:200] if response_1 else 'NO RESPONSE'}..."
        print(f"  [{current_iteration}/{total_iterations}] {result_text_1}")
        print(f"  üìä Confidence: {confidence_1:.2f} ({'‚úì Confident' if is_confident_1 else '‚úó Uncertain'}) - {reason_1}")
        
        # –ï—Å–∫–∞–ª–∞—Ü—ñ—è –¥–ª—è merge detection (—è–∫—â–æ –¥—É–∂–µ –Ω–∏–∑—å–∫–∞ –≤–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—å)
        if not is_confident_1 and escalation_model and confidence_1 < 0.5:
            print(f"  ‚¨ÜÔ∏è  Escalating to {escalation_model} for merge detection...")
            response_1_escalated = _llm_request(lm_studio_url, escalation_model, system_prompt_1, user_prompt_1, max_tokens=800)
            confidence_1_escalated, _, _ = assess_llm_confidence(response_1_escalated, min_items=0)
            
            if confidence_1_escalated > confidence_1:
                response_1 = response_1_escalated
                result_text_1 = f"Iteration 1 (Merge detection) [ESCALATED to {escalation_model}]: {response_1[:200] if response_1 else 'NO RESPONSE'}..."
                print(f"  ‚úÖ Escalation improved confidence: {confidence_1:.2f} ‚Üí {confidence_1_escalated:.2f}")
        
        llm_iterations.append({
            'iteration': current_iteration,
            'total': total_iterations,
            'result': f"{result_text_1} [Confidence: {confidence_1:.2f}]"
        })
        # –ü–∞—Ä—Å–∏–º–æ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –ø—Ä–æ –æ–±'—î–¥–Ω–∞–Ω–Ω—è
        merge_decisions = []
        try:
            import json
            import re
            # –°–ø—Ä–æ–±—É—î–º–æ –≤–∏—Ç—è–≥—Ç–∏ JSON –∑ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ
            json_match = re.search(r'\[.*?\]', response_1, re.DOTALL)
            if json_match:
                merge_data = json.loads(json_match.group())
                for item in merge_data:
                    if item.get('should_merge', False):
                        seg1_idx = item.get('segment1', 0) - 1  # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ –∑ 1-based –≤ 0-based
                        seg2_idx = item.get('segment2', 0) - 1
                        if 0 <= seg1_idx < len(combined_segments) and 0 <= seg2_idx < len(combined_segments):
                            merge_decisions.append((seg1_idx, seg2_idx))
        except Exception as e:
            print(f"  ‚ö†Ô∏è  Could not parse merge decisions: {e}")
        # –Ü–¢–ï–†–ê–¶–Ü–Ø 2: –í–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è –ø–æ–º–∏–ª–æ–∫ —É –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—ñ —Å–ø—ñ–∫–µ—Ä—ñ–≤ (–ø–∏—Ç–∞–Ω–Ω—è-–≤—ñ–¥–ø–æ–≤—ñ–¥—ñ)
        print("üìã LLM Iteration 2: Fixing speaker assignment errors (question-answer patterns)...")
        current_iteration = 2
        system_prompt_2 = "You are a helpful assistant. Analyze the dialogue and fix speaker assignment errors based on question-answer patterns and role relevance. DO NOT blindly alternate speakers."
        user_prompt_2 = f"""FULL DIALOGUE:
{full_dialogue}

INSTRUCTION: Identify and fix speaker assignment errors ONLY when there is clear evidence of role mismatch:
- If a segment contains a QUESTION and the next segment contains an ANSWER, they should be from DIFFERENT speakers (ONLY if this is a clear question-answer pattern)
- If a segment contains an ANSWER and the previous segment contains a QUESTION, they should be from DIFFERENT speakers (ONLY if this is a clear question-answer pattern)
- DO NOT blindly alternate speakers - preserve the diarization result unless there is clear evidence of error
- Only correct when there is a clear role mismatch (e.g., Agent asking for help, Client offering solutions)

CRITICAL: Preserve the diarization result unless there is strong evidence of error. Do not force alternation.

Return ONLY a JSON array of corrections. Format:
[
  {{"segment": 2, "correct_speaker": 1, "reason": "clear question-answer pattern"}},
  {{"segment": 5, "correct_speaker": 2, "reason": "role mismatch detected"}},
  ...
]

If no corrections needed, return empty array: []"""
        response_2 = _llm_request(lm_studio_url, model, system_prompt_2, user_prompt_2, max_tokens=500)
        result_text_2 = f"Iteration 2 (Speaker correction): {response_2[:200] if response_2 else 'NO RESPONSE'}..."
        llm_iterations.append({
            'iteration': current_iteration,
            'total': total_iterations,
            'result': result_text_2
        })
        print(f"  [{current_iteration}/{total_iterations}] {result_text_2}")
        # –ü–∞—Ä—Å–∏–º–æ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –ø—Ä–æ –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è —Å–ø—ñ–∫–µ—Ä—ñ–≤
        speaker_corrections = {}
        try:
            json_match = re.search(r'\[.*?\]', response_2, re.DOTALL)
            if json_match:
                corrections_data = json.loads(json_match.group())
                for item in corrections_data:
                    seg_idx = item.get('segment', 0) - 1  # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ –∑ 1-based –≤ 0-based
                    correct_speaker = item.get('correct_speaker', 0) - 1  # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ –∑ 1-based –≤ 0-based
                    if 0 <= seg_idx < len(combined_segments):
                        speaker_corrections[seg_idx] = correct_speaker
        except Exception as e:
            print(f"  ‚ö†Ô∏è  Could not parse speaker corrections: {e}")
        
        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î–º–æ overlap_corrections —è–∫ –ø–æ—Ä–æ–∂–Ω—ñ–π —Å–ª–æ–≤–Ω–∏–∫ (—è–∫—â–æ –Ω–µ –±—É–ª–æ –æ–±—Ä–æ–±–∫–∏ overlap zones)
        overlap_corrections = {}
        
        # –Ü–¢–ï–†–ê–¶–Ü–Ø 4: –§—ñ–Ω–∞–ª—å–Ω–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ç–∞ –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è
        print("üìã LLM Iteration 4: Final validation and normalization...")
        current_iteration = 4
        system_prompt_3 = "You are a helpful assistant. Validate the dialogue structure. DO NOT blindly alternate speakers - only fix clear errors."
        user_prompt_3 = f"""FULL DIALOGUE:
{full_dialogue}

INSTRUCTION: Validate the dialogue structure and identify ONLY clear errors:
- Check for obvious role mismatches (e.g., Agent describing problems, Client offering solutions)
- Check for clear question-answer patterns where speakers are incorrectly assigned
- DO NOT force alternation - preserve diarization result unless there is clear evidence of error
- DO NOT change speakers just because they don't alternate - only change when role is clearly wrong

CRITICAL: Preserve the diarization result. Only correct when there is strong evidence of role mismatch.

Return ONLY a JSON array of final corrections. Format:
[
  {{"segment": 1, "correct_speaker": 1, "reason": "clear role mismatch"}},
  ...
]

If no corrections needed, return empty array: []"""
        # –°–ø–æ—á–∞—Ç–∫—É –ø—Ä–æ–±—É—î–º–æ —Å–ª–∞–±–∫—É –º–æ–¥–µ–ª—å
        response_3 = _llm_request(lm_studio_url, model, system_prompt_3, user_prompt_3, max_tokens=500)
        confidence_3, is_confident_3, reason_3 = assess_llm_confidence(response_3, min_items=0)
        
        result_text_3 = f"Iteration 4 (Final validation): {response_3[:200] if response_3 else 'NO RESPONSE'}..."
        print(f"  [{current_iteration}/{total_iterations}] {result_text_3}")
        print(f"  üìä Confidence: {confidence_3:.2f} ({'‚úì Confident' if is_confident_3 else '‚úó Uncertain'}) - {reason_3}")
        
        # –ï—Å–∫–∞–ª–∞—Ü—ñ—è –¥–ª—è —Ñ—ñ–Ω–∞–ª—å–Ω–æ—ó –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó (—è–∫—â–æ –¥—É–∂–µ –Ω–∏–∑—å–∫–∞ –≤–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—å)
        if not is_confident_3 and escalation_model and confidence_3 < 0.5:
            print(f"  ‚¨ÜÔ∏è  Escalating to {escalation_model} for final validation...")
            response_3_escalated = _llm_request(lm_studio_url, escalation_model, system_prompt_3, user_prompt_3, max_tokens=800)
            confidence_3_escalated, _, _ = assess_llm_confidence(response_3_escalated, min_items=0)
            
            if confidence_3_escalated > confidence_3:
                response_3 = response_3_escalated
                result_text_3 = f"Iteration 4 (Final validation) [ESCALATED to {escalation_model}]: {response_3[:200] if response_3 else 'NO RESPONSE'}..."
                print(f"  ‚úÖ Escalation improved confidence: {confidence_3:.2f} ‚Üí {confidence_3_escalated:.2f}")
        
        llm_iterations.append({
            'iteration': current_iteration,
            'total': total_iterations,
            'result': f"{result_text_3} [Confidence: {confidence_3:.2f}]"
        })
        # –ü–∞—Ä—Å–∏–º–æ —Ñ—ñ–Ω–∞–ª—å–Ω—ñ –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è
        final_corrections = {}
        try:
            json_match = re.search(r'\[.*?\]', response_3, re.DOTALL)
            if json_match:
                final_data = json.loads(json_match.group())
                for item in final_data:
                    seg_idx = item.get('segment', 0) - 1
                    correct_speaker = item.get('correct_speaker', 0) - 1
                    if 0 <= seg_idx < len(combined_segments):
                        final_corrections[seg_idx] = correct_speaker
        except Exception as e:
            print(f"  ‚ö†Ô∏è  Could not parse final corrections: {e}")
        # –ó–∞—Å—Ç–æ—Å–æ–≤—É—î–º–æ –≤—Å—ñ –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è
        fixed_segments = [seg.copy() for seg in combined_segments]
        # –ö—Ä–æ–∫ 1: –û–±'—î–¥–Ω—É—î–º–æ —Å–µ–≥–º–µ–Ω—Ç–∏
        if merge_decisions:
            print(f"  üîó Applying {len(merge_decisions)} merge decisions...")
            # –°—Ç–≤–æ—Ä—é—î–º–æ –º–Ω–æ–∂–∏–Ω—É —ñ–Ω–¥–µ–∫—Å—ñ–≤, —è–∫—ñ –≤–∂–µ –æ–±'—î–¥–Ω–∞–Ω—ñ
            merged_indices = set()
            new_fixed_segments = []
            i = 0
            while i < len(fixed_segments):
                if i in merged_indices:
                    i += 1
                    continue
                # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ –ø–æ—Ç—Ä—ñ–±–Ω–æ –æ–±'—î–¥–Ω–∞—Ç–∏ –∑ –Ω–∞—Å—Ç—É–ø–Ω–∏–º
                should_merge = False
                merge_end = i
                for merge_i, merge_j in merge_decisions:
                    if merge_i == i:
                        should_merge = True
                        merge_end = merge_j
                        merged_indices.add(merge_j)
                        break
                if should_merge:
                    # –û–±'—î–¥–Ω—É—î–º–æ —Å–µ–≥–º–µ–Ω—Ç–∏
                    merged_seg = {
                        'speaker': fixed_segments[i]['speaker'],
                        'start': fixed_segments[i]['start'],
                        'end': fixed_segments[merge_end]['end'],
                        'text': (fixed_segments[i]['text'] + ' ' + fixed_segments[merge_end]['text']).strip()
                    }
                    new_fixed_segments.append(merged_seg)
                    i = merge_end + 1
                else:
                    new_fixed_segments.append(fixed_segments[i])
                    i += 1
            fixed_segments = new_fixed_segments
            print(f"  ‚úÖ Merged: {len(combined_segments)} ‚Üí {len(fixed_segments)} segments")
        # –ö—Ä–æ–∫ 2: –ó–∞—Å—Ç–æ—Å–æ–≤—É—î–º–æ –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è —Å–ø—ñ–∫–µ—Ä—ñ–≤ (–∑ –ø–µ—Ä–µ—Ç–∏–Ω—ñ–≤ + –∑–∞–≥–∞–ª—å–Ω—ñ + —Ñ—ñ–Ω–∞–ª—å–Ω—ñ)
        all_corrections = {**overlap_corrections, **speaker_corrections, **final_corrections}
        if all_corrections:
            print(f"  üîß Applying {len(all_corrections)} speaker corrections...")
            # –ü–µ—Ä–µ—Ä–∞—Ö–æ–≤—É—î–º–æ —ñ–Ω–¥–µ–∫—Å–∏ –ø—ñ—Å–ª—è –æ–±'—î–¥–Ω–∞–Ω–Ω—è
            correction_map = {}
            current_idx = 0
            for orig_idx in range(len(combined_segments)):
                if orig_idx in all_corrections:
                    correction_map[current_idx] = all_corrections[orig_idx]
                current_idx += 1
            for seg_idx, correct_speaker in correction_map.items():
                if 0 <= seg_idx < len(fixed_segments):
                    fixed_segments[seg_idx]['speaker'] = correct_speaker
            print(f"  ‚úÖ Applied speaker corrections")
        if len(fixed_segments) < len(combined_segments) or all_corrections:
            print(f"‚úÖ LLM fix: {len(combined_segments)} ‚Üí {len(fixed_segments)} segments")
        else:
            print(f"‚úÖ LLM fix: {len(fixed_segments)} segments (no changes)")
        return fixed_segments, llm_iterations
    except Exception as e:
        print(f"‚ùå Critical error in fix_diarization_errors_with_llm: {e}")
        import traceback
        traceback.print_exc()
        return combined_segments, []


def normalize_speaker_order(segments):
    """
    –ù–æ—Ä–º–∞–ª—ñ–∑—É—î –ø–æ—Ä—è–¥–æ–∫ —Å–ø—ñ–∫–µ—Ä—ñ–≤ –ë–ï–ó —Å–ª—ñ–ø–æ–≥–æ —á–µ—Ä–≥—É–≤–∞–Ω–Ω—è.
    –í–∏–ø—Ä–∞–≤–ª—è—î —Ç—ñ–ª—å–∫–∏ –æ—á–µ–≤–∏–¥–Ω—ñ –ø–æ–º–∏–ª–∫–∏:
    1. –ù–æ—Ä–º–∞–ª—ñ–∑—É—î –¥–æ 2 —Å–ø—ñ–∫–µ—Ä—ñ–≤ (—è–∫—â–æ –±—ñ–ª—å—à–µ)
    2. –í–∏–ø—Ä–∞–≤–ª—è—î —Ç—ñ–ª—å–∫–∏ —è–∫—â–æ –ø–µ—Ä—à–∏–π —Å–µ–≥–º–µ–Ω—Ç –º–∞—î —è–≤–Ω–æ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ (–Ω–∞ –æ—Å–Ω–æ–≤—ñ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É)
    3. –ù–ï —Ä–æ–±–∏—Ç—å —Å–ª—ñ–ø–æ–≥–æ —á–µ—Ä–≥—É–≤–∞–Ω–Ω—è - –∑–∞–ª–∏—à–∞—î —Ç–µ, —â–æ –≤–∏–∑–Ω–∞—á–∏–≤ –¥—ñ–∞—Ä–∏–∑–∞—Ç–æ—Ä
    """
    if not segments or len(segments) == 0:
        return segments
    print(f"üîß Normalizing speaker order for {len(segments)} segments (NO blind alternation)...")
    # –°—Ç–≤–æ—Ä—é—î–º–æ –∫–æ–ø—ñ—é –≤—ñ–¥—Ä–∞–∑—É, —â–æ–± –Ω–µ –º–æ–¥–∏—Ñ—ñ–∫—É–≤–∞—Ç–∏ –æ—Ä–∏–≥—ñ–Ω–∞–ª
    fixed_segments = [seg.copy() for seg in segments]
    
    # –ö—Ä–æ–∫ 1: –ù–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ –¥–æ 2 —Å–ø—ñ–∫–µ—Ä—ñ–≤ (—è–∫—â–æ –±—ñ–ª—å—à–µ)
    unique_speakers = sorted(set(seg['speaker'] for seg in fixed_segments))
    if len(unique_speakers) > 2:
        print(f"‚ö†Ô∏è  Found {len(unique_speakers)} speakers, normalizing to 2 speakers")
        # –ì—Ä—É–ø—É—î–º–æ —Å–ø—ñ–∫–µ—Ä—ñ–≤: –ø–µ—Ä—à—ñ 50% ‚Üí 0, –æ—Å—Ç–∞–Ω–Ω—ñ 50% ‚Üí 1
        speaker_group_map = {}
        mid_point = len(unique_speakers) // 2
        for idx, sp in enumerate(unique_speakers):
            speaker_group_map[sp] = 0 if idx < mid_point else 1
        for seg in fixed_segments:
            seg['speaker'] = speaker_group_map.get(seg['speaker'], seg['speaker'] % 2)
        unique_speakers = [0, 1]
    
    # –ö—Ä–æ–∫ 2: –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –ø–µ—Ä—à–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞ - —Ç—ñ–ª—å–∫–∏ —è–∫—â–æ —è–≤–Ω–æ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ
    # –ù–ï —Ä–æ–±–∏–º–æ —Å–ª—ñ–ø–æ–≥–æ —á–µ—Ä–≥—É–≤–∞–Ω–Ω—è - –∑–∞–ª–∏—à–∞—î–º–æ —Ç–µ, —â–æ –≤–∏–∑–Ω–∞—á–∏–≤ –¥—ñ–∞—Ä–∏–∑–∞—Ç–æ—Ä
    first_speaker = fixed_segments[0]['speaker']
    
    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ç—ñ–ª—å–∫–∏ —è–∫—â–æ –ø–µ—Ä—à–∏–π —Å–µ–≥–º–µ–Ω—Ç –º–∞—î —Å–ø—ñ–∫–µ—Ä–∞ 1, –∞–ª–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–∫–∞–∑—É—î –Ω–∞ —Å–ø—ñ–∫–µ—Ä–∞ 0
    # –ê–ª–µ –ù–ï –∑–º—ñ–Ω—é—î–º–æ, —è–∫—â–æ –¥—ñ–∞—Ä–∏–∑–∞—Ç–æ—Ä –ø—Ä–∞–≤–∏–ª—å–Ω–æ –≤–∏–∑–Ω–∞—á–∏–≤
    if first_speaker == 1 and len(fixed_segments) > 1:
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ –ø–µ—Ä—à—ñ –∫—ñ–ª—å–∫–∞ —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ –º–∞—é—Ç—å –æ–¥–Ω–∞–∫–æ–≤–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
        # –Ø–∫—â–æ —Ç–∞–∫, —ñ —Ü–µ —Å–ø—ñ–∫–µ—Ä 1, –º–æ–∂–ª–∏–≤–æ –¥—ñ–∞—Ä–∏–∑–∞—Ç–æ—Ä –ø—Ä–∞–≤–∏–ª—å–Ω–æ –≤–∏–∑–Ω–∞—á–∏–≤
        first_few_speakers = [seg['speaker'] for seg in fixed_segments[:min(3, len(fixed_segments))]]
        if all(sp == 1 for sp in first_few_speakers):
            # –í—Å—ñ –ø–µ—Ä—à—ñ —Å–µ–≥–º–µ–Ω—Ç–∏ –º–∞—é—Ç—å —Å–ø—ñ–∫–µ—Ä–∞ 1 - –º–æ–∂–ª–∏–≤–æ –¥—ñ–∞—Ä–∏–∑–∞—Ç–æ—Ä –ø—Ä–∞–≤–∏–ª—å–Ω–æ –≤–∏–∑–Ω–∞—á–∏–≤
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ç–µ–∫—Å—Ç—É –¥–ª—è –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è —Ä–æ–ª—ñ
            first_text = fixed_segments[0]['text'].lower()
            # –Ø–∫—â–æ —Ç–µ–∫—Å—Ç –≤–∏–≥–ª—è–¥–∞—î —è–∫ –∫–ª—ñ—î–Ω—Ç (–æ–ø–∏—Å—É—î –ø—Ä–æ–±–ª–µ–º—É, –ø—Ä–æ—Å–∏—Ç—å –¥–æ–ø–æ–º–æ–≥—É), –∑–∞–ª–∏—à–∞—î–º–æ —è–∫ —î
            client_indicators = ['i have', 'i need', 'i can\'t', 'help me', 'problem', 'issue', 'error']
            if any(indicator in first_text for indicator in client_indicators):
                print(f"‚úÖ First segment appears to be Client (Speaker 2), keeping diarization result")
                # –ó–∞–ª–∏—à–∞—î–º–æ —è–∫ —î - –¥—ñ–∞—Ä–∏–∑–∞—Ç–æ—Ä –ø—Ä–∞–≤–∏–ª—å–Ω–æ –≤–∏–∑–Ω–∞—á–∏–≤
            else:
                # –ú–æ–∂–ª–∏–≤–æ –ø–æ–º–∏–ª–∫–∞ - –∞–ª–µ –Ω–µ –∑–º—ñ–Ω—é—î–º–æ –±–µ–∑–ø—ñ–¥—Å—Ç–∞–≤–Ω–æ
                print(f"‚ö†Ô∏è  First segment is Speaker 2, but context unclear. Keeping diarization result.")
        else:
            # –†—ñ–∑–Ω—ñ —Å–ø—ñ–∫–µ—Ä–∏ –Ω–∞ –ø–æ—á–∞—Ç–∫—É - –∑–∞–ª–∏—à–∞—î–º–æ —è–∫ —î
            print(f"‚úÖ Mixed speakers at start, keeping diarization result")
    
    # –ö—Ä–æ–∫ 3: –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –±–∞–ª–∞–Ω—Å—É —Å–ø—ñ–∫–µ—Ä—ñ–≤ (—Ç—ñ–ª—å–∫–∏ –¥–ª—è –ª–æ–≥—É–≤–∞–Ω–Ω—è)
    speaker_counts = {}
    for seg in fixed_segments:
        sp = seg['speaker']
        speaker_counts[sp] = speaker_counts.get(sp, 0) + 1
    print(f"üìä Final speaker distribution (preserving diarization):")
    for sp, count in sorted(speaker_counts.items()):
        print(f"   Speaker {sp+1}: {count} segments ({count/len(fixed_segments)*100:.1f}%)")
    print(f"‚úÖ Speaker normalization complete. First segment: Speaker {fixed_segments[0]['speaker']+1} (preserved from diarization)")
    return fixed_segments


def fix_diarization_errors_advanced(combined_segments):
    """
    –ü–æ–∫—Ä–∞—â–µ–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è –ø–æ–º–∏–ª–æ–∫ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º —Å–µ–º–∞–Ω—Ç–∏–∫–∏ —Ç–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –¥—ñ–∞–ª–æ–≥—É.
    –í–∏–ø—Ä–∞–≤–ª—è—î –ø–æ–º–∏–ª–∫–∏, –∫–æ–ª–∏:
    - –†–µ–ø–ª—ñ–∫–∏ –æ–¥–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞ —Ä–æ–∑–±–∏–≤–∞—é—Ç—å—Å—è –Ω–∞ –∫—ñ–ª—å–∫–∞ —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ –∑ —Ä—ñ–∑–Ω–∏–º–∏ —Å–ø—ñ–∫–µ—Ä–∞–º–∏
    - –î—É–∂–µ –∫–æ—Ä–æ—Ç–∫—ñ —Å–µ–≥–º–µ–Ω—Ç–∏ (< 0.5 —Å–µ–∫) —î —á–∞—Å—Ç–∏–Ω–æ—é –±—ñ–ª—å—à–∏—Ö —Ä–µ–ø–ª—ñ–∫
    - –î—ñ–∞—Ä–∏–∑–∞—Ü—ñ—è –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ –≤–∏–∑–Ω–∞—á–∞—î —Å–ø—ñ–∫–µ—Ä–∞ –Ω–∞ –º–µ–∂–∞—Ö —Ä–µ–ø–ª—ñ–∫
    Args:
        combined_segments: —Å–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ [{'speaker': int, 'start': float, 'end': float, 'text': str}]
    Returns:
        fixed_segments: –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–∏–π —Å–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç—ñ–≤
    """
    if not combined_segments or len(combined_segments) < 2:
        return combined_segments
    print(f"üîß Advanced fixing diarization errors in {len(combined_segments)} segments...")
    # –ì—Ä–∞–º–∞—Ç–∏—á–Ω—ñ –º–∞—Ä–∫–µ—Ä–∏ –Ω–µ–ø–æ–≤–Ω–∏—Ö —Ñ—Ä–∞–∑ (–∞–Ω–≥–ª—ñ–π—Å—å–∫–∞)
    incomplete_endings = [
        'to', 'and', 'or', 'but',
        'did you', 'can you', 'will you', 'would you', 'could you', 'should you',
        'try to', 'want to', 'need to', 'have to', 'going to', 'supposed to',
        'if', 'when', 'where', 'what', 'who', 'which', 'how',
        'that', 'this', 'these', 'those',
        'because', 'although', 'however', 'therefore',
        'i', 'it', 'the', 'a', 'an'  # –î—É–∂–µ –∫–æ—Ä–æ—Ç–∫—ñ —Å–ª–æ–≤–∞ –Ω–∞ –∫—ñ–Ω—Ü—ñ
    ]
    # –ú–∞—Ä–∫–µ—Ä–∏ –ø—Ä–æ–¥–æ–≤–∂–µ–Ω–Ω—è (–ø–æ—á–∞—Ç–æ–∫ –Ω–∞—Å—Ç—É–ø–Ω–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞)
    continuation_markers = [
        'reset', 'open', 'close', 'check', 'try', 'do', 'make', 'get', 'set',
        'configure', 'connect', 'disconnect', 'restart', 'reboot', 'update',
        'enter', 'access', 'see', 'show', 'find', 'look'
    ]
    fixed_segments = []
    i = 0
    while i < len(combined_segments):
        current_seg = combined_segments[i]
        current_text = current_seg['text'].strip()
        current_text_lower = current_text.lower()
        current_duration = current_seg['end'] - current_seg['start']
        # –ö—Ä–æ–∫ 1: –û–±—Ä–æ–±–∫–∞ –¥—É–∂–µ –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ (< 0.5 —Å–µ–∫)
        if current_duration < 0.5 and i > 0 and i < len(combined_segments) - 1:
            prev_seg = combined_segments[i - 1]
            next_seg = combined_segments[i + 1]
            # –í–∏–∑–Ω–∞—á–∞—î–º–æ, –¥–æ —è–∫–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞ –ø—Ä–∏—î–¥–Ω–∞—Ç–∏ –∫–æ—Ä–æ—Ç–∫–∏–π
            gap_to_prev = current_seg['start'] - prev_seg['end']
            gap_to_next = next_seg['start'] - current_seg['end']
            # –ü—Ä–∏—î–¥–Ω—É—î–º–æ –¥–æ –Ω–∞–π–±–ª–∏–∂—á–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞
            if gap_to_prev < gap_to_next and gap_to_prev < 1.0:
                # –ü—Ä–∏—î–¥–Ω—É—î–º–æ –¥–æ –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ–≥–æ
                prev_seg['end'] = max(prev_seg['end'], current_seg['end'])
                prev_seg['text'] = (prev_seg['text'] + ' ' + current_text).strip()
                print(f"  üîó Merged short segment '{current_text[:30]}...' ({current_duration:.2f}s) to previous")
                i += 1
                continue
            elif gap_to_next < 1.0:
                # –ü—Ä–∏—î–¥–Ω—É—î–º–æ –¥–æ –Ω–∞—Å—Ç—É–ø–Ω–æ–≥–æ
                next_seg['start'] = min(next_seg['start'], current_seg['start'])
                next_seg['text'] = (current_text + ' ' + next_seg['text']).strip()
                print(f"  üîó Merged short segment '{current_text[:30]}...' ({current_duration:.2f}s) to next")
                i += 1
                continue
        # –ö—Ä–æ–∫ 2: –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–µ–ø–æ–≤–Ω–∏—Ö —Ñ—Ä–∞–∑
        words = current_text_lower.split()
        is_incomplete = False
        if len(words) > 0:
            last_word = words[-1].rstrip('.,!?;:')
            last_two_words = ' '.join(words[-2:]).rstrip('.,!?;:') if len(words) >= 2 else ''
            last_three_words = ' '.join(words[-3:]).rstrip('.,!?;:') if len(words) >= 3 else ''
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –≥—Ä–∞–º–∞—Ç–∏—á–Ω—ñ –º–∞—Ä–∫–µ—Ä–∏ –Ω–µ–ø–æ–≤–Ω–æ—Ç–∏
            if last_word in incomplete_endings:
                is_incomplete = True
            elif last_two_words in incomplete_endings:
                is_incomplete = True
            elif last_three_words in incomplete_endings:
                is_incomplete = True
            # –î–æ–¥–∞—Ç–∫–æ–≤–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞: –¥—É–∂–µ –∫–æ—Ä–æ—Ç–∫—ñ —Ñ—Ä–∞–∑–∏ (< 3 —Å–ª–æ–≤–∞) —á–∞—Å—Ç–æ –Ω–µ–ø–æ–≤–Ω—ñ
            elif len(words) < 3 and current_duration < 2.0:
                is_incomplete = True
        # –ö—Ä–æ–∫ 3: –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞—Å—Ç—É–ø–Ω–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞
        if is_incomplete and i + 1 < len(combined_segments):
            next_seg = combined_segments[i + 1]
            next_text = next_seg['text'].strip()
            next_text_lower = next_text.lower()
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —É–º–æ–≤–∏ –¥–ª—è –æ–±'—î–¥–Ω–∞–Ω–Ω—è:
            different_speakers = current_seg['speaker'] != next_seg['speaker']
            pause = next_seg['start'] - current_seg['end']
            short_pause = pause < 1.5 and pause >= 0
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ –Ω–∞—Å—Ç—É–ø–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç —î –ø—Ä–æ–¥–æ–≤–∂–µ–Ω–Ω—è–º
            is_continuation = False
            next_words = next_text_lower.split()
            if next_words:
                first_word = next_words[0].rstrip('.,!?;:')
                # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –º–∞—Ä–∫–µ—Ä–∏ –ø—Ä–æ–¥–æ–≤–∂–µ–Ω–Ω—è
                if first_word in continuation_markers:
                    is_continuation = True
                # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ –ø–µ—Ä—à–µ —Å–ª–æ–≤–æ –Ω–µ –∑ –≤–µ–ª–∏–∫–æ—ó –ª—ñ—Ç–µ—Ä–∏ (–ø—Ä–æ–¥–æ–≤–∂–µ–Ω–Ω—è —Ä–µ—á–µ–Ω–Ω—è)
                elif not next_text[0].isupper() and len(next_words) < 10:
                    is_continuation = True
                # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —Ä–∞–∑–æ–º —É—Ç–≤–æ—Ä—é—é—Ç—å –≥—Ä–∞–º–∞—Ç–∏—á–Ω–æ –ø—Ä–∞–≤–∏–ª—å–Ω—É —Ñ—Ä–∞–∑—É
                combined_text = (current_text + ' ' + next_text).strip()
                if len(combined_text.split()) < 25 and '  ' not in combined_text:
                    is_continuation = True
            # –ö—Ä–æ–∫ 4: –ê–Ω–∞–ª—ñ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –¥—ñ–∞–ª–æ–≥—É (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ü—ñ—è —Å–ø—ñ–∫–µ—Ä—ñ–≤)
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ —Å–µ–≥–º–µ–Ω—Ç–∏ –¥–ª—è –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –ø–∞—Ç—Ç–µ—Ä–Ω—É
            speaker_pattern = []
            for j in range(max(0, i - 3), i + 2):
                if j < len(combined_segments):
                    speaker_pattern.append(combined_segments[j]['speaker'])
            # –Ø–∫—â–æ —Å–ø—ñ–∫–µ—Ä–∏ –ø–æ—Å—Ç—ñ–π–Ω–æ –∑–º—ñ–Ω—é—é—Ç—å—Å—è (–∞–Ω–æ–º–∞–ª—ñ—è), —Ü–µ –º–æ–∂–µ –±—É—Ç–∏ –ø–æ–º–∏–ª–∫–∞
            frequent_changes = sum(1 for k in range(len(speaker_pattern) - 1) 
                                 if speaker_pattern[k] != speaker_pattern[k + 1])
            is_anomaly = frequent_changes >= len(speaker_pattern) - 1
            # –ö—Ä–æ–∫ 5: –í–∏–∑–Ω–∞—á–µ–Ω–Ω—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
            if is_continuation and different_speakers and short_pause:
                # –í–∏–∑–Ω–∞—á–∞—î–º–æ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
                current_word_count = len(words)
                next_word_count = len(next_words)
                current_duration = current_seg['end'] - current_seg['start']
                next_duration = next_seg['end'] - next_seg['start']
                # –ö—Ä–∏—Ç–µ—Ä—ñ–π 1: –•—Ç–æ –ø–æ—á–∞–≤ —Ñ—Ä–∞–∑—É (–ø–µ—Ä—à–∏–π —Å–µ–≥–º–µ–Ω—Ç) - –Ω–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–∏–π
                # –ö—Ä–∏—Ç–µ—Ä—ñ–π 2: –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥—ñ–∞–ª–æ–≥—É (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ü—ñ—è —Å–ø—ñ–∫–µ—Ä—ñ–≤)
                # –ö—Ä–∏—Ç–µ—Ä—ñ–π 3: –ë—ñ–ª—å—à–∞ —á–∞—Å—Ç–∏–Ω–∞ —Ñ—Ä–∞–∑–∏
                # –Ø–∫—â–æ —Ü–µ –∞–Ω–æ–º–∞–ª—ñ—è (—á–∞—Å—Ç–æ –∑–º—ñ–Ω—é—é—Ç—å—Å—è —Å–ø—ñ–∫–µ—Ä–∏), –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ç–æ–≥–æ, —Ö—Ç–æ –ø–æ—á–∞–≤
                if is_anomaly:
                    correct_speaker = current_seg['speaker']
                # –Ø–∫—â–æ –ø–æ—Ç–æ—á–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç –∑–Ω–∞—á–Ω–æ –±—ñ–ª—å—à–∏–π, –≤—ñ–Ω –ø—Ä–∞–≤–∏–ª—å–Ω–∏–π
                elif current_word_count >= next_word_count * 1.2 or current_duration >= next_duration * 1.2:
                    correct_speaker = current_seg['speaker']
                # –Ø–∫—â–æ –Ω–∞—Å—Ç—É–ø–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç –∑–Ω–∞—á–Ω–æ –±—ñ–ª—å—à–∏–π, –≤—ñ–Ω –ø—Ä–∞–≤–∏–ª—å–Ω–∏–π
                elif next_word_count > current_word_count * 1.5:
                    correct_speaker = next_seg['speaker']
                # –ó–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º - —Ç–æ–π, —Ö—Ç–æ –ø–æ—á–∞–≤ —Ñ—Ä–∞–∑—É
                else:
                    correct_speaker = current_seg['speaker']
                # –û–±'—î–¥–Ω—É—î–º–æ —Å–µ–≥–º–µ–Ω—Ç–∏
                merged_seg = {
                    'speaker': correct_speaker,
                    'start': current_seg['start'],
                    'end': next_seg['end'],
                    'text': (current_text + ' ' + next_text).strip()
                }
                fixed_segments.append(merged_seg)
                print(f"  üîß Merged: '{current_text[:40]}...' + '{next_text[:40]}...' ‚Üí Speaker {correct_speaker} (was {current_seg['speaker']} + {next_seg['speaker']})")
                # –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –Ω–∞—Å—Ç—É–ø–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç
                i += 2
                continue
        # –Ø–∫—â–æ –Ω–µ –æ–±'—î–¥–Ω—É–≤–∞–ª–∏, –¥–æ–¥–∞—î–º–æ –ø–æ—Ç–æ—á–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç —è–∫ —î
        fixed_segments.append(current_seg)
        i += 1
    if len(fixed_segments) < len(combined_segments):
        print(f"‚úÖ Advanced fix: {len(combined_segments)} ‚Üí {len(fixed_segments)} segments")
    else:
        print(f"‚úÖ No advanced fixes needed: {len(fixed_segments)} segments")
    return fixed_segments


def fix_diarization_errors(combined_segments):
    """
    –í–∏–ø—Ä–∞–≤–ª—è—î –ø–æ–º–∏–ª–∫–∏ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó, –æ–±'—î–¥–Ω—É—é—á–∏ —Å–µ–≥–º–µ–Ω—Ç–∏, —è–∫—ñ –Ω–∞—Å–ø—Ä–∞–≤–¥—ñ –Ω–∞–ª–µ–∂–∞—Ç—å –æ–¥–Ω–æ–º—É —Å–ø—ñ–∫–µ—Ä—É.
    –í–∏—è–≤–ª—è—î —Ç–∞ –æ–±'—î–¥–Ω—É—î —Å–µ–≥–º–µ–Ω—Ç–∏, –¥–µ:
    - –ü–æ–ø–µ—Ä–µ–¥–Ω—ñ–π —Å–µ–≥–º–µ–Ω—Ç –∑–∞–∫—ñ–Ω—á—É—î—Ç—å—Å—è –Ω–µ–ø–æ–≤–Ω–æ—é —Ñ—Ä–∞–∑–æ—é
    - –ù–∞—Å—Ç—É–ø–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç —î –ø—Ä–æ–¥–æ–≤–∂–µ–Ω–Ω—è–º –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ—ó —Ñ—Ä–∞–∑–∏
    - –ú—ñ–∂ –Ω–∏–º–∏ –∫–æ—Ä–æ—Ç–∫–∞ –ø–∞—É–∑–∞ (< 1.5 —Å–µ–∫)
    - –†—ñ–∑–Ω—ñ —Å–ø—ñ–∫–µ—Ä–∏ (–ø–æ–º–∏–ª–∫–∞ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó)
    Args:
        combined_segments: —Å–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ [{'speaker': int, 'start': float, 'end': float, 'text': str}]
    Returns:
        fixed_segments: –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–∏–π —Å–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç—ñ–≤
    """
    if not combined_segments or len(combined_segments) < 2:
        return combined_segments
    print(f"üîß Fixing diarization errors in {len(combined_segments)} segments...")
    # –ì—Ä–∞–º–∞—Ç–∏—á–Ω—ñ –º–∞—Ä–∫–µ—Ä–∏ –Ω–µ–ø–æ–≤–Ω–∏—Ö —Ñ—Ä–∞–∑ (–∞–Ω–≥–ª—ñ–π—Å—å–∫–∞)
    incomplete_phrase_markers = [
        ' to ', ' to', ' and ', ' and', ' or ', ' or', ' but ', ' but',
        ' did you', ' can you', ' will you', ' would you', ' could you', ' should you',
        ' try to', ' want to', ' need to', ' have to', ' going to', ' supposed to',
        ' if ', ' when ', ' where ', ' what ', ' who ', ' which ', ' how ',
        ' that ', ' this ', ' these ', ' those ',
        ' because ', ' although ', ' however ', ' therefore '
    ]
    # –ú–∞—Ä–∫–µ—Ä–∏ –ø—Ä–æ–¥–æ–≤–∂–µ–Ω–Ω—è (–ø–æ—á–∞—Ç–æ–∫ –Ω–∞—Å—Ç—É–ø–Ω–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞)
    continuation_markers = [
        'reset', 'open', 'close', 'check', 'try', 'do', 'make', 'get', 'set',
        'configure', 'connect', 'disconnect', 'restart', 'reboot', 'update'
    ]
    fixed_segments = []
    i = 0
    while i < len(combined_segments):
        current_seg = combined_segments[i]
        current_text = current_seg['text'].strip().lower()
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ –ø–æ—Ç–æ—á–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç –∑–∞–∫—ñ–Ω—á—É—î—Ç—å—Å—è –Ω–µ–ø–æ–≤–Ω–æ—é —Ñ—Ä–∞–∑–æ—é
        is_incomplete = False
        words = current_text.split()
        if len(words) > 0:
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –æ—Å—Ç–∞–Ω–Ω—ñ —Å–ª–æ–≤–∞ —Ñ—Ä–∞–∑–∏
            last_word = words[-1].rstrip('.,!?;:')
            last_two_words = ' '.join(words[-2:]).rstrip('.,!?;:') if len(words) >= 2 else ''
            last_three_words = ' '.join(words[-3:]).rstrip('.,!?;:') if len(words) >= 3 else ''
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –≥—Ä–∞–º–∞—Ç–∏—á–Ω—ñ –º–∞—Ä–∫–µ—Ä–∏ –Ω–µ–ø–æ–≤–Ω–æ—Ç–∏
            incomplete_endings = [
                'to', 'and', 'or', 'but',
                'did you', 'can you', 'will you', 'would you', 'could you', 'should you',
                'try to', 'want to', 'need to', 'have to', 'going to', 'supposed to',
                'if', 'when', 'where', 'what', 'who', 'which', 'how',
                'that', 'this', 'these', 'those',
                'because', 'although', 'however', 'therefore'
            ]
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –æ—Å—Ç–∞–Ω–Ω—î —Å–ª–æ–≤–æ
            if last_word in incomplete_endings:
                is_incomplete = True
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –æ—Å—Ç–∞–Ω–Ω—ñ –¥–≤–∞ —Å–ª–æ–≤–∞
            elif last_two_words in incomplete_endings:
                is_incomplete = True
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –æ—Å—Ç–∞–Ω–Ω—ñ —Ç—Ä–∏ —Å–ª–æ–≤–∞
            elif last_three_words in incomplete_endings:
                is_incomplete = True
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —Ñ—Ä–∞–∑–∞ –∑–∞–∫—ñ–Ω—á—É—î—Ç—å—Å—è –Ω–∞ –º–∞—Ä–∫–µ—Ä –∑ –ø—Ä–æ–±—ñ–ª–æ–º –ø–µ—Ä–µ–¥ –Ω–∏–º
            for marker in incomplete_phrase_markers:
                marker_clean = marker.strip()
                if current_text.endswith(marker_clean) or current_text.endswith(marker_clean + '.'):
                    is_incomplete = True
                    break
                # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ –º–∞—Ä–∫–µ—Ä –≤ –æ—Å—Ç–∞–Ω–Ω—ñ—Ö —Å–ª–æ–≤–∞—Ö
                if marker_clean in last_three_words or marker_clean in last_two_words:
                    is_incomplete = True
                    break
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –Ω–∞—Å—Ç—É–ø–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç, —è–∫—â–æ —î
        if is_incomplete and i + 1 < len(combined_segments):
            next_seg = combined_segments[i + 1]
            next_text = next_seg['text'].strip().lower()
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —É–º–æ–≤–∏ –¥–ª—è –æ–±'—î–¥–Ω–∞–Ω–Ω—è:
            # 1. –†—ñ–∑–Ω—ñ —Å–ø—ñ–∫–µ—Ä–∏ (–ø–æ–º–∏–ª–∫–∞ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó)
            different_speakers = current_seg['speaker'] != next_seg['speaker']
            # 2. –ö–æ—Ä–æ—Ç–∫–∞ –ø–∞—É–∑–∞ –º—ñ–∂ —Å–µ–≥–º–µ–Ω—Ç–∞–º–∏ (< 1.5 —Å–µ–∫)
            pause = next_seg['start'] - current_seg['end']
            short_pause = pause < 1.5 and pause >= 0
            # 3. –ù–∞—Å—Ç—É–ø–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç –≤–∏–≥–ª—è–¥–∞—î —è–∫ –ø—Ä–æ–¥–æ–≤–∂–µ–Ω–Ω—è
            is_continuation = False
            next_words = next_text.split()
            if next_words:
                first_word = next_words[0].rstrip('.,!?;:').lower()
                # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ –ø–æ—á–∏–Ω–∞—î—Ç—å—Å—è –∑ –¥—ñ—î—Å–ª–æ–≤–∞ (–ø—Ä–æ–¥–æ–≤–∂–µ–Ω–Ω—è)
                if first_word in continuation_markers:
                    is_continuation = True
                # –ê–±–æ —è–∫—â–æ –ø–µ—Ä—à–µ —Å–ª–æ–≤–æ –Ω–µ –∑ –≤–µ–ª–∏–∫–æ—ó –ª—ñ—Ç–µ—Ä–∏ (–ø—Ä–æ–¥–æ–≤–∂–µ–Ω–Ω—è —Ä–µ—á–µ–Ω–Ω—è)
                elif not next_seg['text'][0].isupper() and len(next_words) < 10:
                    is_continuation = True
                # –ê–±–æ —è–∫—â–æ —Ä–∞–∑–æ–º —É—Ç–≤–æ—Ä—é—é—Ç—å –≥—Ä–∞–º–∞—Ç–∏—á–Ω–æ –ø—Ä–∞–≤–∏–ª—å–Ω—É —Ñ—Ä–∞–∑—É
                combined_text = (current_seg['text'] + ' ' + next_seg['text']).strip()
                # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ –≤–∏–≥–ª—è–¥–∞—î —è–∫ –æ–¥–Ω–µ —Ä–µ—á–µ–Ω–Ω—è (–Ω–µ –¥—É–∂–µ –¥–æ–≤–≥–∞ —Ñ—Ä–∞–∑–∞, –Ω–µ–º–∞—î –ø–æ–¥–≤—ñ–π–Ω–∏—Ö –ø—Ä–æ–±—ñ–ª—ñ–≤)
                if len(combined_text.split()) < 25 and '  ' not in combined_text:
                    # –î–æ–¥–∞—Ç–∫–æ–≤–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞: —á–∏ –ø–µ—Ä—à–µ —Å–ª–æ–≤–æ –Ω–∞—Å—Ç—É–ø–Ω–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞ –ª–æ–≥—ñ—á–Ω–æ –ø—Ä–æ–¥–æ–≤–∂—É—î –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π
                    if first_word in continuation_markers or first_word in ['reset', 'open', 'close', 'check', 'try', 'do', 'make', 'get', 'set', 'configure', 'connect']:
                        is_continuation = True
            # 4. –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —Ä–∞–∑–æ–º —É—Ç–≤–æ—Ä—é—é—Ç—å –ª–æ–≥—ñ—á–Ω—É —Ñ—Ä–∞–∑—É
            # (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, "did you try to" + "reset" = "did you try to reset")
            if is_continuation and different_speakers and short_pause:
                # –í–∏–∑–Ω–∞—á–∞—î–º–æ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
                # –ö—Ä–∏—Ç–µ—Ä—ñ–π 1: —Ö—Ç–æ –ø–æ—á–∞–≤ —Ñ—Ä–∞–∑—É (–ø–µ—Ä—à–∏–π —Å–µ–≥–º–µ–Ω—Ç)
                # –ö—Ä–∏—Ç–µ—Ä—ñ–π 2: –±—ñ–ª—å—à–µ —Å–ª—ñ–≤
                # –ö—Ä–∏—Ç–µ—Ä—ñ–π 3: –±—ñ–ª—å—à–∞ —Ç—Ä–∏–≤–∞–ª—ñ—Å—Ç—å
                current_word_count = len(current_text.split())
                next_word_count = len(next_text.split())
                current_duration = current_seg['end'] - current_seg['start']
                next_duration = next_seg['end'] - next_seg['start']
                # –í–∏–∑–Ω–∞—á–∞—î–º–æ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
                if current_word_count >= next_word_count and current_duration >= next_duration:
                    correct_speaker = current_seg['speaker']
                elif next_word_count > current_word_count * 1.5:  # –ù–∞—Å—Ç—É–ø–Ω–∏–π –∑–Ω–∞—á–Ω–æ –±—ñ–ª—å—à–∏–π
                    correct_speaker = next_seg['speaker']
                else:
                    # –ó–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º - —Ç–æ–π, —Ö—Ç–æ –ø–æ—á–∞–≤ —Ñ—Ä–∞–∑—É
                    correct_speaker = current_seg['speaker']
                # –û–±'—î–¥–Ω—É—î–º–æ —Å–µ–≥–º–µ–Ω—Ç–∏
                merged_seg = {
                    'speaker': correct_speaker,
                    'start': current_seg['start'],
                    'end': next_seg['end'],
                    'text': (current_seg['text'] + ' ' + next_seg['text']).strip()
                }
                fixed_segments.append(merged_seg)
                print(f"üîß Merged segments: '{current_seg['text'][:50]}...' + '{next_seg['text'][:50]}...' ‚Üí Speaker {correct_speaker}")
                # –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –Ω–∞—Å—Ç—É–ø–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç, –±–æ –≤–∂–µ –æ–±'—î–¥–Ω–∞–Ω–æ
                i += 2
                continue
        # –Ø–∫—â–æ –Ω–µ –æ–±'—î–¥–Ω—É–≤–∞–ª–∏, –¥–æ–¥–∞—î–º–æ –ø–æ—Ç–æ—á–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç —è–∫ —î
        fixed_segments.append(current_seg)
        i += 1
    if len(fixed_segments) < len(combined_segments):
        print(f"‚úÖ Fixed: {len(combined_segments)} ‚Üí {len(fixed_segments)} segments")
    else:
        print(f"‚úÖ No errors found, kept {len(fixed_segments)} segments")
    return fixed_segments


def allowed_file(filename):
    """–ü–µ—Ä–µ–≤—ñ—Ä—è—î, —á–∏ –¥–æ–∑–≤–æ–ª–µ–Ω–∏–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª—É"""
    return '.' in filename and \
           filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS


@app.route('/api/health', methods=['GET'])
def health():
    """–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Å—Ç–∞–Ω—É —Å–µ—Ä–≤–µ—Ä–∞"""
    return jsonify({
        'status': 'ok',
        'speaker_model_loaded': speaker_model is not None,
        'whisper_model_loaded': whisper_model is not None
    })


@app.route('/api/diarize', methods=['POST'])
def api_diarize():
    """API –µ–Ω–¥–ø–æ—ñ–Ω—Ç –¥–ª—è –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó —Ç–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó"""
    try:
        if 'file' not in request.files:
            return jsonify({'error': 'No file uploaded'}), 400
        file = request.files['file']
        if file.filename == '':
            return jsonify({'error': 'No file selected'}), 400
        # –û—Ç—Ä–∏–º—É—î–º–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
        num_speakers = request.form.get('num_speakers', type=int)
        language = request.form.get('language', type=str) or None
        segment_duration = float(request.form.get('segment_duration', 1.5))
        overlap = float(request.form.get('overlap', 0.5))
        include_transcription = request.form.get('include_transcription', 'true').lower() == 'true'
        use_separation = request.form.get('use_separation', 'false').lower() == 'true'
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Ñ–∞–π–ª —Ç–∏–º—á–∞—Å–æ–≤–æ
        filepath = os.path.join(UPLOAD_FOLDER, file.filename)
        file.save(filepath)
        print(f"üìÅ Processing file: {file.filename}")
        print(f"üîÄ Use separation: {use_separation}")
        # –ö—Ä–æ–∫ 1: –ó–∞–≤–∂–¥–∏ –≤–∏–∫–æ–Ω—É—î–º–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—é —Å–ø–æ—á–∞—Ç–∫—É
        print("üîç Step 1: Standard diarization...")
        embeddings, timestamps = extract_speaker_embeddings(
            filepath, 
            segment_duration=segment_duration, 
            overlap=overlap
        )
        print("üë• Performing standard diarization...")
        standard_diarization_segments = diarize_audio(embeddings, timestamps, num_speakers)
        print("‚úÖ Step 1 finished: Standard diarization completed")
        # –Ø–∫—â–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è —Å–ø—ñ–∫–µ—Ä—ñ–≤
        if use_separation:
            print("üîÄ Step 1: Separating speakers...")
            separation_result = separate_speakers(filepath)
            if not separation_result.get('success'):
                return jsonify({
                    'success': False,
                    'error': f"Separation failed: {separation_result.get('error', 'Unknown error')}"
                }), 500
            # –î—ñ–∞—Ä–∏–∑—É—î–º–æ –∫–æ–∂–µ–Ω —Ä–æ–∑–¥—ñ–ª–µ–Ω–∏–π —Ç—Ä–µ–∫ –æ–∫—Ä–µ–º–æ
            all_diarization_segments = []
            separation_output_dir = separation_result['output_dir']
            for speaker_info in separation_result['speakers']:
                speaker_path = speaker_info['path']
                speaker_name = speaker_info['name']
                speaker_index = speaker_info['index']
                print(f"üîç Processing {speaker_name}...")
                # –í–∏—Ç—è–≥—É—î–º–æ –µ–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Ü—å–æ–≥–æ —Ç—Ä–µ–∫—É
                embeddings, timestamps = extract_speaker_embeddings(
                    speaker_path,
                    segment_duration=segment_duration,
                    overlap=overlap
                )
                if embeddings is not None and len(embeddings) > 0:
                    # –î—ñ–∞—Ä–∏–∑—É—î–º–æ (–¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç—Ä–µ–∫—É –º–∞—î –±—É—Ç–∏ –æ–¥–∏–Ω —Å–ø—ñ–∫–µ—Ä, –∞–ª–µ –ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ)
                    track_segments = diarize_audio(embeddings, timestamps, num_speakers=1)
                    # –î–æ–¥–∞—î–º–æ —Å–µ–≥–º–µ–Ω—Ç–∏ –∑ –ø—Ä–∞–≤–∏–ª—å–Ω–∏–º —Å–ø—ñ–∫–µ—Ä–æ–º
                    for seg in track_segments:
                        seg['speaker'] = speaker_index  # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —ñ–Ω–¥–µ–∫—Å –∑ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è
                        all_diarization_segments.append(seg)
                else:
                    print(f"‚ö†Ô∏è  No embeddings extracted for {speaker_name}")
            # –°–æ—Ä—Ç—É—î–º–æ –≤—Å—ñ —Å–µ–≥–º–µ–Ω—Ç–∏ –∑–∞ —á–∞—Å–æ–º
            all_diarization_segments.sort(key=lambda x: x['start'])
            # –ó–ª–∏–≤–∞—î–º–æ —Å—É—Å—ñ–¥–Ω—ñ —Å–µ–≥–º–µ–Ω—Ç–∏ –æ–¥–Ω–æ–≥–æ —Å–ø—ñ–∫–µ—Ä–∞
            diarization_segments = []
            if all_diarization_segments:
                current_speaker = all_diarization_segments[0]['speaker']
                current_start = all_diarization_segments[0]['start']
                prev_end = all_diarization_segments[0]['end']
                for seg in all_diarization_segments[1:]:
                    if seg['speaker'] != current_speaker:
                        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π —Å–µ–≥–º–µ–Ω—Ç
                        diarization_segments.append({
                            'speaker': current_speaker,
                            'start': round(current_start, 2),
                            'end': round(prev_end, 2)
                        })
                        # –ü–æ—á–∏–Ω–∞—î–º–æ –Ω–æ–≤–∏–π —Å–µ–≥–º–µ–Ω—Ç
                        current_speaker = seg['speaker']
                        current_start = seg['start']
                    prev_end = seg['end']
                # –î–æ–¥–∞—î–º–æ –æ—Å—Ç–∞–Ω–Ω—ñ–π —Å–µ–≥–º–µ–Ω—Ç
                diarization_segments.append({
                    'speaker': current_speaker,
                    'start': round(current_start, 2),
                    'end': round(prev_end, 2)
                })
            print(f"‚úÖ Combined diarization from {len(separation_result['speakers'])} separated tracks: {len(diarization_segments)} segments")
            # –û—á–∏—â–∞—î–º–æ —Ç–∏–º—á–∞—Å–æ–≤—ñ —Ñ–∞–π–ª–∏ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è
            try:
                import shutil
                if os.path.exists(separation_output_dir):
                    shutil.rmtree(separation_output_dir)
            except Exception as e:
                print(f"‚ö†Ô∏è  Could not clean up separation directory: {e}")
        else:
            # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ—ó –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó
            diarization_segments = standard_diarization_segments
        result = {
            'success': True,
            'diarization': {
                'segments': diarization_segments,
                'num_speakers': len(set(seg.get('speaker', 0) for seg in diarization_segments)) if diarization_segments else 0
            }
        }
        # –î–æ–¥–∞—î–º–æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—é, —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ
        print(f"üìù Include transcription: {include_transcription}")
        if include_transcription:
            print("üìù Transcribing audio...")
            try:
                transcription, transcription_segments, words = transcribe_audio(filepath, language)
                print(f"üìä Transcription result: transcription={bool(transcription)}, segments={len(transcription_segments) if transcription_segments else 0}, words={len(words) if words else 0}")
                if words and len(words) > 0:
                    print(f"‚úÖ Transcription completed: {len(words)} words extracted")
                    print(f"üìä First 5 words: {words[:5]}")
                else:
                    print("‚ö†Ô∏è  Warning: No words extracted from transcription")
                    print(f"üìä Transcription data: transcription={transcription[:100] if transcription else 'None'}..., segments={transcription_segments[:2] if transcription_segments else 'None'}")
            except Exception as e:
                print(f"‚ùå Error in transcribe_audio: {e}")
                import traceback
                traceback.print_exc()
                transcription, transcription_segments, words = None, [], []
            
            result['transcription'] = {
                'full_text': transcription or '',
                'segments': transcription_segments or []
            }
            # –û–±'—î–¥–Ω—É—î–º–æ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—é —Ç–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—é
            if words and len(words) > 0:
                print("üîó Combining diarization and transcription...")
                print(f"üìä Input: {len(diarization_segments)} diarization segments, {len(words)} words")
                try:
                    # –°–∫–∏–¥–∞—î–º–æ –∫–µ—à –ø–µ—Ä–µ–¥ –æ–±—Ä–æ–±–∫–æ—é
                    global _llm_iterations_cache
                    _llm_iterations_cache = []
                    print(f"üîÑ Reset LLM iterations cache")
                    combined = combine_diarization_and_transcription(
                        diarization_segments, 
                        words
                    )
                    print(f"üìä After combine_diarization_and_transcription: {len(combined) if combined else 0} segments")
                    # –û—Ç—Ä–∏–º—É—î–º–æ llm_iterations –∑ –∫–µ—à—É (—è–∫—â–æ –≤—ñ–Ω —î)
                    llm_iterations = _llm_iterations_cache if '_llm_iterations_cache' in globals() else []
                    print(f"üìä LLM iterations from cache: {len(llm_iterations)}")
                    if combined:
                        num_speakers = len(set(seg.get('speaker', 0) for seg in combined))
                        print(f"üìä Unique speakers in combined: {num_speakers}")
                    else:
                        print("‚ö†Ô∏è  Combined segments is None or empty")
                    result['combined'] = {
                        'segments': combined if combined else [],
                        'num_speakers': len(set(seg.get('speaker', 0) for seg in combined)) if combined else 0,
                        'num_segments': len(combined) if combined else 0,
                        'llm_iterations': llm_iterations  # –Ü–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ LLM —ñ—Ç–µ—Ä–∞—Ü—ñ—ó –¥–ª—è –¥–µ–±–∞–≥ –∫–æ–Ω—Å–æ–ª—ñ
                    }
                    print(f"‚úÖ Combined result prepared: {len(combined) if combined else 0} segments, {len(llm_iterations)} LLM iterations")
                except Exception as e:
                    print(f"‚ùå Error in combine_diarization_and_transcription: {e}")
                    import traceback
                    error_traceback = traceback.format_exc()
                    print(f"üìã Combine error traceback:\n{error_traceback}")
                    traceback.print_exc()
                    result['combined'] = {
                        'segments': [],
                        'num_speakers': 0,
                        'num_segments': 0,
                        'llm_iterations': []
                    }
            else:
                print("‚ö†Ô∏è  Warning: Cannot combine - no words available")
                result['combined'] = {
                    'segments': [],
                    'num_speakers': 0,
                    'num_segments': 0,
                    'llm_iterations': []  # –î–æ–¥–∞—î–º–æ –ø–æ—Ä–æ–∂–Ω—ñ–π —Å–ø–∏—Å–æ–∫ –¥–ª—è llm_iterations
                }
        # –í–∏–¥–∞–ª—è—î–º–æ —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª
        try:
            os.remove(filepath)
        except:
            pass
        print("‚úÖ Processing complete!")
        return jsonify(result)
    except Exception as e:
        print(f"‚ùå Error in api_diarize: {e}")
        import traceback
        error_traceback = traceback.format_exc()
        print(f"üìã Full traceback:\n{error_traceback}")
        traceback.print_exc()
        # –í–∏–¥–∞–ª—è—î–º–æ —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª —É —Ä–∞–∑—ñ –ø–æ–º–∏–ª–∫–∏
        try:
            if 'filepath' in locals():
                os.remove(filepath)
        except Exception as cleanup_error:
            print(f"‚ö†Ô∏è  Could not clean up file: {cleanup_error}")
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': error_traceback if app.debug else None
        }), 500


@app.route('/process', methods=['POST', 'OPTIONS'])
def process_audio():
    """
    –û—Å–Ω–æ–≤–Ω–∏–π –µ–Ω–¥–ø–æ—ñ–Ω—Ç –¥–ª—è iOS Shortcuts.
    –ü–æ–≤–µ—Ä—Ç–∞—î —Ñ–æ—Ä–º–∞—Ç –∑–≥—ñ–¥–Ω–æ —Å–ø–µ—Ü–∏—Ñ—ñ–∫–∞—Ü—ñ—ó.
    """
    # –û–±—Ä–æ–±–∫–∞ OPTIONS –¥–ª—è preflight –∑–∞–ø–∏—Ç—ñ–≤ (CORS)
    if request.method == 'OPTIONS':
        print("‚úÖ OPTIONS preflight request received from", request.remote_addr)
        response = jsonify({})
        response.headers.add('Access-Control-Allow-Origin', '*')
        response.headers.add('Access-Control-Allow-Headers', 'Content-Type')
        response.headers.add('Access-Control-Allow-Methods', 'POST, OPTIONS')
        return response
    print(f"üì• POST /process request received from {request.remote_addr}")
    print(f"üìã Headers: {dict(request.headers)}")
    print(f"üì¶ Files in request: {list(request.files.keys())}")
    print(f"üìù Form data keys: {list(request.form.keys())}")
    filepath = None
    start_time = time.time()
    try:
        # –í–∞–ª—ñ–¥–∞—Ü—ñ—è —Ñ–∞–π–ª—É
        if 'file' not in request.files:
            return jsonify({
                'success': False,
                'error': 'No file uploaded',
                'code': 'NO_FILE'
            }), 400
        file = request.files['file']
        if file.filename == '':
            return jsonify({
                'success': False,
                'error': 'No file selected',
                'code': 'NO_FILE'
            }), 400
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç—É —Ñ–∞–π–ª—É
        if not allowed_file(file.filename):
            return jsonify({
                'success': False,
                'error': f'Invalid audio format. Allowed: {", ".join(ALLOWED_EXTENSIONS)}',
                'code': 'INVALID_FORMAT'
            }), 400
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Ñ–∞–π–ª —Ç–∏–º—á–∞—Å–æ–≤–æ
        filename = secure_filename(file.filename)
        filepath = os.path.join(UPLOAD_FOLDER, filename)
        file.save(filepath)
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ä–æ–∑–º—ñ—Ä—É —Ñ–∞–π–ª—É
        file_size = os.path.getsize(filepath)
        if file_size > MAX_FILE_SIZE:
            os.remove(filepath)
            return jsonify({
                'success': False,
                'error': f'File too large. Maximum size: {MAX_FILE_SIZE / (1024*1024):.0f} MB',
                'code': 'FILE_SIZE_EXCEEDED'
            }), 413
        # –û—Ç—Ä–∏–º—É—î–º–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
        num_speakers = request.form.get('num_speakers', type=int)
        language = request.form.get('language', type=str) or None
        if language and language.lower() == 'auto':
            language = None
        segment_duration = float(request.form.get('segment_duration', 1.5))
        overlap = float(request.form.get('overlap', 0.5))
        print(f"üìÅ Processing file: {filename} ({file_size / (1024*1024):.2f} MB)")
        # –û–±—á–∏—Å–ª—é—î–º–æ —Ç—Ä–∏–≤–∞–ª—ñ—Å—Ç—å –∞—É–¥—ñ–æ
        try:
            audio_duration = librosa.get_duration(path=filepath)
            print(f"‚è±Ô∏è  Audio duration: {audio_duration:.2f} seconds")
        except Exception as e:
            print(f"‚ö†Ô∏è  Could not determine audio duration: {e}")
            audio_duration = 0
        # –í–∏—Ç—è–≥—É—î–º–æ –µ–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ç–∞ –≤–∏–∫–æ–Ω—É—î–º–æ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—é
        print("üîç Extracting speaker embeddings...")
        embeddings, timestamps = extract_speaker_embeddings(
            filepath, 
            segment_duration=segment_duration, 
            overlap=overlap
        )
        if embeddings is None:
            os.remove(filepath)
            return jsonify({
                'success': False,
                'error': 'Failed to extract speaker embeddings. Audio may be corrupted or unsupported format.',
                'code': 'PROCESSING_ERROR'
            }), 500
        if len(embeddings) == 0:
            os.remove(filepath)
            return jsonify({
                'success': False,
                'error': f'Audio too short (duration: {audio_duration:.2f}s). Minimum recommended: 2 seconds.',
                'code': 'PROCESSING_ERROR'
            }), 500
        print("üë• Performing diarization...")
        diarization_segments = diarize_audio(embeddings, timestamps, num_speakers)
        if not diarization_segments:
            os.remove(filepath)
            return jsonify({
                'success': False,
                'error': 'Diarization failed. Could not identify speakers.',
                'code': 'PROCESSING_ERROR'
            }), 500
        # –¢—Ä–∞–Ω—Å–∫—Ä–∏–±—É—î–º–æ –∞—É–¥—ñ–æ
        print("üìù Transcribing audio...")
        transcription, transcription_segments, words = transcribe_audio(filepath, language)
        if not transcription or not words:
            os.remove(filepath)
            return jsonify({
                'success': False,
                'error': 'Transcription failed. Could not transcribe audio.',
                'code': 'PROCESSING_ERROR'
            }), 500
        # –û–±'—î–¥–Ω—É—î–º–æ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—é —Ç–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—é
        print("üîó Combining diarization and transcription...")
        combined_segments = combine_diarization_and_transcription(
            diarization_segments, 
            words
        )
        # –í–∏–ø—Ä–∞–≤–ª—è—î–º–æ –ø–æ–º–∏–ª–∫–∏ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó (–æ–±'—î–¥–Ω—É—î–º–æ —Å–µ–≥–º–µ–Ω—Ç–∏, —è–∫—ñ –Ω–∞—Å–ø—Ä–∞–≤–¥—ñ –Ω–∞–ª–µ–∂–∞—Ç—å –æ–¥–Ω–æ–º—É —Å–ø—ñ–∫–µ—Ä—É)
        # –¢–ò–ú–ß–ê–°–û–í–û –í–ò–ú–ö–ù–ï–ù–û - –ø–æ—Ç—Ä–µ–±—É—î –¥–æ–æ–ø—Ä–∞—Ü—é–≤–∞–Ω–Ω—è
        # if combined_segments and len(combined_segments) > 0:
        #     combined_segments = fix_diarization_errors(combined_segments)
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ç–∞–π–º–∞—É—Ç—É
        processing_time = time.time() - start_time
        if processing_time > PROCESSING_TIMEOUT:
            os.remove(filepath)
            return jsonify({
                'success': False,
                'error': 'Processing timeout',
                'code': 'TIMEOUT'
            }), 408
        # –§–æ—Ä–º—É—î–º–æ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –∑–≥—ñ–¥–Ω–æ —Å–ø–µ—Ü–∏—Ñ—ñ–∫–∞—Ü—ñ—ó
        result = {
            'success': True,
            'duration': round(audio_duration, 2),
            'full_text': transcription,
            'segments': [
                {
                    'speaker': seg['speaker'],
                    'start': seg['start'],
                    'end': seg['end'],
                    'text': seg['text']
                }
                for seg in combined_segments
            ]
        }
        # –í–∏–¥–∞–ª—è—î–º–æ —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª
        try:
            os.remove(filepath)
        except:
            pass
        processing_time = time.time() - start_time
        print(f"‚úÖ Processing complete! Time: {processing_time:.2f}s")
        return jsonify(result)
    except Exception as e:
        print(f"‚ùå Error in process_audio: {e}")
        import traceback
        traceback.print_exc()
        # –í–∏–¥–∞–ª—è—î–º–æ —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª —É —Ä–∞–∑—ñ –ø–æ–º–∏–ª–∫–∏
        try:
            if filepath and os.path.exists(filepath):
                os.remove(filepath)
        except:
            pass
        return jsonify({
            'success': False,
            'error': str(e),
            'code': 'PROCESSING_ERROR'
        }), 500


if __name__ == '__main__':
    port = int(os.environ.get('DEMO2_PORT', 5005))
    print(f"üöÄ Starting Flask server for iOS Shortcuts on port {port}")
    print(f"üìÇ Upload folder: {UPLOAD_FOLDER}")
    print(f"üåê Server will be accessible at: http://0.0.0.0:{port}")
    print(f"üì± Use your Mac's IP address for iOS Shortcuts")
    app.run(host='0.0.0.0', port=port, debug=False)

